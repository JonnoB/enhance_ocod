{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e3e22df",
   "metadata": {},
   "source": [
    "# Unit tag and Span cleaning\n",
    "\n",
    "This script combines two steps in the OCOD processing pipeline.\n",
    "\n",
    "* unit tagging\n",
    "* Removing overlapping spans\n",
    "\n",
    "These two processes are separated by the weak labelling in humanloop but as they are relatively simple they are included in a single script\n",
    "\n",
    "* **Raw CSV loaded and lightly processed. Output**: two column csv columns, property address, unit tag\n",
    "* Data labelled in programmatic. Output: json file of entities.\n",
    "* **Data programmatic output json cleaned ordered and overlaps removed**. Output: json file\n",
    "* Clean json converted to dataframe and multi-addresses expanded. Output: CSV\n",
    "* Count and locate addresses\n",
    "* Create address matcher and match businesses\n",
    "* Classify address types\n",
    "\n",
    "## Unit tagging\n",
    "\n",
    "This park of the pipeline adds in a binary value indicating whether the line contains flats/units/stores etc which are likely to have unit level ID. This is important as such addresses are likely to have a unit ID AND an street number and as such need to be treated with care"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "320c86fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "from helper_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03b9655a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-ca9fcbca7600>:1: DtypeWarning: Columns (24,28,30,32,33,34) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  ocod_data =  pd.read_csv('./empty_homes_data/' +\n",
      "<ipython-input-2-ca9fcbca7600>:14: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  ocod_data.property_address = ocod_data.property_address.str.replace('\\s{2,}', r' ')\n",
      "<ipython-input-2-ca9fcbca7600>:28: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  ocod_data['flat_tag'] = ocod_data['property_address'].str.contains(flatregex + '|'+flatregex2, case = False)\n",
      "<ipython-input-2-ca9fcbca7600>:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  ocod_data['commercial_park_tag'] = ocod_data['property_address'].str.contains(r\"(retail|industrial|commercial|business|distribution|car)\", case = False)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "ocod_data =  pd.read_csv('./data/' +\n",
    "                    'OCOD_FULL_2022_02.csv',\n",
    "                   encoding_errors= 'ignore').rename(columns = lambda x: x.lower().replace(\" \", \"_\"))\n",
    "ocod_data['postcode'] = ocod_data['postcode'].str.upper()\n",
    "#empty addresses cannot be used. however there are only three so not a problem\n",
    "ocod_data = ocod_data.dropna(subset = 'property_address')\n",
    "ocod_data.reset_index(inplace = True, drop = True)\n",
    "ocod_data['property_address'] = ocod_data['property_address'].str.lower()\n",
    "\n",
    "#ensure there is a space after commas\n",
    "#This is because some numbers are are written as 1,2,3,4,5 which causes issues during tokenisation\n",
    "ocod_data.property_address = ocod_data.property_address.str.replace(',', r', ')\n",
    "#remove multiple spaces\n",
    "ocod_data.property_address = ocod_data.property_address.str.replace('\\s{2,}', r' ')\n",
    "\n",
    "\n",
    "#different words associated with unit ID's\n",
    "flatregex = r\"(flat|apartment|penthouse|unit)\" #unit|store|storage these a\n",
    "\n",
    "#This is not an exhaustive list of road names but it covers about 80% of all road types in the VOA business register.\n",
    "#The cardinal directions are includted as an option as they can appear after the road type. However they serve no real purpose in this particular regex and are \n",
    "#included for completness\n",
    "road_regex  = r\"((road|street|lane|way|gate|avenue|close|drive|hill|place|terrace|crescent|gardens|square|walk|grove|mews|row|view|boulevard|pleasant|vale|yard|chase|rise|green|passage|friars|viaduct|promenade|end|ridge|embankment|villas|circus))\\b( east| west| north| south)?\"\n",
    "#These names may be followed by a road type e.g. Earls court road. A negative lookahead is used to prevent these roads being tagged as units.\n",
    "flatregex2 = r\"(mansions|villa|court)\\b(?!(\\s\"+road_regex+\"))\"\n",
    "\n",
    "#flat_tag is used for legacy reasons but refers to sub-units in general\n",
    "ocod_data['flat_tag'] = ocod_data['property_address'].str.contains(flatregex + '|'+flatregex2, case = False)\n",
    "\n",
    "ocod_data['commercial_park_tag'] = ocod_data['property_address'].str.contains(r\"(retail|industrial|commercial|business|distribution|car)\", case = False)\n",
    "\n",
    "#typo in the data leads to a large number of fake flats\n",
    "ocod_data.loc[:, 'property_address'] = ocod_data['property_address'].str.replace(\"stanley court \", \"stanley court, \")\n",
    "#This typo leads to some rather silly addresses\n",
    "ocod_data.loc[:, 'property_address'] = ocod_data['property_address'].str.replace(\"100-1124\", \"100-112\")\n",
    "ocod_data.loc[:, 'property_address'] = ocod_data['property_address'].str.replace(\"40a, 40, 40¨, 42, 44\", \"40a, 40, 40, 42, 44\")\n",
    "\n",
    "\n",
    "#only two columns are needed for the humanloop labelling process\n",
    "ocod_data[['property_address', 'flat_tag', 'commercial_park_tag', 'title_number']].rename(columns = {'property_address':'text'}).to_csv('./data/property_address_only.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "503ca22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the index for the ground truth\n",
    "random.seed(2017)\n",
    "test_set = random.sample([*range(0, ocod_data.shape[0])], 1000) \n",
    "\n",
    "test_set = ocod_data.loc[test_set, 'title_number'].reset_index().rename(columns = {'index':'datapoint_id'})\n",
    "\n",
    "test_set.to_csv('./data/test_set_indices_space_cleaned_v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15b8c0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Belatedly create dev set\n",
    "#This also needs to be manually labelled and so is also pretty small.\n",
    "\n",
    "dev_set_all = ocod_data.loc[~ocod_data.title_number.isin(test_set.title_number),:]\n",
    "random.seed(2017)\n",
    "dev_set = random.sample(dev_set_all.title_number.to_list(), 2000)\n",
    "\n",
    "dev_set = dev_set_all.loc[dev_set_all.title_number.isin(dev_set), 'title_number'].reset_index().rename(columns = {'index':'datapoint_id'})\n",
    "\n",
    "dev_set.to_csv('./data/dev_set.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cd0d5d",
   "metadata": {},
   "source": [
    "## Labelling in Humanloop\n",
    "\n",
    "This part of process uses the humanloop programmatic app and is an external process. Once the labelling step is complete the process outputs a json file containing the labels and spans, this is then cleaned in the next step.\n",
    "\n",
    "## Removing overlapping spans\n",
    "\n",
    "During the humanloop tagging process the rules may result in the same words being tagged as part of multiple spans, this often occures for road names made up of multiple parts \n",
    "e.g. Canberra Crescent Gardens may be tagges as Canberra Cresecent and Canberra Crescent Gardens. The overlaps need to be removed before further prcoessing.\n",
    "For simplicity the largest span of any two overlapping spans is kept and the smaller of the two is removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33c6012a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#These libraries are specific to this part of the process\n",
    "import json\n",
    "import requests \n",
    "import config #contains hidden api key\n",
    "import operator #used for sorting the label dictionaries by start point. This is the basis for removing overlaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ae4faed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening JSON file\n",
    "f =open(\"./data/test.json\")  #aggregate and download button\n",
    "\n",
    "# returns JSON object as \n",
    "# a dictionary\n",
    "data = json.load(f)\n",
    "\n",
    "#this makes a list of all the observation rows. These refer to the row of the orginal observation text and so can be linked back to the original OCOD dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4270572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count = 5000\n",
      "count = 10000\n",
      "count = 15000\n",
      "count = 20000\n",
      "count = 25000\n",
      "count = 30000\n",
      "count = 35000\n",
      "count = 40000\n",
      "count = 45000\n",
      "count = 50000\n",
      "count = 55000\n",
      "count = 60000\n",
      "count = 65000\n",
      "count = 70000\n",
      "count = 75000\n",
      "count = 80000\n",
      "count = 85000\n",
      "count = 90000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "datapoint_id_list = [*range(0,len(data['datapoints']))]\n",
    "\n",
    "data_and_labels = []\n",
    "data_labels_dict = []\n",
    "\n",
    "count_it = 0\n",
    "for i in set(datapoint_id_list):\n",
    "\n",
    "    count_it += 1\n",
    "    if count_it % 5000 == 0: \n",
    "        print('count = {}'.format(count_it))\n",
    "        \n",
    "    #single_id_index = np.where(np.array(datapoint_id_list)==i)\n",
    "    ##these labels are in tuple form\n",
    "   # list_of_labels = [(data[x]['start'], data[x]['end'], data[x]['label']) for x in single_id_index[0].tolist()]\n",
    "    list_of_labels_dict = results_list = data['datapoints'][i]['programmatic']['results']\n",
    "    ##these labels are in dictionary form\n",
    "#     list_of_labels_dict = [{'start': x['start'], \n",
    "#                             'end':x['end'], \n",
    "#                             'label': x['label'], \n",
    "#                             'label_text': x['text'] } for x in results_list]\n",
    "    \n",
    "    #this inplace sorting using operator orders the dictionary by the start point. ties are automatically broken\n",
    "    #it required the operator library\n",
    "    list_of_labels_dict.sort(key=operator.itemgetter('start'))\n",
    "    \n",
    "    list_of_labels_dict = remove_overlapping_spans2(list_of_labels_dict)\n",
    "\n",
    "    #create the NER dataset structure shown on the spacy website\n",
    "   # data_and_labels = data_and_labels + [ ( ocod_data['property_address'][i], list_of_labels ) ]\n",
    "    #create a list of dictionaries using a similar structure to save as a json\n",
    "    data_labels_dict = data_labels_dict + [\n",
    "        {\n",
    "            'text' : ocod_data['property_address'][i],\n",
    "            'labels' : list_of_labels_dict,\n",
    "            'datapoint_id': i,\n",
    "            'title_number':ocod_data['title_number'][i]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "#Save the cleaned data back as a json file ready to be processed further  \n",
    "with open('./data/full_dataset_no_overlaps.json', 'w') as f:\n",
    "    json.dump(data_labels_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2e7f63",
   "metadata": {},
   "source": [
    "### Uploading to humanloop cloud\n",
    "\n",
    "This allows a sample of the data to be uploaded to the humanloop cloud so that an example model can be made.\n",
    "The model provides another way to check the quality of the weak labelling. However, only 10k obersvations can be uploaded, as such a sub-sample is used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "63fffc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "f =open('./data/full_dataset_no_overlaps.json')  \n",
    "\n",
    "data_labels_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d45c5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "jason_test_data ={\n",
    "     \"name\": \"test_set_24_04_22\",\n",
    "     \"description\": \"the ground truth test set for labelling\",\n",
    "     \"fields\": [\n",
    "         {\"name\": \"text\", \n",
    "          \"data_type\": \"text\"\n",
    "         },\n",
    "         {\"name\": \"labels\", \n",
    "          \"data_type\": \"character_offsets\"},\n",
    "         {\"name\": \"datapoint_id\", \n",
    "          \"data_type\": \"text\"\n",
    "         }\n",
    "     ],\n",
    "     \"data\": [data_labels_dict[x] for x in test_set.loc[:,'datapoint_id']]#, #upload only data from the test set\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "550b60b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "jason_dev_data ={\n",
    "     \"name\": \"dev_set_13_05_22\",\n",
    "     \"description\": \"the ground truth test set for labelling\",\n",
    "     \"fields\": [\n",
    "         {\"name\": \"text\", \n",
    "          \"data_type\": \"text\"\n",
    "         },\n",
    "         {\"name\": \"labels\", \n",
    "          \"data_type\": \"character_offsets\"},\n",
    "         {\"name\": \"datapoint_id\", \n",
    "          \"data_type\": \"text\"\n",
    "         }\n",
    "     ],\n",
    "     \"data\": [data_labels_dict[x] for x in dev_set.loc[:,'datapoint_id']]#, #upload only data from the test set\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efa90b7",
   "metadata": {},
   "source": [
    "# Create new project with unlabelled data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9032c7dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Navigate to https://app.humanloop.com/projects/1651\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "Step 1: Specify URL and headers for your API requests and some helper methods \n",
    "Reference: https://api.humanloop.com/docs#section/Authentication\n",
    "Notes: \n",
    "    - If you don't already have a Humanloop account,\n",
    "      signup @ https://app.humanloop.com/signup\n",
    "    - Replace <INSERT YOUR API KEY HERE> with your users X-API-KEY \n",
    "      @ https://app.humanloop.com/profile\n",
    "\"\"\"\n",
    "base_url = \"https://api.humanloop.com\"\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"X-API-KEY\":  config.api_key,#the api key is hidden in a config file\n",
    "}\n",
    "# use the email associated to your Humanloop account\n",
    "project_owner = \"jonathan.s.bourne@gmail.com\"\n",
    "\n",
    "\n",
    "def get_field_id_by_name(name: str, fields):\n",
    "    \"\"\"Helper method for parsing field_id from dataset.fields given the name\"\"\"\n",
    "    return [field for field in fields if field[\"name\"] == name][0][\"id\"]\n",
    "\n",
    "\n",
    "\"\"\" \n",
    "Step 2: Create a dataset\n",
    "Reference: https://api.humanloop.com/docs#operation/upload_data_datasets_post\n",
    "Notes:\n",
    "    - It can be helpful to include your own unique identifiers for your data-points\n",
    "      if available so that you can easily correlate any annotations and predictions \n",
    "      created by Humanloop back to your system.\n",
    "    - If using large datasets (> 10k rows), you will have to upload it in multiple \n",
    "      batches using the API. Starting with the POST as shown below, then adding \n",
    "      subsequent batches using the PUT against the newly created dataset \n",
    "      (https://api.humanloop.com/docs#operation/update_data_datasets__id__put.)\n",
    "\"\"\"\n",
    "\n",
    "dataset_fields = requests.post(\n",
    "    url=f\"{base_url}/datasets\", data=json.dumps(jason_dev_data), headers=headers ######################## CHANGE this depending on whether dev or test\n",
    ").json()[\"fields\"]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Step 3: Create a project\n",
    "Reference: https://api.humanloop.com/docs#operation/create_project_projects_post\n",
    "Notes:\n",
    "    - A Humanloop project is made up of one or more datasets, a team of annotators \n",
    "      and a model. As your team begin to annotate the data, a model is trained in real \n",
    "      time and used to prioritise what data your annotators should focus on next \n",
    "      (see https://humanloop.com/blog/why-you-should-be-using-active-learning).\n",
    "    - The project inputs specify those dataset fields you wish to show to \n",
    "      your annotators and the model. \n",
    "    - The project output specifies the type of model you wish to train and the \n",
    "      corresponding label taxonomy. \n",
    "    - If your dataset has a field with existing annotations, you can use this to \n",
    "      warm start your project as shown in the following examples. \n",
    "      If you want your team to first review these existing annotations in Humanloop, \n",
    "      set \"review_existing_annotations\" to True, otherwise they will be used \n",
    "      automatically to train an initial model.\n",
    "    - Both classification (single and multi-label) and extraction\n",
    "      projects are supported.\n",
    "    - You can update your project with more data by either connecting another dataset \n",
    "      or simply adding additional data-points to your existing dataset. \n",
    "      Alternatively, you can submit tasks for your model and/or team to complete\n",
    "      (see our Human-in-the-loop tutorial for more information on this!).\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Step 3b: Span extraction project \n",
    "\"\"\"\n",
    "extraction_project_request = {\n",
    "    \"name\": \"Ground truth for offshore empties dev set for spacy training\",\n",
    "    \"inputs\": [\n",
    "        {\n",
    "            \"name\": \"text\",\n",
    "            \"data_type\": \"text\",\n",
    "            \"description\": \"unparsed addresses\",\n",
    "            \"data_sources\": [\n",
    "                {\"field_id\": get_field_id_by_name(\"text\", dataset_fields)}\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"datapoint_id\",\n",
    "            \"data_type\": \"text\",\n",
    "            \"description\": \"The original row the data is on\",\n",
    "            \"display_only\": True,\n",
    "            \"data_sources\": [\n",
    "                {\"field_id\": get_field_id_by_name(\"datapoint_id\", dataset_fields)}\n",
    "            ],\n",
    "        },\n",
    "    ],\n",
    "    \"outputs\": [\n",
    "        {\n",
    "            \"name\": \"labels\",\n",
    "            \"description\": \"entities address parts\",\n",
    "            \"task_type\": \"sequence_tagging\",\n",
    "            \"data_sources\": [\n",
    "                {\"field_id\": get_field_id_by_name(\"labels\", dataset_fields)}\n",
    "            ],\n",
    "            # which input you wish your model to extract from\n",
    "            \"input\": \"text\",\n",
    "        }\n",
    "    ],\n",
    "    \"users\": [project_owner],\n",
    "    \"guidelines\": \"Insert your markdown annotator guidelines here\",\n",
    "    \"review_existing_annotations\": True,\n",
    "}\n",
    "\n",
    "extraction_project_id = requests.post(\n",
    "    url=f\"{base_url}/projects\",\n",
    "    data=json.dumps(extraction_project_request),\n",
    "    headers=headers,\n",
    ").json()[\"id\"]\n",
    "\n",
    "print(f\"Navigate to https://app.humanloop.com/projects/{extraction_project_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407e2eec",
   "metadata": {},
   "source": [
    "# Spacy "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55f4bf2",
   "metadata": {},
   "source": [
    "## create spacy format\n",
    "\n",
    "The below chunk creates the format using the output of programmatic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fce550f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count = 5000\n",
      "count = 10000\n",
      "count = 15000\n",
      "count = 20000\n",
      "count = 25000\n",
      "count = 30000\n",
      "count = 35000\n",
      "count = 40000\n",
      "count = 45000\n",
      "count = 50000\n",
      "count = 55000\n",
      "count = 60000\n",
      "count = 65000\n",
      "count = 70000\n",
      "count = 75000\n",
      "count = 80000\n",
      "count = 85000\n",
      "count = 90000\n"
     ]
    }
   ],
   "source": [
    "datapoint_id_list = [*range(0,len(data['datapoints']))]\n",
    "data_and_labels = []\n",
    "data_labels_dict = []\n",
    "\n",
    "count_it = 0\n",
    "for i in set(datapoint_id_list):\n",
    "    count_it += 1\n",
    "    if count_it % 5000 == 0: \n",
    "        print('count = {}'.format(count_it))\n",
    "        \n",
    "    single_id_index = np.where(np.array(datapoint_id_list)==i)\n",
    "    ##these labels are in tuple form\n",
    "    \n",
    "    results_list = data['datapoints'][i]['programmatic']['results']\n",
    "    list_of_labels =[(x['start'],x['end'],x['label'] ) for x in results_list]\n",
    "    \n",
    "    #list_of_labels = [(data[x]['start'], data[x]['end'], data[x]['label']) for x in single_id_index[0].tolist()]\n",
    "    \n",
    "    list_of_labels.sort(key=lambda y: y[0])\n",
    "    \n",
    "    list_of_labels = remove_overlapping_spans_tuples(list_of_labels)\n",
    "    #print(list_of_labels)\n",
    "    #create the NER dataset structure shown on the spacy website\n",
    "    data_and_labels = data_and_labels + [ {\n",
    "        'datapoint_id': i,\n",
    "        'text':ocod_data['property_address'][i], \n",
    "                                           'entities':list_of_labels}  ]\n",
    "\n",
    "#Save the cleaned data back as a json file ready to be processed further  \n",
    "with open('./data/humanloop_spacy_format.json', 'w') as f:\n",
    "    json.dump(data_and_labels, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb615337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'datapoint_id': 0,\n",
       " 'text': 'westleigh lodge care home, nel pan lane, leigh (wn7 5jt)',\n",
       " 'entities': [(0, 25, 'building_name'),\n",
       "  (27, 39, 'street_name'),\n",
       "  (41, 46, 'city'),\n",
       "  (48, 55, 'postcode')]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_and_labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1971a0d9",
   "metadata": {},
   "source": [
    "This chunk creates the format using the output of the humanloop cloud labelling. This acts as the ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f16eb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "f =open('./data/ground_truth_dev_set_labels.json')  \n",
    "\n",
    "data_labels_dict_gt = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0d29666",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_and_labels = []\n",
    "\n",
    "count_it = 0\n",
    "for i in range(0, len(data_labels_dict_gt)):\n",
    "    count_it += 1\n",
    "    if count_it % 5000 == 0: \n",
    "        print('count = {}'.format(count_it))\n",
    "        \n",
    "    inputs = data_labels_dict_gt[i]['inputs']\n",
    "\n",
    "    labels = data_labels_dict_gt[i]['data']['labels']\n",
    "\n",
    "    list_of_labels =list_of_labels =[(x['start'],x['end'],x['label'] ) for x in labels]\n",
    "    \n",
    "    list_of_labels.sort(key=lambda y: y[0])\n",
    "    \n",
    "    #create the NER dataset structure shown on the spacy website\n",
    "    data_and_labels = data_and_labels + [ {\n",
    "        'datapoint_id': inputs['datapoint_id'],\n",
    "        'text':inputs['text'], \n",
    "                                           'entities':list_of_labels}  ]\n",
    "    \n",
    "    data_and_labels.sort(key=lambda y: y.get('datapoint_id'))\n",
    "\n",
    "#Save the cleaned data back as a json file ready to be processed further  \n",
    "with open('./data/humanloop_spacy_format_gt.json', 'w') as f:\n",
    "    json.dump(data_and_labels, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2e11140",
   "metadata": {},
   "outputs": [],
   "source": [
    "f =open('./data/humanloop_spacy_format.json')  #aggregate and download button\n",
    "spacy_data = json.load(f)\n",
    "\n",
    "f =open('./data/humanloop_spacy_format_gt.json')  #aggregate and download button\n",
    "dev_set = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fa5d30a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0626f7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(2017)\n",
    "#dev_set_indices = dev_set.loc[:, 'datapoint_id'].to_list()\n",
    "#dev_set_indices = random.sample([*range(0, len(spacy_data))], 9400) \n",
    "\n",
    "from operator import itemgetter\n",
    "dev_set_indices = list(map(itemgetter('datapoint_id'), dev_set))\n",
    "dev_set_indices = list(map(int, dev_set_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e83b03ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dev_set = [spacy_data[x] for x in dev_set.loc[:, 'datapoint_id'].to_list()]\n",
    "train_set = [spacy_data[x] for x in [*range(0, len(spacy_data))] if x not in dev_set_indices ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8543b025",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92088"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a13bd4d3",
   "metadata": {},
   "source": [
    "# Create spaCy training data\n",
    "\n",
    "This chunk creates the spacy training data needed to create a spacy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0e4eb9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "from spacy.lang.char_classes import LIST_PUNCT, LIST_ELLIPSES, LIST_QUOTES, LIST_CURRENCY\n",
    "from spacy.lang.char_classes import LIST_ICONS, HYPHENS, CURRENCY, UNITS\n",
    "from spacy.lang.char_classes import CONCAT_QUOTES, ALPHA_LOWER, ALPHA_UPPER, ALPHA, PUNCT\n",
    "from spacy.util import compile_infix_regex\n",
    "\n",
    "alignment_mode_type = \"expand\"#\"strict\"\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "\n",
    "infixes = (\n",
    "    LIST_ELLIPSES\n",
    "    + LIST_ICONS\n",
    "    + [\n",
    "        r\"(?<=[0-9])[+\\-\\,*^\\(\\)\\/](?=[0-9-])\", #added in / to break up 34/45 etc. added in , to break up 34,35 although this should now be removed in the cleaning stage\n",
    "        r\"(?<=[{al}{q}])\\.(?=[{au}{q}])\".format(\n",
    "            al=ALPHA_LOWER, au=ALPHA_UPPER, q=CONCAT_QUOTES\n",
    "        ),\n",
    "        r\"(?<=[{a}]),(?=[{a}])\".format(a=ALPHA),\n",
    "        r\"(?<=[{a}])(?:{h})(?=[{a}])\".format(a=ALPHA, h=HYPHENS),\n",
    "        r\"(?<=[{a}0-9])[:<>=/\\(\\)](?=[{a}])\".format(a=ALPHA),\n",
    "        r\"(?<=[{a}])[:<>=/\\(\\)](?=[{a}0-9])\".format(a=ALPHA), #I added this one in to try and break things like \"(odd)33-45\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "infix_re = compile_infix_regex(infixes)\n",
    "nlp.tokenizer.infix_finditer = infix_re.finditer\n",
    "\n",
    "\n",
    "\n",
    "training_data = train_set\n",
    "# the DocBin will store the example documents\n",
    "db = DocBin()\n",
    "for i in range(0, len(training_data)):\n",
    "    current_set = training_data[i]\n",
    "    #print(i) #printing is used for debugging\n",
    "    doc = nlp(current_set['text'])\n",
    "    ents = []\n",
    "    for start, end, label in current_set['entities']:\n",
    "        span = doc.char_span(start, end, label=label, alignment_mode = alignment_mode_type )\n",
    "        ents.append(span)\n",
    "    doc.ents = ents\n",
    "    db.add(doc)\n",
    "db.to_disk(\"./data/spacy_data/train.spacy\")\n",
    "\n",
    "training_data = dev_set\n",
    "# the DocBin will store the example documents\n",
    "db = DocBin()\n",
    "for i in range(0, len(training_data)):\n",
    "    current_set = training_data[i]\n",
    "    #print(i)\n",
    "    doc = nlp(current_set['text'])\n",
    "    ents = []\n",
    "    for start, end, label in current_set['entities']:\n",
    "        span = doc.char_span(start, end, label=label, alignment_mode = alignment_mode_type )\n",
    "        ents.append(span)\n",
    "    doc.ents = ents\n",
    "    db.add(doc)\n",
    "db.to_disk(\"./data/spacy_data/dev.spacy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5436272c",
   "metadata": {},
   "source": [
    "## spacy training data debugging\n",
    "\n",
    "The below chunks help debug the creation of the spacy training data.\n",
    "\n",
    "Errors are usually caused by tokenization issues. Several of this issues have been solved by changing the 'infixes' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "537d2ba6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'datapoint_id': 1110,\n",
       " 'text': 'flat 40/41, aldford house, park street, london (w1k 7lg)',\n",
       " 'entities': [[0, 4, 'unit_type'],\n",
       "  [5, 7, 'unit_id'],\n",
       "  [12, 25, 'building_name'],\n",
       "  [27, 38, 'street_name'],\n",
       "  [40, 46, 'city'],\n",
       "  [48, 55, 'postcode']]}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data = spacy_data\n",
    "\n",
    "training_data[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bd6f5fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flat\n",
      "None\n",
      "aldford house\n",
      "park street\n",
      "london\n",
      "w1k 7lg\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-3dc27f8d0dbf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0ments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/spacy/tokens/doc.pyx\u001b[0m in \u001b[0;36mspacy.tokens.doc.Doc.ents.__set__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/spacy/tokens/doc.pyx\u001b[0m in \u001b[0;36mspacy.tokens.doc.get_entity_info\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "#nlp = spacy.blank(\"en\")\n",
    "training_data = spacy_data\n",
    "# the DocBin will store the example documents\n",
    "db = DocBin()\n",
    "for i in range(i,i+1):\n",
    "    current_set = training_data[i]\n",
    "    doc = nlp(current_set['text'])\n",
    "    ents = []\n",
    "    for start, end, label in current_set['entities']:\n",
    "        span = doc.char_span(start, end, label=label, alignment_mode = alignment_mode_type )\n",
    "        print(span)\n",
    "        ents.append(span)\n",
    "    doc.ents = ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "85de41f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\r\n",
      "\u001b[38;5;2m✔ Saved config\u001b[0m\r\n",
      "/tf/empty_homes_london/config.cfg\r\n",
      "You can now add your data and train your pipeline:\r\n",
      "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\r\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy init fill-config /tf/empty_homes_london/base_config.cfg /tf/empty_homes_london/config.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9f90db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the code to run the training model\n",
    "#!python -m spacy train config.cfg --paths.train /tf/data/spacy_data/train.spacy --paths.dev /tf/data/spacy_data/dev.spacy --output /tf/data/spacy_data/ --gpu-id 1\n",
    "#!python -m spacy train config.cfg --paths.train /home/jonno/data/spacy_data/train.spacy --paths.dev /home/jonno/data/spacy_data/dev.spacy --output /home/jonno/data/spacy_data/cpu\n",
    "#python -m spacy train ./spacy_config_files/cpu_config.cfg --paths.train ./data/spacy_data/train.spacy --paths.dev ./data/spacy_data/dev.spacy --output ./data/spacy_data/cpu3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc898c8",
   "metadata": {},
   "source": [
    "\n",
    "# Predicting using spacy\n",
    "\n",
    "The below chunks are used to predict label from data using spacy. There are some issues that appear to be related to a recent update of CUDA which has caused a variety of problems. As such this part of the code is being kept separate and some of the code choices may look very strange. \n",
    "\n",
    "Interestingly the performance of the spaCy model is pretty much identical to the labels created using rules. This suggests that the labelling is probably very good and also that spacy is overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f92ef917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tf/empty_homes_london\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/empty_homes_london/address_parsing_helper_functions.py:463: DtypeWarning: Columns (24,28,30,32,33,34) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  ocod_data =  pd.read_csv(file_path,\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import spacy\n",
    "\n",
    "from address_parsing_helper_functions import load_and_prep_OCOD_data\n",
    "\n",
    "import os\n",
    "print(os.getcwd())\n",
    "\n",
    "#spacy.require_gpu()\n",
    "#spacy.prefer_gpu()\n",
    "\n",
    "nlp1 = spacy.load(\"/tf/data/spacy_data/cpu/model-best\") \n",
    "\n",
    "ocod_data = load_and_prep_OCOD_data('/tf/data/' +'OCOD_FULL_2022_02.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d0ea683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3141946792602539\n"
     ]
    }
   ],
   "source": [
    "#transformer takes about 85 minutes with cpu and 2.35 minutes with GPU\n",
    "#However if there is a recent update to pytorch there can be porblems with the GPU inference https://github.com/explosion/spaCy/issues/8229\n",
    "#In addition if nvidia updates something Docker can have difficult to resolve bugs\n",
    "\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "#with torch.no_grad():\n",
    "spacy_docs_list = list(nlp1.pipe(ocod_data.loc[1:100,'property_address']))\n",
    "end_time = time.time()\n",
    "\n",
    "print(end_time - start_time)\n",
    "\n",
    "#This runtime comparison of spacy using cpu and gpu. \n",
    "#GPU about 5-6 times faster for inference on a transformer\n",
    "#https://github.com/BlueBrain/Search/issues/337"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fc5fe894",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_model_path = \"/tf/data/spacy_data/cpu/model-best\"\n",
    "\n",
    "def spacy_pred_fn(spacy_model_path, ocod_data, print_every =1000):\n",
    "    nlp1 = spacy.load(spacy_model_path) \n",
    "\n",
    "    ocod_context = [(ocod_data.loc[x,'property_address'], {'datapoint_id':x}) for x in range(0,ocod_data.shape[0])]\n",
    "    i = 0\n",
    "    all_entities_json = []        \n",
    "    for doc, context in list(nlp1.pipe(ocod_context[0:1000], as_tuples = True)):\n",
    "\n",
    "        #This doesn't print as it is a stream not a conventional loop\n",
    "        #if i%print_every==0: print(\"doc \", i, \" of \"+ str(ocod_data.shape[0]))\n",
    "        #i = i+1\n",
    "\n",
    "        temp = doc.to_json()\n",
    "        temp.pop('tokens')\n",
    "        temp.update({'datapoint_id':context['datapoint_id']})\n",
    "        all_entities_json = spacy_docs_list + [temp]\n",
    "\n",
    "    all_entities = pd.json_normalize(all_entities_json, record_path = \"ents\", meta= ['text', 'datapoint_id'])\n",
    "    \n",
    "    return all_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ccaa18da",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spacy_pred_fn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-a0a1e53f220e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy_pred_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspacy_model_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mocod_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spacy_pred_fn' is not defined"
     ]
    }
   ],
   "source": [
    "test = spacy_pred_fn(spacy_model_path, ocod_data, print_every =1000)\n",
    "\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5527173c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin getting labels\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "target_data = ocod_data.loc[:,'property_address']\n",
    "lower = [0, 20000,40000,60000,80000]\n",
    "upper = [20000,40000,60000,80000,len(target_data)]\n",
    "\n",
    "ocod_context = [(ocod_data.loc[x,'property_address'], {'datapoint_id':x}) for x in range(0,ocod_data.shape[0])]\n",
    "\n",
    "print('begin getting labels')\n",
    "\n",
    "for x in range(0,5):\n",
    "    print(x)\n",
    "    spacy_docs_list = []        \n",
    "    for doc, context in list(nlp1.pipe(ocod_context[lower[x]:upper[x]], as_tuples = True)):\n",
    "        temp = doc.to_json()\n",
    "        temp.pop('tokens')\n",
    "        temp.update({'datapoint_id':context['datapoint_id']})\n",
    "        spacy_docs_list = spacy_docs_list + [temp]\n",
    "        \n",
    "    file_name = '/home/jonno/data/spacy_pred_labels' + str(x) + '.json'\n",
    "    with open(file_name, 'w') as f:\n",
    "        json.dump(spacy_docs_list, f)\n",
    "        \n",
    "all_entities_json = []\n",
    "for x in range(0,5):\n",
    "    print(x)\n",
    "    file_name = '/home/jonno/data/spacy_pred_labels' + str(x) + '.json'\n",
    "    f =open(file_name)  #aggregate and download button\n",
    "    all_entities_json = all_entities_json  + json.load(f)\n",
    "        \n",
    "all_entities = pd.json_normalize(all_entities_json, record_path = \"ents\", meta= ['text', 'datapoint_id'])\n",
    "\n",
    "all_entities.to_csv('/home/jonno/data/spacy_preds_normalised.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1bceeccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_entities = []\n",
    "for x in range(0,5):\n",
    "    file_name = '/home/jonno/data/spacy_pred_labels' + str(x) + '.json'\n",
    "    f =open(file_name)  #aggregate and download button\n",
    "    all_entities = spacy_labels  +json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d56a1b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/jonno/data/spacy_pred_labels.json', 'w') as f:\n",
    "    json.dump(spacy_labels, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4623d7f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jonno/empty_homes_london/address_parsing_helper_functions.py:464: DtypeWarning: Columns (24,28,30,32,33,34) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  ocod_data =  pd.read_csv(file_path,\n",
      "Loading the spaCy model\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jonno/empty_homes_london/full_ocod_parse_process.py\", line 21, in <module>\n",
      "    all_entities = spacy_pred_fn(spacy_model_path = root_path+'full_dataset_no_overlaps.json', ocod_data = ocod_data)\n",
      "  File \"/home/jonno/empty_homes_london/address_parsing_helper_functions.py\", line 524, in spacy_pred_fn\n",
      "    nlp1 = spacy.load(spacy_model_path) \n",
      "  File \"/home/jonno/parse_process/lib/python3.8/site-packages/spacy/__init__.py\", line 51, in load\n",
      "    return util.load_model(\n",
      "  File \"/home/jonno/parse_process/lib/python3.8/site-packages/spacy/util.py\", line 422, in load_model\n",
      "    return load_model_from_path(Path(name), **kwargs)  # type: ignore[arg-type]\n",
      "  File \"/home/jonno/parse_process/lib/python3.8/site-packages/spacy/util.py\", line 484, in load_model_from_path\n",
      "    meta = get_model_meta(model_path)\n",
      "  File \"/home/jonno/parse_process/lib/python3.8/site-packages/spacy/util.py\", line 856, in get_model_meta\n",
      "    return load_meta(model_path / \"meta.json\")\n",
      "  File \"/home/jonno/parse_process/lib/python3.8/site-packages/spacy/util.py\", line 816, in load_meta\n",
      "    raise IOError(Errors.E053.format(path=path.parent, name=\"meta.json\"))\n",
      "OSError: [E053] Could not read meta.json from /home/jonno/empty_homes_data/full_dataset_no_overlaps.json\n"
     ]
    }
   ],
   "source": [
    "!python ~/empty_homes_london/full_ocod_parse_process.py /home/jonno/data/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
