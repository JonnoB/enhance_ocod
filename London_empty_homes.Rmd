---
title: "Untitled"
author: "Jonathan Bourne"
date: "07/09/2019"
output: html_document
editor_options: 
  chunk_output_type: console
---

jonno

THe prices should be updated and the change in price calculated over the brexit period and probably the last crash, this will let us see if areas with high numbers of empty property have higher volatility

#Fascinating law change in minneapolis. 
Minneapolis Saw That NIMBYism Has Victims
Single-family zoning hurts a lot of people. In Minnesotaâ€™s largest city, reformers put them front and center.
Richard D. Kahlenberg
Senior fellow at The Century Foundation 

https://www.theatlantic.com/ideas/archive/2019/10/how-minneapolis-defeated-nimbyism/600601/?utm_campaign=the-atlantic&utm_medium=social&utm_term=2019-10-24T10%3A00%3A24&utm_source=facebook&utm_content=edit-promo&fbclid=IwAR30lz6cF4-vXkNDRBvJ_ZWDcoVJp8JjJMOfXc9JnQBYp9jz2Pfp8d0Hudk

non paywall article
https://www.southwestjournal.com/news/2019/10/triplex-change-slated-for-2040s-first-day/



Sam Comber and Dani Arribas Bell
Machine learning innovations in address matching: A practical comparison of word2vec and CRFs
 https://doi.org/10.1111/tgis.12522

#NExt MOOOVE

##missing sales data
I need to create a function that fills in missing geographies with the nearest non-missing data. This could be done using either and ego network or LSOA -> MSOA ->LAD data. ego network is better LAD is easier.
The first step is to workout how many geographies actually have missing data. if it is 0 at MSOA level then I can just do that

The best idea to solve this is to simply append the filler data to the price df where the geogrphy code is that which needs to be added. This requires minimum complexity during the sampling process, which will keep it fast and simple.


##Aggregation. 

Aggregating the total values does not seem to be working well, the weights seem all over the place, this needs to be fixed.

```{r}
packages <- c("tidyverse", "lubridate","spdep", "sf" ,"readxl", "sp","rgdal", "rgeos","forcats", "stringr", "data.table", "xtable")

#SubCode <- "~/Dropbox/SSE/Empty Homes/EmptyHomesCode/SubCode"
#setwd(SubCode)
#source("Setup.R")

new.packages <- packages[!(packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)


lapply(packages, library, character.only = TRUE)

basewd <- "/home/jonno/empty_homes_data"
basecd <- "/home/jonno/empty_homes_london"

list.files(file.path(basecd, "functions"), full.names = T) %>%
  walk(~source(.x))

Figures <- "/home/jonno/Dropbox/Apps/ShareLaTeX/Offshore London paper/Figures" #file.path(basewd, "Figures")
TexTables <- "/home/jonno/Dropbox/Apps/ShareLaTeX/Offshore London paper/Tables"
#suppressMessages(source(file.path(CommonCode, "AuxdataLoad.R")))
```

#Postcodes for London

The postcode data also includes upto date deprivation data

```{r}
#Is this even used
postcodes_df <- read_csv(unz(file.path(basewd, "ONSPD_NOV_2021_UK.zip"), "Data/ONSPD_NOV_2021_UK.csv"))  %>%
  filter(rgn =="E12000007") %>%
    select(Postcode = pcd, LSOA11CD = lsoa11, MSOA11CD = msoa11, LAD11CD = oslaua, imd_rank = imd) %>%
  mutate(Postcode = gsub(" ", "", Postcode)) %>%
  #add on the names of the local authorities for ease of understanding
  left_join(read_csv(unz(file.path(basewd, "ONSPD_NOV_2021_UK.zip"), "Documents/LA_UA names and codes UK as at 04_21.csv")) %>%
  select(LAD11CD = LAD21CD, LAD11NM = LAD21NM))



```


```{r}



prices <- list.files(file.path(basewd, "price_paid_files"), full.names = T) %>%
  map_df(~{
    prices <- read_csv(.x, col_names = FALSE ) %>%
    filter(X14 =="GREATER LONDON")  
    
  }) %>%
    filter(X5 %in% c("D", "S", "T", "F")) %>% #The property types, filtering here greatly reduces the size of the vector
    mutate(X4 = gsub(" ", "", X4))  %>%
    left_join(., postcodes_df,
              by = c("X4"="Postcode"))

  
  test <- prices %>%
    filter(grepl("one hyde", X8, ignore.case = T))
  
  
  test <- prices %>%
    group_by(MSOA11CD) %>%
    summarise(counts = n())
```


#Load low use data

This is the low use data processed in the same way as the 2019 paper.

I should think about whether I need to add more here or not

```{r}

  DATAdf <- read_csv(file.path(basewd, "London_data.csv"))

```

#airbnb data

```{r}

LSOAshapedata <- file.path("~/Dropbox/SSE/Empty Homes","ShapeFiles",
                           "Lower_Layer_Super_Output_Areas_December_2011_Generalised_Clipped__Boundaries_in_England_and_Wales")

airbnb_df <- process_airnbnb_data(LSOAshapedata, postcodes_df, airbnb_csv = file.path(basewd, "airbnb_listings.csv"))

```

#offshore

```{r}
#This file is the output of the Analysing OCOD ipynb
offshore_df <- read_csv(file.path(basewd, "ocod_lsoa.csv")) %>%
  select(-1) %>%
  rename(offshore = '0') %>%
  rename(LSOA11CD = lsoa11cd) %>%
  filter(class2 == "domestic") %>%
  select(LSOA11CD, offshore)


```

#Bind key variables together

This chunk takes the total homes, the empty homes, the offshore homes, the Airbnb homes and the multiple indices of deprivation and binds them together by LSOA.

```{r}
#The data is summarised due to some cross over between local authorities
#This means the LSOA price averages need to be calculated again
all_variables <- postcodes_df %>% select(LSOA11CD, MSOA11CD, LAD11CD, LAD11NM, imd_rank) %>% 
  distinct() %>%
  left_join(  DATAdf %>% 
                select(LSOA11CD, homes = Homes, low_use = LowUse) %>%
                filter(complete.cases(.)) %>%
                group_by(LSOA11CD) %>%
                summarise(across(.fns = sum)) 
  ) %>%
  left_join(airbnb_df) %>%
  left_join(offshore_df) %>% 
  #replaces NA's caused by zero entries with 0
mutate_if(is.numeric,coalesce,0) %>%
  #there are a few straggler LSOA with no homes I don't know why but the number is small so I will ignore it
  #These are removed to not cause issues in the analysis
  filter(homes != 0)

all_variables_lad <- all_variables %>%
  group_by(LAD11CD) %>%
  summarise(homes = sum(homes),
            low_use = sum(low_use),
            offshore = sum(offshore),
            airbnb = sum(airbnb)) %>%
  left_join(postcodes_df %>% select(LAD11CD, LAD11NM) %>% distinct())


#almost no corellation between emptyness and deprivation
cor(all_variables$low_use, all_variables$imd_rank, method = "kendall")

```

#Price adaptor

There is something weird with the prices when they are missing from the geographies

```{r}

price_test <- prices %>%
  filter(LAD11CD=="E09000020") %>%
  group_by(MSOA11CD) %>%
  summarise(counts = n())

length(unique(price_test$LSOA11CD))


test[!(unique(test$LSOA11CD) %in% unique(price_test$LSOA11CD)),]

length(unique(df$MSOA11CD))
test <- DATAdf  %>%
  filter(LAD11CD=="E09000020")

```

#sample prices monte-carlo

```{r}



how_many_zero <- all_variables %>%
  mutate(low_use = low_use ==0,
         airbnb = airbnb==0,
         offshore = offshore==0) %>%
  group_by(MSOA11CD, LAD11CD) %>%
  summarise(low_use = sum(low_use),
            airbnb = sum(airbnb),
            offshore = sum(offshore))  %>%
  group_by(LAD11CD) %>%
  summarise(low_use = sum(low_use),
            airbnb = sum(airbnb),
            offshore = sum(offshore)) %>%
  left_join(all_variables_lad %>% select(-airbnb, -low_use, -offshore))


test_data <- tibble(MSOA11CD = c( "E02000577" ,  "E02000578"# , "E02000579","E02000580", "E02000581"
                                  ), 
                    homes = c(10, 5#, 20, 15, 2
                              ), 
                    low_use = c(0, 4#, 20, 10, 1
                                ),
                    airbnb = c(1, 3#, 6, 4, 1
                               )) %>%
  mutate(LAD11CD = "E09000020")



all_vars_monte <- c("low_use", "airbnb", "offshore") %>% map(~{
  
  file_path <- file.path(basewd, "samples_by_type", paste0(.x, ".csv"))
  
  if(file.exists(file_path)){
    print("already exists loading file")
    temp <- read_rds(file_path)
    
  } else{
    
    temp <- all_variables %>%
      mutate(non_target = homes-.data[[.x]]) %>%
      monte_carlo_stratified_dataset(.,c("non_target", .x), prices, 501, geography_name = "MSOA11CD")
    
    write_rds(temp, file_path)
  }
  
  return(temp)
  
})

all_vars_monte_df <- all_vars_monte %>%
  map_df(~{
    
.x[[1]] %>%
  select(1, 3:5) %>%
  pivot_longer(., cols = 2:3) %>%
  filter(name !="non_target")
    
  }) %>%
  #Total is the value of all property
  bind_rows(all_vars_monte[[1]][[1]] %>%
  select(1, 2:5) %>%
  pivot_longer(., cols = 2:4) %>%
  filter(name =="total"))


all_vars_monte_df %>%
  group_by(LAD11CD, name) %>%
  summarise(mean_value = mean(value)) %>%
  left_join(DATAdf %>% select(LAD11CD, LAD11NM) %>% distinct()) %>%
  ggplot(aes(x = reorder(LAD11NM, mean_value), y = mean_value,
             colour = name, group = name)) + geom_line() +
  labs(title = "Mean value of property classes across London", x = "Local authority in ascending order of property value",
       y = "mean bootstrapped value") +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))
ggsave(file.path(Figures, "mean_price_per_lad.pdf"))

test <-all_vars_monte_df %>%
  group_by(LAD11CD, name) %>%
  summarise(mean_value = mean(value)) %>%
  group_by(LAD11CD) %>%
  mutate(rank_value = rank(mean_value)) 

table(test$name, test$rank_value)

```


```{r}


test <- all_vars_monte_df %>%
  left_join(all_variables_lad %>% ungroup %>%
  rename(total = homes) %>%
  pivot_longer(cols = c(low_use, offshore, airbnb, total), values_to = "counts")
) %>%
  mutate(total_value = counts *value) 

test_per_sim <- test %>%
  group_by(id, name) %>%
  summarise(total_value = sum(total_value),
            counts = sum(counts)) %>%
  mutate(mean_value = total_value/counts)


test_per_sim %>%
  ggplot(aes(x = name, y =  mean_value, fill = name)) + geom_boxplot() +
  labs(x = "property type", y = "mean price per property", title = "Comparison of mean property price by type across 501 simulations")
ggsave(file.path(Figures, "mean_price_london.pdf"))

test_per_sim  %>%
  filter(name != "total") %>%
  ggplot(aes(x = name, y =  total_value, fill = name)) + geom_boxplot() +
  labs(x = "property type", y = "total value by property type", title = "Comparison of total property value by type across 501 simulations")
ggsave(file.path(Figures, "total_value_london.pdf"))


#There are just a lot more low use properties counter balancing the substantially higher value of offshore properties
#However, it should be remembered that offshore properties overlap with low-use properties.
test_per_sim%>% ungroup %>%
  group_by(name) %>%
  summarise(across(.cols = c(-id), .fns = c(mean, median))) 
```

## Entropy

calculating the entropy of the data across the LSOA

```{r}
entropy_val <- function(vect){
  
    probs <- vect/sum(vect)
#output is in bits or shannons
    #change to regular log for output in nats.
    #it really doesn't make any difference
     temp = -probs * log2(probs)
  sum(ifelse(is.na(temp), 0, temp))
}



c("homes", "low_use", "airbnb", "offshore") %>%
  map_df(~{
    
    tibble(type = .x, bits = entropy_val(all_variables[[.x]]))
    
  }) 


#Calculating entropy using boostrap sampling shows that
#The intervall of the entropy is small compared to the difference between
#the mean entropy values
entropy_boot <-1:501 %>%
  map_df(~{
   set.seed(.x)
    temp <- all_variables %>% slice_sample(prop = 1, replace = TRUE)
    
    c("homes", "low_use", "airbnb", "offshore") %>%
  map_df(~{
    
    tibble(type = .x, bits = entropy_val(temp[[.x]]))
    
  }) %>%
      mutate(id = .x)
    
  })


entropy_boot %>%
  ggplot(aes(y = bits, x = type, fill = type)) + geom_boxplot()  +
  labs(x = "property type", y = "mean entropy in shannons", title = "Comparison of mean entropy by property type across 501 simulations")
ggsave(file.path(Figures, "entropy_boxplot_london.pdf"))


```


#check out UPRN etc

```{r}

zip_folder <- "/home/jonno/Downloads"
NSUL_path <-  file.path(zip_folder, "NSUL_JAN_2022.zip")

master <- as.character(unzip(NSUL_path, list = TRUE)$Name)
# load the first file "file1.csv"
data <- read.csv(unz(NSUL_path, "Data/NSUL_JAN_2022_LN.csv"), header = TRUE,
                 sep = ",") 

```


#plot map of London
```{r}
{
  LSOAshape <- st_read(LSOAshapedata) %>%
  filter(lsoa11cd %in% postcodes_df$LSOA11CD) 

  LSOAshape <- LSOAshape %>%
    left_join(c("offshore", "low_use") %>%
                map_df(~{
                  
                  tibble(lsoa11cd = LSOAshape$lsoa11cd, name = .x)
                  
                }))
  
  #Population estimates may be out of date. Newham has more offshore properties than homes.
  
  LSOAshape <- all_variables %>% 
    group_by(LSOA11CD) %>%
  summarise(across(.cols = homes:offshore, .fns = sum)) %>%
              # mutate(low_use = (low_use/homes)) %>%
              #        offshore = (offshore/sum(offshore)) ,
              #        airbnb = (airbnb/sum(airbnb))) %>%
  select(lsoa11cd = LSOA11CD, homes:offshore) %>%
  pivot_longer(., cols = c(homes,low_use, airbnb, offshore)) %>%
  full_join(LSOAshape, .) %>%
  group_by(name) 
  
  
  
}


MSOAshapedata <- "~/Dropbox/SSE/Empty Homes/ShapeFiles/Middle_Layer_Super_Output_Areas_December_2011_Super_Generalised_Clipped_Boundaries_in_England_and_Wales"
{
  MSOAshape <- st_read(MSOAshapedata) %>%
  filter(msoa11cd %in% postcodes_df$MSOA11CD) 

  MSOAshape <- MSOAshape %>%
  left_join(c("homes","offshore", "airbnb", "low_use") %>%
  map_df(~{
    
    tibble(msoa11cd = MSOAshape$msoa11cd, name = .x)
    
    
    
  }))
  
  #Population estimates may be out of date. Newham has more offshore properties than homes.
MSOAshape <- all_variables %>% 
  group_by(MSOA11CD) %>%
  summarise(across(.cols = homes:offshore, .fns = sum)) %>%
              # mutate(low_use = (low_use/homes)) %>%
              #        offshore = (offshore/sum(offshore)) ,
              #        airbnb = (airbnb/sum(airbnb))) %>%
  select(msoa11cd = MSOA11CD, homes:offshore) %>%
  pivot_longer(., cols = c(homes,low_use, airbnb, offshore)) %>%
  full_join(MSOAshape, .) %>%
  group_by(name) %>%
  mutate(
   # perc = value/homes,
    value2 = log10(value/sum(value)),
    non_zero = unique(sort(value2,partial=2))[2],
    value2 = ifelse(!is.finite(value2), non_zero, value2),
    value3 = (value/sum(value))*log(value),
    value3 = ifelse(is.finite(value3),value3, 0))
}







MSOAshape %>%
  filter(name =="low_use") %>%
ggplot() +
   geom_sf(aes(fill = value),  colour = NA, lwd = 0, pch = 0) +
  facet_wrap(facets = ~name, nrow = 2) +
  scale_fill_viridis_c() +
  labs(title = "The distribution of the property types across London log base 10 scale", 
       fill = "log10(P)")
ggsave(file.path(Figures, "property_maps.pdf"))

```

#Moran's I

Spatial auto-correlation

```{r}

morans_df <-unique(MSOAshape$name) %>%
  map_df(~{
    
    target_df <- MSOAshape %>% filter(name == .x) %>%
      mutate(value = ifelse(is.na(value), 0, value))
    
    nb <- target_df %>% poly2nb(., queen=TRUE)
    
    lw <- nb2listw(nb, style="W", zero.policy=TRUE) 
    
    MC<- moran.mc(target_df$value, lw, nsim=10000, alternative="greater")
    
    tibble(name = .x, morans_I = MC$statistic, p_value = MC$p.value )
    
  })


morans_df

#airbnb shows the strongest spatial auto-corellation
#this is despite the high levels of concetration of offshore property.
#This difference may be becuase offshore has two main drivers
#properties owned by single individual
#property portfolios such as housing estates.
#The separation of these two types could reveal distinct behaviours and 
#prices.

```

# comparing prices for nested addresses and normal

```{r}

offshore_df_nested <- read_csv(file.path(basewd, "ocod_lsoa_by_nested_type.csv")) %>%
  select(-1) %>%
  rename(offshore = '0') %>%
  rename(LSOA11CD = lsoa11cd) %>%
  filter(class2 == "domestic") %>%
  select(LSOA11CD, offshore, nested = within_larger_title) %>%
  mutate(nested = ifelse(nested, "nested", "normal")) %>%
  group_by(LSOA11CD, nested) %>%
  summarise(offshore = sum(offshore)) %>%
  pivot_wider(id_cols = LSOA11CD, names_from = nested, values_from = offshore, values_fill = 0) %>%
  ungroup


all_variables_nested <-postcodes_df %>% select(LSOA11CD, MSOA11CD, LAD11CD, LAD11NM, imd_rank) %>% 
  distinct() %>%
  left_join(  DATAdf %>% 
                select(LSOA11CD, homes = Homes, low_use = LowUse) %>%
                filter(complete.cases(.)) %>%
                group_by(LSOA11CD) %>%
                summarise(across(.fns = sum)) 
  ) %>%
  left_join(offshore_df_nested) %>% 
  #replaces NA's caused by zero entries with 0
mutate_if(is.numeric,coalesce,0) %>%
  #there are a few straggler LSOA with no homes I don't know why but the number is small so I will ignore it
  #These are removed to not cause issues in the analysis
  mutate(homes = normal + nested) %>%
  filter(homes != 0) 

all_variables_nested_lad <- all_variables_nested %>%
  group_by(LAD11CD) %>%
  summarise(homes = sum(homes),
            nested = sum(nested),
            normal = sum(normal)) %>%
  left_join(postcodes_df %>% select(LAD11CD, LAD11NM) %>% distinct())


all_vars_monte_nested <- c("nested") %>% map(~{
  
  file_path <- file.path(basewd, "samples_by_type", paste0(.x, ".csv"))
  
  if(file.exists(file_path)){
    print("already exists loading file")
    temp <- read_rds(file_path)
    
  } else{
    
    temp <- all_variables_nested %>%
      mutate(non_target = homes-.data[[.x]]) %>%
      monte_carlo_stratified_dataset(.,c("non_target", .x), prices, 5001, geography_name = "MSOA11CD")
    
    write_rds(temp, file_path)
  }
  
  return(temp)
  
})


all_vars_monte_nested_df <- all_vars_monte_nested %>%
  map_df(~{
    
.x[[1]] %>%
  select(1, 3:5) %>%
  pivot_longer(., cols = 2:3) %>%
  filter(name !="non_target")
    
  }) %>%
  #Total is the value of all property
  bind_rows(all_vars_monte_nested[[1]][[1]] %>%
  select(1, 2:5) %>%
  pivot_longer(., cols = 2:4) %>%
  filter(name =="non_target")) %>%
  mutate(name = ifelse(name =="non_target", "normal", name)) %>%
  left_join(all_variables_nested_lad %>% ungroup %>%
  rename(total = homes) %>%
  pivot_longer(cols = c( normal, nested, total), values_to = "counts")
) %>%
  mutate(total_value = counts *value) %>%
  group_by(id, name) %>%
  summarise(
    counts = sum(counts),
     value = sum(value),
    total_value = sum(total_value)) %>%
  mutate(mean_value = total_value/counts)


#There is not any overlap between the two types they are very different in terms of price
all_vars_monte_nested_df %>%
  ggplot(aes(x = mean_value, colour = name)) + geom_density()

```


## entropy

```{r}


c("nested", "normal") %>%
  map_df(~{
    
    tibble(type = .x, bits = entropy_val(all_variables_nested[[.x]]))
    
  }) 

```

## Morans I

```{r}
{
  MSOAshape_nested <- st_read(MSOAshapedata) %>%
  filter(msoa11cd %in% postcodes_df$MSOA11CD) 

  MSOAshape_nested <- MSOAshape_nested %>%
  left_join(c("normal", "nested") %>%
  map_df(~{
    
    tibble(msoa11cd = MSOAshape_nested$msoa11cd, name = .x)
    
    
    
  }))
  
  #Population estimates may be out of date. Newham has more offshore properties than homes.
MSOAshape_nested <- all_variables_nested %>% 
  group_by(MSOA11CD) %>%
  summarise(across(.cols = c(homes, low_use, normal, nested), .fns = sum)) %>%
              # mutate(low_use = (low_use/homes)) %>%
              #        offshore = (offshore/sum(offshore)) ,
              #        airbnb = (airbnb/sum(airbnb))) %>%
  select(msoa11cd = MSOA11CD, homes, low_use, normal, nested) %>%
  pivot_longer(., cols = c(homes, nested, normal)) %>%
  full_join(MSOAshape_nested, .) 
}



morans_df_nested <-c("normal", "nested") %>%
  map_df(~{
    
    target_df <- MSOAshape_nested %>% filter(name == .x) %>%
      mutate(value = ifelse(is.na(value), 0, value)) 
    
    nb <- target_df %>% poly2nb(., queen=TRUE)
    
    lw <- nb2listw(nb, style="W", zero.policy=TRUE) 
    
    MC<- moran.mc(target_df$value, lw, nsim=5001, alternative="greater")
    
    tibble(name = .x, morans_I = MC$statistic, p_value = MC$p.value )
    
  })


morans_df_nested


```

##nested conclusion

The nested addresses are substantially cheaper and more concentrated than the normal offshore properties. They also have almost no spatial auto-corellation whilst the normal offshore has high spatial auto corellation. This points to the different natures of the property types. Nested properties represent portfolios within or consisting of a housing development whilst, the onormal offshore property is not and more likely to be owned by an individual.


# ocod cleaned expanded tests

```{r}

test <- read_csv(file.path(basewd, "OCOD_cleaned_expanded.csv")) 

test <- test %>%
  filter(within_title_id==1)

ocod_base = read_csv(file.path(basewd, "OCOD_FULL_2022_02.csv")) 

ocod_ex <- ocod_base %>%
  filter(`Title Number`=="CB400630")


voa_names <- make.names(c("Incrementing Entry Number", "Billing Authority Code", "NDR Community Code", 
 "BA Reference Number", "Primary And Secondary Description Code", "Primary Description Text",
"Unique Address Reference Number UARN", "Full Property Identifier", "Firms Name", "Number Or Name",
"Street", "Town", "Postal District", "County", "Postcode", "Effective Date", "Composite Indicator",
 "Rateable Value", "Appeal Settlement Code", "Assessment Reference", "List Alteration Date", "SCAT Code And Suffix",
 "Sub Street level 3", "Sub Street level 2", "Sub Street level 1", "Case Number", 
 "Current From Date", "Current To Date")) %>% tolower()


voa_base <- read_delim(file.path(basewd,  'uk-englandwales-ndr-2017-listentries-compiled-epoch-0029-baseline-csv.csv'), delim = "*",
                       col_names = voa_names)



voa <- voa_base %>%
  filter(tolower(street)  =="holloway road") %>%
  filter(town =="LONDON") %>%
  filter(grepl("422", number.or.name))

table(grepl("ADVERTISING", voa_base$primary.description.text))

```

