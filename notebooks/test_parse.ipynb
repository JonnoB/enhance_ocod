{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 99 OCOD history files.\n",
      "Loading common reference data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/teamspace/studios/this_studio/enhance_ocod/src/enhance_ocod/address_parsing.py:35: DtypeWarning: Columns (18,31,39,44,51) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  postcode_district_lookup = pd.read_csv(f)[['pcds', 'oslaua', 'oa11', 'lsoa11', 'msoa11', 'ctry']]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial rows: 2282385\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from enhance_ocod.inference import parse_addresses_pipeline, convert_to_entity_dataframe\n",
    "from enhance_ocod.address_parsing import (\n",
    "    load_and_prep_OCOD_data, parsing_and_expansion_process, post_process_expanded_data, load_postcode_district_lookup)\n",
    "from enhance_ocod.locate_and_classify import (preprocess_expandaded_ocod_data, \n",
    "    add_missing_lads_ocod, load_voa_ratinglist, street_and_building_matching, substreet_matching,\n",
    "    counts_of_businesses_per_oa_lsoa, voa_address_match_all_data, classification_type1, classification_type2,\n",
    "    contract_ocod_after_classification\n",
    ")\n",
    "from enhance_ocod.price_paid_process import load_and_process_pricepaid_data\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import gc  # Add for memory management\n",
    "\n",
    "import pickle\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "# There is a warning related to bfill and ffill which is basically internal to pandas so silencing here\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', message='.*Downcasting object dtype arrays.*')\n",
    "\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "SCRIPT_DIR = Path('..').parent.absolute()\n",
    "\n",
    "# ====== CONSTANT PATHS AND SETTINGS ======\n",
    "input_dir = SCRIPT_DIR.parent / \"data\" / \"ocod_history\"\n",
    "output_dir = SCRIPT_DIR.parent / \"data\" / \"ocod_history_processed2\"\n",
    "model_path = SCRIPT_DIR.parent / \"models\" / \"address_parser_original_fullset\" / \"final_model\"\n",
    "ONSPD_path = SCRIPT_DIR.parent / \"data\" / \"ONSPD_FEB_2025.zip\"\n",
    "price_paid_path = SCRIPT_DIR.parent / \"data\" / \"price_paid_data\" / \"price_paid_complete_may_2025.csv\"\n",
    "processed_price_paid_dir = SCRIPT_DIR.parent / \"data\" / \"processed_price_paid\"\n",
    "voa_path = SCRIPT_DIR.parent / \"data\" / \"2023_non_domestic_rating_list_entries.zip\"\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "parsed_results_dir = SCRIPT_DIR.parent / \"data\" / \"parsed_ocod_dicts\"\n",
    "parsed_results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# List of all zip files in input_dir\n",
    "#\n",
    "# TESTING!!! only 10 files!\n",
    "#\n",
    "all_files = sorted([f for f in input_dir.glob(\"OCOD_FULL_*.zip\")])\n",
    "\n",
    "\n",
    "#test_indices = [0, 25, 50, 75]\n",
    "#all_files = [all_files[i] for i in test_indices if i < len(all_files)]\n",
    "print(f\"Found {len(all_files)} OCOD history files.\")\n",
    "\n",
    "# Load common data once (if these don't change between files)\n",
    "print(\"Loading common reference data...\")\n",
    "postcode_district_lookup = load_postcode_district_lookup(str(ONSPD_path))\n",
    "voa_businesses = load_voa_ratinglist(str(voa_path), postcode_district_lookup)\n",
    "\n",
    "'../data/ocod_history_processed/OCOD_FULL_2022_02.parquet'\n",
    "zip_file = input_dir / \"OCOD_FULL_2022_02.zip\"\n",
    "out_name = zip_file.stem + \".parquet\"\n",
    "out_path = output_dir / out_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define parsed results file path\n",
    "parsed_results_file = parsed_results_dir / f\"{zip_file.stem}_parsed_results.pkl\"\n",
    "\n",
    "if out_path.exists():\n",
    "    print(f\"Skipping {zip_file.name}: already processed.\")\n",
    "    continue\n",
    "\n",
    "print(f\"Processing {zip_file.name}...\")\n",
    "\n",
    "# Load and process the OCOD data\n",
    "ocod_data = load_and_prep_OCOD_data(str(zip_file))\n",
    "\n",
    "###############\n",
    "# Parse addresses\n",
    "###############\n",
    "if parsed_results_file.exists():\n",
    "    print(f\"Loading cached parsing results for {zip_file.name}...\")\n",
    "    with open(parsed_results_file, 'rb') as f:\n",
    "        results = pickle.load(f)\n",
    "    print(f\"Loaded cached results with success rate: {results['summary']['success_rate']:.1%}\")\n",
    "else:\n",
    "    print(f\"Parsing addresses for {zip_file.name}...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    results = parse_addresses_pipeline(\n",
    "        df=ocod_data,\n",
    "        short_batch_size = 128,# The default seems really slow, might be to do with loading not sure\n",
    "        model_path=str(model_path),\n",
    "        target_column=\"property_address\",\n",
    "    )\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Address parsing took {end_time - start_time:.2f} seconds\")\n",
    "    print(f\"Success rate: {results['summary']['success_rate']:.1%}\")\n",
    "    \n",
    "    # Save parsing results\n",
    "    print(f\"Saving parsing results to {parsed_results_file}...\")\n",
    "    with open(parsed_results_file, 'wb') as f:\n",
    "        pickle.dump(results, f)\n",
    "\n",
    "# Continue with post-parsing processing\n",
    "test = convert_to_entity_dataframe(results)\n",
    "test = parsing_and_expansion_process(all_entities=test)\n",
    "ocod_data = post_process_expanded_data(test, ocod_data)\n",
    "\n",
    "# Clean up\n",
    "del results, test\n",
    "gc.collect()\n",
    "\n",
    "###############\n",
    "# Geolocate\n",
    "###############\n",
    "print(f\"Geolocating {zip_file.name}...\")\n",
    "\n",
    "ocod_data = preprocess_expandaded_ocod_data(ocod_data, postcode_district_lookup)\n",
    "\n",
    "price_paid_df = load_and_process_pricepaid_data(\n",
    "    file_path=str(price_paid_path), \n",
    "    processed_dir=processed_price_paid_dir,\n",
    "    postcode_district_lookup=postcode_district_lookup, \n",
    "    years_needed=[2017, 2018, 2019]\n",
    ")\n",
    "\n",
    "ocod_data = add_missing_lads_ocod(ocod_data, price_paid_df)\n",
    "ocod_data = street_and_building_matching(ocod_data, price_paid_df, voa_businesses)\n",
    "ocod_data = substreet_matching(ocod_data, price_paid_df, voa_businesses)\n",
    "\n",
    "# Clean up price paid data\n",
    "del price_paid_df\n",
    "gc.collect()\n",
    "\n",
    "###########\n",
    "# Classify\n",
    "###########\n",
    "print(f\"Classifying {zip_file.name}...\")\n",
    "ocod_data = counts_of_businesses_per_oa_lsoa(ocod_data, voa_businesses)\n",
    "ocod_data = voa_address_match_all_data(ocod_data, voa_businesses)\n",
    "\n",
    "ocod_data = classification_type1(ocod_data)\n",
    "ocod_data = classification_type2(ocod_data)\n",
    "\n",
    "ocod_data = contract_ocod_after_classification(ocod_data, class_type='class2', classes=['residential'])\n",
    "\n",
    "columns = ['title_number', 'within_title_id', 'within_larger_title', 'unique_id', \n",
    "            'unit_id', 'unit_type', 'building_name', 'street_number', 'street_name', \n",
    "            'postcode', 'city', 'district', 'region', 'property_address', 'oa11cd', \n",
    "            'lsoa11cd', 'msoa11cd', 'lad11cd','country_incorporated' ,'class', 'class2']\n",
    "\n",
    "ocod_data = ocod_data.loc[:, columns].rename(columns={\n",
    "    'within_title_id': 'nested_id',\n",
    "    'within_larger_title': 'nested_title'\n",
    "})\n",
    "# Save results\n",
    "ocod_data.to_parquet(out_path)\n",
    "print(f\"Saved processed data to {out_path}\")\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "print(\"All files process"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
