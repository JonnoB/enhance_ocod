{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A lot of chaos has happened this ipynb is supposed to work it out and re-create the weakly labelled data\n",
    "\n",
    "At the end I should be able to create a clearly labelled and docstringed .py file and delete this\n",
    "\n",
    "Then I will need to \n",
    "- re-train model\n",
    "- re-tesst results\n",
    "- re-choose model\n",
    "- re-parse everything again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 99 OCOD history files.\n",
      "Loading common reference data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/teamspace/studios/this_studio/enhance_ocod/src/enhance_ocod/address_parsing.py:35: DtypeWarning: Columns (18,31,39,44,51) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  postcode_district_lookup = pd.read_csv(f)[['pcds', 'oslaua', 'oa11', 'lsoa11', 'msoa11', 'ctry']]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial rows: 2282385\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from enhance_ocod.inference import parse_addresses_pipeline, convert_to_entity_dataframe\n",
    "from enhance_ocod.address_parsing import (\n",
    "    load_and_prep_OCOD_data, parsing_and_expansion_process, post_process_expanded_data, load_postcode_district_lookup)\n",
    "from enhance_ocod.locate_and_classify import (preprocess_expandaded_ocod_data, \n",
    "    add_missing_lads_ocod, load_voa_ratinglist, street_and_building_matching, substreet_matching,\n",
    "    counts_of_businesses_per_oa_lsoa, voa_address_match_all_data, classification_type1, classification_type2,\n",
    "    contract_ocod_after_classification\n",
    ")\n",
    "from enhance_ocod.price_paid_process import load_and_process_pricepaid_data\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import gc  # Add for memory management\n",
    "\n",
    "import pickle\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "# There is a warning related to bfill and ffill which is basically internal to pandas so silencing here\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', message='.*Downcasting object dtype arrays.*')\n",
    "\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "SCRIPT_DIR = Path('..').parent.absolute()\n",
    "\n",
    "# ====== CONSTANT PATHS AND SETTINGS ======\n",
    "input_dir = SCRIPT_DIR.parent / \"data\" / \"ocod_history\"\n",
    "output_dir = SCRIPT_DIR.parent / \"data\" / \"ocod_history_processed2\"\n",
    "model_path = SCRIPT_DIR.parent / \"models\" / \"address_parser_original_fullset\" / \"final_model\"\n",
    "ONSPD_path = SCRIPT_DIR.parent / \"data\" / \"ONSPD_FEB_2025.zip\"\n",
    "price_paid_path = SCRIPT_DIR.parent / \"data\" / \"price_paid_data\" / \"price_paid_complete_may_2025.csv\"\n",
    "processed_price_paid_dir = SCRIPT_DIR.parent / \"data\" / \"processed_price_paid\"\n",
    "voa_path = SCRIPT_DIR.parent / \"data\" / \"2023_non_domestic_rating_list_entries.zip\"\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "parsed_results_dir = SCRIPT_DIR.parent / \"data\" / \"parsed_ocod_dicts\"\n",
    "parsed_results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# List of all zip files in input_dir\n",
    "#\n",
    "# TESTING!!! only 10 files!\n",
    "#\n",
    "all_files = sorted([f for f in input_dir.glob(\"OCOD_FULL_*.zip\")])\n",
    "\n",
    "\n",
    "#test_indices = [0, 25, 50, 75]\n",
    "#all_files = [all_files[i] for i in test_indices if i < len(all_files)]\n",
    "print(f\"Found {len(all_files)} OCOD history files.\")\n",
    "\n",
    "# Load common data once (if these don't change between files)\n",
    "print(\"Loading common reference data...\")\n",
    "postcode_district_lookup = load_postcode_district_lookup(str(ONSPD_path))\n",
    "voa_businesses = load_voa_ratinglist(str(voa_path), postcode_district_lookup)\n",
    "\n",
    "'../data/ocod_history_processed/OCOD_FULL_2022_02.parquet'\n",
    "zip_file = input_dir / \"OCOD_FULL_2022_02.zip\"\n",
    "out_name = zip_file.stem + \".parquet\"\n",
    "out_path = output_dir / out_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing OCOD_FULL_2022_02.zip...\n",
      "Loading cached parsing results for OCOD_FULL_2022_02.zip...\n",
      "Loaded cached results with success rate: 100.0%\n",
      "Processing 421,285 entities into DataFrame...\n",
      "Processed 350,000/421,285 entities\n",
      "Computing label counts...\n",
      "âœ“ Named Entity Recognition processing complete\n",
      "Total entities extracted: 421,285\n",
      "Geolocating OCOD_FULL_2022_02.zip...\n",
      "Processed data found. Skipping preprocessing.\n",
      "Loaded 1067153 records for year 2017\n",
      "Loaded 1037132 records for year 2018\n",
      "Loaded 1011344 records for year 2019\n",
      "lad 100 of 280\n",
      "lad 200 of 280\n",
      "Classifying OCOD_FULL_2022_02.zip...\n",
      "Processing 318 LADs\n",
      "Creating street match lookup...\n",
      "Creating address lookup...\n",
      "Performing address matching...\n",
      "Saved processed data to /teamspace/studios/this_studio/enhance_ocod/data/ocod_history_processed2/OCOD_FULL_2022_02.parquet\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Define parsed results file path\n",
    "parsed_results_file = parsed_results_dir / f\"{zip_file.stem}_parsed_results.pkl\"\n",
    "\n",
    "\n",
    "print(f\"Processing {zip_file.name}...\")\n",
    "\n",
    "# Load and process the OCOD data\n",
    "ocod_data = load_and_prep_OCOD_data(str(zip_file))\n",
    "\n",
    "###############\n",
    "# Parse addresses\n",
    "###############\n",
    "if parsed_results_file.exists():\n",
    "    print(f\"Loading cached parsing results for {zip_file.name}...\")\n",
    "    with open(parsed_results_file, 'rb') as f:\n",
    "        results = pickle.load(f)\n",
    "    print(f\"Loaded cached results with success rate: {results['summary']['success_rate']:.1%}\")\n",
    "else:\n",
    "    print(f\"Parsing addresses for {zip_file.name}...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    results = parse_addresses_pipeline(\n",
    "        df=ocod_data,\n",
    "        short_batch_size = 128,# The default seems really slow, might be to do with loading not sure\n",
    "        model_path=str(model_path),\n",
    "        target_column=\"property_address\",\n",
    "    )\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Address parsing took {end_time - start_time:.2f} seconds\")\n",
    "    print(f\"Success rate: {results['summary']['success_rate']:.1%}\")\n",
    "    \n",
    "    # Save parsing results\n",
    "    print(f\"Saving parsing results to {parsed_results_file}...\")\n",
    "    with open(parsed_results_file, 'wb') as f:\n",
    "        pickle.dump(results, f)\n",
    "\n",
    "# Continue with post-parsing processing\n",
    "test = convert_to_entity_dataframe(results)\n",
    "test = parsing_and_expansion_process(all_entities=test)\n",
    "ocod_data = post_process_expanded_data(test, ocod_data)\n",
    "\n",
    "# Clean up\n",
    "del results, test\n",
    "gc.collect()\n",
    "\n",
    "###############\n",
    "# Geolocate\n",
    "###############\n",
    "print(f\"Geolocating {zip_file.name}...\")\n",
    "\n",
    "ocod_data = preprocess_expandaded_ocod_data(ocod_data, postcode_district_lookup)\n",
    "\n",
    "price_paid_df = load_and_process_pricepaid_data(\n",
    "    file_path=str(price_paid_path), \n",
    "    processed_dir=processed_price_paid_dir,\n",
    "    postcode_district_lookup=postcode_district_lookup, \n",
    "    years_needed=[2017, 2018, 2019]\n",
    ")\n",
    "\n",
    "ocod_data = add_missing_lads_ocod(ocod_data, price_paid_df)\n",
    "ocod_data = street_and_building_matching(ocod_data, price_paid_df, voa_businesses)\n",
    "ocod_data = substreet_matching(ocod_data, price_paid_df, voa_businesses)\n",
    "\n",
    "# Clean up price paid data\n",
    "del price_paid_df\n",
    "gc.collect()\n",
    "\n",
    "###########\n",
    "# Classify\n",
    "###########\n",
    "print(f\"Classifying {zip_file.name}...\")\n",
    "ocod_data = counts_of_businesses_per_oa_lsoa(ocod_data, voa_businesses)\n",
    "ocod_data = voa_address_match_all_data(ocod_data, voa_businesses)\n",
    "\n",
    "ocod_data = classification_type1(ocod_data)\n",
    "ocod_data = classification_type2(ocod_data)\n",
    "\n",
    "ocod_data = contract_ocod_after_classification(ocod_data, class_type='class2', classes=['residential'])\n",
    "\n",
    "columns = ['title_number', 'within_title_id', 'within_larger_title', 'unique_id', \n",
    "            'unit_id', 'unit_type', 'building_name', 'street_number', 'street_name', \n",
    "            'postcode', 'city', 'district', 'region', 'property_address', 'oa11cd', \n",
    "            'lsoa11cd', 'msoa11cd', 'lad11cd','country_incorporated' ,'class', 'class2']\n",
    "\n",
    "ocod_data = ocod_data.loc[:, columns].rename(columns={\n",
    "    'within_title_id': 'nested_id',\n",
    "    'within_larger_title': 'nested_title'\n",
    "})\n",
    "# Save results\n",
    "ocod_data.to_parquet(out_path)\n",
    "print(f\"Saved processed data to {out_path}\")\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2022/909859672.py:6: DtypeWarning: Columns (24,28,30,32,33,34) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(zip_file)\n"
     ]
    }
   ],
   "source": [
    "from enhance_ocod.labelling.weak_labelling import process_dataframe_batch, convert_weak_labels_to_standard_format, remove_overlapping_spans\n",
    "\n",
    "zip_file = input_dir / \"OCOD_FULL_2022_02.zip\"\n",
    "\n",
    "\n",
    "df = pd.read_csv(zip_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'text': 'Ground to ninth Floor Flats being 101-114, 201-214, 301-314, 401-414, 501-514, 601-613 and 701-704 Alaska Building, 101-114, 201-214,301-314, 401-412, 501-506 and 601-605 Arizona Building, 101-114, 201-214, 301-314, 401-414, 501-514, 601-614, 701-708, 801-804, 901-903 California Building, 101-108,     201-208, 301-307, 401-408, 501-508, 601-608, 701-708, 801-808 and 901-903 Colorado Building, 1-4, 101-109, 201-210, 301-310, 401-410, 501-510 and 601-605 Dakota Building, 1-7, 101-108, 201-208, 301-308, 401-408, 501-506 and 601-604 Idaho Building, 102-112, 201-212, 301-312, 401-412, 501-508 and 601-604 Indiana Building, 1-15, 101-116, 201-216, 301-315, 401-416, 501-510 Montana Building, 101-108, 201-208, 301-308, 401-408, 501-506 and 601-604 Nebraska Building, 1-10, 101-110, 201-210, 301-310 and 402-403 Utah Building, 1-10 and 101-110 Boston Building, 1-6, 101-106, 201-206, 301-306, 401-408 and 501-507 Madison Building, Deals Gateway, London', 'spans': [{'text': 'Alaska Building', 'start': 99, 'end': 114, 'label': 'building_name', 'function': 'building_regex_fn'}, {'text': 'Arizona Building', 'start': 171, 'end': 187, 'label': 'building_name', 'function': 'building_regex_fn'}, {'text': 'California Building', 'start': 269, 'end': 288, 'label': 'building_name', 'function': 'building_regex_fn'}, {'text': 'Colorado Building', 'start': 377, 'end': 394, 'label': 'building_name', 'function': 'building_regex_fn'}, {'text': 'Dakota Building', 'start': 457, 'end': 472, 'label': 'building_name', 'function': 'building_regex_fn'}, {'text': 'Idaho Building', 'start': 535, 'end': 549, 'label': 'building_name', 'function': 'building_regex_fn'}, {'text': 'Indiana Building', 'start': 607, 'end': 623, 'label': 'building_name', 'function': 'building_regex_fn'}, {'text': 'Montana Building', 'start': 675, 'end': 691, 'label': 'building_name', 'function': 'building_regex_fn'}, {'text': 'Nebraska Building', 'start': 749, 'end': 766, 'label': 'building_name', 'function': 'building_regex_fn'}, {'text': 'Utah Building', 'start': 812, 'end': 825, 'label': 'building_name', 'function': 'building_regex_fn'}, {'text': 'Boston Building', 'start': 844, 'end': 859, 'label': 'building_name', 'function': 'building_regex_fn'}, {'text': 'Madison Building', 'start': 913, 'end': 929, 'label': 'building_name', 'function': 'building_regex_fn'}, {'text': 'London', 'start': 946, 'end': 952, 'label': 'city', 'function': 'city_match2'}, {'text': 'Deals Gateway', 'start': 931, 'end': 944, 'label': 'street_name', 'function': 'road_names_basic'}, {'text': 'Deals Gateway', 'start': 931, 'end': 944, 'label': 'street_name', 'function': 'words_waygate_fn'}, {'text': '701', 'start': 91, 'end': 94, 'label': 'unit_id', 'function': 'and_space_n_flats'}, {'text': '601', 'start': 163, 'end': 166, 'label': 'unit_id', 'function': 'and_space_n_flats'}, {'text': '901', 'start': 369, 'end': 372, 'label': 'unit_id', 'function': 'and_space_n_flats'}, {'text': '601', 'start': 449, 'end': 452, 'label': 'unit_id', 'function': 'and_space_n_flats'}, {'text': '601', 'start': 527, 'end': 530, 'label': 'unit_id', 'function': 'and_space_n_flats'}, {'text': '601', 'start': 599, 'end': 602, 'label': 'unit_id', 'function': 'and_space_n_flats'}, {'text': '601', 'start': 741, 'end': 744, 'label': 'unit_id', 'function': 'and_space_n_flats'}, {'text': '402', 'start': 804, 'end': 807, 'label': 'unit_id', 'function': 'and_space_n_flats'}, {'text': '101', 'start': 836, 'end': 839, 'label': 'unit_id', 'function': 'and_space_n_flats'}, {'text': '501', 'start': 905, 'end': 908, 'label': 'unit_id', 'function': 'and_space_n_flats'}, {'text': '101-114', 'start': 34, 'end': 41, 'label': 'unit_id', 'function': 'xx_to_yy_flat'}, {'text': '201-214', 'start': 43, 'end': 50, 'label': 'unit_id', 'function': 'xx_to_yy_flat'}, {'text': '301-314', 'start': 52, 'end': 59, 'label': 'unit_id', 'function': 'xx_to_yy_flat'}, {'text': '401-414', 'start': 61, 'end': 68, 'label': 'unit_id', 'function': 'xx_to_yy_flat'}, {'text': '501-514', 'start': 70, 'end': 77, 'label': 'unit_id', 'function': 'xx_to_yy_flat'}, {'text': '601-613', 'start': 79, 'end': 86, 'label': 'unit_id', 'function': 'xx_to_yy_flat'}, {'text': '701-704', 'start': 91, 'end': 98, 'label': 'unit_id', 'function': 'xx_to_yy_flat'}, {'text': '101-114', 'start': 116, 'end': 123, 'label': 'unit_id', 'function': 'xx_to_yy_flat'}, {'text': '201-214', 'start': 125, 'end': 132, 'label': 'unit_id', 'function': 'xx_to_yy_flat'}, {'text': '301-314', 'start': 133, 'end': 140, 'label': 'unit_id', 'function': 'xx_to_yy_flat'}, {'text': '401-412', 'start': 142, 'end': 149, 'label': 'unit_id', 'function': 'xx_to_yy_flat'}, {'text': '501-506', 'start': 151, 'end': 158, 'label': 'unit_id', 'function': 'xx_to_yy_flat'}, {'text': '601-605', 'start': 163, 'end': 170, 'label': 'unit_id', 'function': 'xx_to_yy_flat'}, {'text': '101-114', 'start': 189, 'end': 196, 'label': 'unit_id', 'function': 'xx_to_yy_flat'}, {'text': '201-214', 'start': 198, 'end': 205, 'label': 'unit_id', 'function': 'xx_to_yy_flat'}, {'text': '301-314', 'start': 207, 'end': 214, 'label': 'unit_id', 'function': 'xx_to_yy_flat'}, {'text': '401-414', 'start': 216, 'end': 223, 'label': 'unit_id', 'function': 'xx_to_yy_flat'}, {'text': '501-514', 'start': 225, 'end': 232, 'label': 'unit_id', 'function': 'xx_to_yy_flat'}, {'text': '601-614', 'start': 234, 'end': 241, 'label': 'unit_id', 'function': 'xx_to_yy_flat'}, {'text': '701-708', 'start': 243, 'end': 250, 'label': 'unit_id', 'function': 'xx_to_yy_flat'}, {'text': '801-804', 'start': 252, 'end': 259, 'label': 'unit_id', 'function': 'xx_to_yy_flat'}, {'text': '901-903', 'start': 261, 'end': 268, 'label': 'unit_id', 'function': 'xx_to_yy_flat'}, {'text': '101-108', 'start': 290, 'end': 297, 'label': 'unit_id', 'function': 'xx_to_yy_flat'}, {'text': '201-208', 'start': 303, 'end': 310, 'label': 'unit_id', 'function': 'xx_to_yy_flat'}, {'text': '301-307', 'start': 312, 'end': 319, 'label': 'unit_id', 'function': 'xx_to_yy_flat'}, {'text': '401-408', 'start': 321, 'end': 328, 'label': 'unit_id', 'function': 'xx_to_yy_flat'}, {'text': '501-508', 'start': 330, 'end': 337, 'label': 'unit_id', 'function': 'xx_to_yy_flat'}, {'text': '601-608', 'start': 339, 'end': 346, 'label': 'unit_id', 'function': 'xx_to_yy_flat'}, {'text': '701-708', 'start': 348, 'end': 355, 'label': 'unit_id', 'function': 'xx_to_yy_flat'}, {'text': '801-808', 'start': 357, 'end': 364, 'label': 'unit_id', 'function': 'xx_to_yy_flat'}, {'text': '901-903', 'start': 369, 'end': 376, 'label': 'unit_id', 'function': 'xx_to_yy_flat'}, {'text': '1-4', 'start': 396, 'end': 399, 'label': 'unit_id', 'function': 'xx_to_yy_flat'}, {'text': '101-109', 'start': 401, 'end': 408, 'label': 'unit_id', 'function': 'xx_to_yy_flat'}, {'text': '201-210', 'start': 410, 'end': 417, 'label': 'unit_id', 'function': 'xx_to_yy_flat'}, {'text': '301-310', 'start': 419, 'end': 426, 'label': 'unit_id', 'function': 'xx_to_yy_flat'}, {'text': '401-410', 'start': 428, 'end': 435, 'label': 'unit_id', 'function': 'xx_to_yy_flat'}, {'text': '501-510', 'start': 437, 'end': 444, 'label': 'unit_id', 'function': 'xx_to_yy_flat'}, {'text': '601-605', 'start': 449, 'end': 456, 'label': 'unit_id', 'function': 'xx_to_yy_flat'}, {'text': '1-7', 'start': 474, 'end': 477, 'label': 'unit_id', 'function': 'xx_to_yy_flat'}, {'text': '101-108', 'start': 479, 'end': 486, 'label': 'unit_id', 'function': 'xx_to_yy_flat'}, {'text': '201-208', 'start': 488, 'end': 495, 'label': 'unit_id', 'function': 'xx_to_yy_flat'}, {'text': '301-308', 'start': 497, 'end': 504, 'label': 'unit_id', 'function': 'xx_to_yy_flat'}, {'text': '401-408', 'start': 506, 'end': 513, 'label': 'unit_id', 'function': 'xx_to_yy_flat'}, {'text': '501-506', 'start': 515, 'end': 522, 'label': 'unit_id', 'function': 'xx_to_yy_flat'}, {'text': '601-604', 'start': 527, 'end': 534, 'label': 'unit_id', 'function': 'xx_to_yy_flat'}, {'text': '102-112', 'start': 551, 'end': 558, 'label': 'unit_id', 'function': 'xx_to_yy_flat'}, {'text': '201-212', 'start': 560, 'end': 567, 'label': 'unit_id', 'function': 'xx_to_yy_flat'}, {'text': '301-312', 'start': 569, 'end': 576, 'label': 'unit_id', 'function': 'xx_to_yy_flat'}, {'text': '401-412', 'start': 578, 'end': 585, 'label': 'unit_id', 'function': 'xx_to_yy_flat'}, {'text': '501-508', 'start': 587, 'end': 594, 'label': 'unit_id', 'function': 'xx_to_yy_flat'}, {'text': '601-604', 'start': 599, 'end': 606, 'label': 'unit_id', 'function': 'xx_to_yy_flat'}, {'text': '1-15', 'start': 625, 'end': 629, 'label': 'unit_id', 'function': 'xx_to_yy_flat'}, {'text': '101-116', 'start': 631, 'end': 638, 'label': 'unit_id', 'function': 'xx_to_yy_flat'}, {'text': '201-216', 'start': 640, 'end': 647, 'label': 'unit_id', 'function': 'xx_to_yy_flat'}, {'text': '301-315', 'start': 649, 'end': 656, 'label': 'unit_id', 'function': 'xx_to_yy_flat'}, {'text': '401-416', 'start': 658, 'end': 665, 'label': 'unit_id', 'function': 'xx_to_yy_flat'}, {'text': '501-510', 'start': 667, 'end': 674, 'label': 'unit_id', 'function': 'xx_to_yy_flat'}, {'text': '101-108', 'start': 693, 'end': 700, 'label': 'unit_id', 'function': 'xx_to_yy_flat'}, {'text': '201-208', 'start': 702, 'end': 709, 'label': 'unit_id', 'function': 'xx_to_yy_flat'}, {'text': '301-308', 'start': 711, 'end': 718, 'label': 'unit_id', 'function': 'xx_to_yy_flat'}, {'text': '401-408', 'start': 720, 'end': 727, 'label': 'unit_id', 'function': 'xx_to_yy_flat'}, {'text': '501-506', 'start': 729, 'end': 736, 'label': 'unit_id', 'function': 'xx_to_yy_flat'}, {'text': '601-604', 'start': 741, 'end': 748, 'label': 'unit_id', 'function': 'xx_to_yy_flat'}, {'text': '1-10', 'start': 768, 'end': 772, 'label': 'unit_id', 'function': 'xx_to_yy_flat'}, {'text': '101-110', 'start': 774, 'end': 781, 'label': 'unit_id', 'function': 'xx_to_yy_flat'}, {'text': '201-210', 'start': 783, 'end': 790, 'label': 'unit_id', 'function': 'xx_to_yy_flat'}, {'text': '301-310', 'start': 792, 'end': 799, 'label': 'unit_id', 'function': 'xx_to_yy_flat'}, {'text': '402-403', 'start': 804, 'end': 811, 'label': 'unit_id', 'function': 'xx_to_yy_flat'}, {'text': '1-10', 'start': 827, 'end': 831, 'label': 'unit_id', 'function': 'xx_to_yy_flat'}, {'text': '101-110', 'start': 836, 'end': 843, 'label': 'unit_id', 'function': 'xx_to_yy_flat'}, {'text': '1-6', 'start': 861, 'end': 864, 'label': 'unit_id', 'function': 'xx_to_yy_flat'}, {'text': '101-106', 'start': 866, 'end': 873, 'label': 'unit_id', 'function': 'xx_to_yy_flat'}, {'text': '201-206', 'start': 875, 'end': 882, 'label': 'unit_id', 'function': 'xx_to_yy_flat'}, {'text': '301-306', 'start': 884, 'end': 891, 'label': 'unit_id', 'function': 'xx_to_yy_flat'}, {'text': '401-408', 'start': 893, 'end': 900, 'label': 'unit_id', 'function': 'xx_to_yy_flat'}, {'text': '501-507', 'start': 905, 'end': 912, 'label': 'unit_id', 'function': 'xx_to_yy_flat'}], 'datapoint_id': 51353}]\n"
     ]
    }
   ],
   "source": [
    "search_term = \"Ground to ninth Floor Flats being 101-114, 201-214, 301-314, 401-414, 501-514, 601-613 and 701-704 Alaska Building\"\n",
    "results = [entry for entry in weakly_labelled_dict if search_term.lower() in entry['text'].lower()]\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enhance_ocod.labelling.ner_spans import lfs  # Import your functions\n",
    "from enhance_ocod.labelling.weak_labelling import create_flat_tag, create_commercial_park_tag\n",
    "\n",
    "def debug_labelling_functions(text: str, \n",
    "                             functions = None,\n",
    "                             create_mock_row: bool = True) -> None:\n",
    "    \"\"\"\n",
    "    Debug labelling functions by showing what each one extracts from the text.\n",
    "    \n",
    "    Args:\n",
    "        text: The text to analyze\n",
    "        functions: List of labelling functions (defaults to lfs)\n",
    "        create_mock_row: Whether to create a mock row with flat/commercial tags\n",
    "    \"\"\"\n",
    "    \n",
    "    if functions is None:\n",
    "        functions = lfs\n",
    "    \n",
    "    # Create a mock row\n",
    "    if create_mock_row:\n",
    "        import pandas as pd\n",
    "        df = pd.DataFrame({'text': [text]})\n",
    "        df = create_flat_tag(df, 'text')\n",
    "        df = create_commercial_park_tag(df, 'text')\n",
    "        row = df.iloc[0]\n",
    "    else:\n",
    "        # Simple row with just text\n",
    "        import pandas as pd\n",
    "        row = pd.Series({'text': text})\n",
    "    \n",
    "    print(f\"TEXT: {text}\")\n",
    "    print(f\"LENGTH: {len(text)}\")\n",
    "    if create_mock_row:\n",
    "        print(f\"FLAT_TAG: {row.get('flat_tag', 'N/A')}\")\n",
    "        print(f\"COMMERCIAL_PARK_TAG: {row.get('commercial_park_tag', 'N/A')}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    all_spans = []\n",
    "    \n",
    "    for func in functions:\n",
    "        try:\n",
    "            spans = func(row)\n",
    "            print(f\"\\n{func.__name__.upper()}:\")\n",
    "            \n",
    "            if not spans:\n",
    "                print(\"  No spans found\")\n",
    "            else:\n",
    "                for start, end, label in spans:\n",
    "                    extracted_text = text[start:end]\n",
    "                    print(f\"  [{start:3d}-{end:3d}] {label:15s} '{extracted_text}'\")\n",
    "                    all_spans.append((start, end, label, func.__name__))\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n{func.__name__.upper()}: ERROR - {e}\")\n",
    "    \n",
    "    # Show overlaps\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"OVERLAP ANALYSIS:\")\n",
    "    \n",
    "    # Sort by start position\n",
    "    all_spans.sort(key=lambda x: x[0])\n",
    "    \n",
    "    overlaps = []\n",
    "    for i in range(len(all_spans)):\n",
    "        for j in range(i+1, len(all_spans)):\n",
    "            span1 = all_spans[i]\n",
    "            span2 = all_spans[j]\n",
    "            \n",
    "            # Check if they overlap\n",
    "            if not (span1[1] <= span2[0] or span2[1] <= span1[0]):\n",
    "                overlaps.append((span1, span2))\n",
    "    \n",
    "    if overlaps:\n",
    "        print(f\"Found {len(overlaps)} overlaps:\")\n",
    "        for span1, span2 in overlaps:\n",
    "            print(f\"  OVERLAP:\")\n",
    "            print(f\"    {span1[3]:20s} [{span1[0]:3d}-{span1[1]:3d}] {span1[2]:15s} '{text[span1[0]:span1[1]]}'\")\n",
    "            print(f\"    {span2[3]:20s} [{span2[0]:3d}-{span2[1]:3d}] {span2[2]:15s} '{text[span2[0]:span2[1]]}'\")\n",
    "            print()\n",
    "    else:\n",
    "        print(\"No overlaps found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT: Ground to ninth Floor Flats being 101-114, 201-214, 301-314, 401-414, 501-514, 601-613 and 701-704 Alaska Building, 101-114, 201-214,301-314, 401-412, 501-506 and 601-605 Arizona Building, 101-114, 201-214, 301-314, 401-414, 501-514, 601-614, 701-708, 801-804, 901-903 California Building, 101-108,     201-208, 301-307, 401-408, 501-508, 601-608, 701-708, 801-808 and 901-903 Colorado Building, 1-4, 101-109, 201-210, 301-310, 401-410, 501-510 and 601-605 Dakota Building, 1-7, 101-108, 201-208, 301-308, 401-408, 501-506 and 601-604 Idaho Building, 102-112, 201-212, 301-312, 401-412, 501-508 and 601-604 Indiana Building, 1-15, 101-116, 201-216, 301-315, 401-416, 501-510 Montana Building, 101-108, 201-208, 301-308, 401-408, 501-506 and 601-604 Nebraska Building, 1-10, 101-110, 201-210, 301-310 and 402-403 Utah Building, 1-10 and 101-110 Boston Building, 1-6, 101-106, 201-206, 301-306, 401-408 and 501-507 Madison Building, Deals Gateway, London\n",
      "LENGTH: 952\n",
      "FLAT_TAG: True\n",
      "COMMERCIAL_PARK_TAG: False\n",
      "================================================================================\n",
      "\n",
      "BUILDING_REGEX_FN:\n",
      "  [ 99-114] building_name   'Alaska Building'\n",
      "  [171-187] building_name   'Arizona Building'\n",
      "  [269-288] building_name   'California Building'\n",
      "  [377-394] building_name   'Colorado Building'\n",
      "  [457-472] building_name   'Dakota Building'\n",
      "  [535-549] building_name   'Idaho Building'\n",
      "  [607-623] building_name   'Indiana Building'\n",
      "  [675-691] building_name   'Montana Building'\n",
      "  [749-766] building_name   'Nebraska Building'\n",
      "  [812-825] building_name   'Utah Building'\n",
      "  [844-859] building_name   'Boston Building'\n",
      "  [913-929] building_name   'Madison Building'\n",
      "\n",
      "BUILDING_SPECIAL_FN:\n",
      "  No spans found\n",
      "\n",
      "MILL:\n",
      "  No spans found\n",
      "\n",
      "NUMBER_COMMA_BUILDING_FN:\n",
      "  No spans found\n",
      "\n",
      "START_ONLY_LETTERS_FN:\n",
      "  No spans found\n",
      "\n",
      "WHARF_BUILDING:\n",
      "  No spans found\n",
      "\n",
      "WORD_BUILDING_NUMBER:\n",
      "  No spans found\n",
      "\n",
      "CITY_MATCH2:\n",
      "  [946-952] city            'London'\n",
      "\n",
      "NUMBER_FILTER:\n",
      "  No spans found\n",
      "\n",
      "POSTCODES:\n",
      "  No spans found\n",
      "\n",
      "KNIGHTSBRIDGE_ROAD:\n",
      "  No spans found\n",
      "\n",
      "MEADOWS_REGEX:\n",
      "  No spans found\n",
      "\n",
      "NUMBER_SPACE_MULTI_WORDS_ROADTITLE_FN:\n",
      "  No spans found\n",
      "\n",
      "PARK_ROADS:\n",
      "  No spans found\n",
      "\n",
      "ROAD_FOLLOWED_CITY:\n",
      "  No spans found\n",
      "\n",
      "ROAD_NAMES_BASIC:\n",
      "  [931-944] street_name     'Deals Gateway'\n",
      "\n",
      "SIDE:\n",
      "  No spans found\n",
      "\n",
      "SPECIAL_STREET_NAMES:\n",
      "  No spans found\n",
      "\n",
      "SPECIAL_WELSH:\n",
      "  No spans found\n",
      "\n",
      "THE_DALES:\n",
      "  No spans found\n",
      "\n",
      "SPECIAL_WELSH:\n",
      "  No spans found\n",
      "\n",
      "WHARF_ROAD:\n",
      "  No spans found\n",
      "\n",
      "WORDS_WAYGATE_FN:\n",
      "  [931-944] street_name     'Deals Gateway'\n",
      "\n",
      "ADJECTIVE_SPACE_NUMBER_WORDS_ROADTITLE_FN:\n",
      "  No spans found\n",
      "\n",
      "AND_SPACE_N:\n",
      "  No spans found\n",
      "\n",
      "BEGINS_WITH_NUMBER:\n",
      "  No spans found\n",
      "\n",
      "COMMA_SPACE_NUMBER_WORDS_ROADTITLE_FN:\n",
      "  No spans found\n",
      "\n",
      "NO_ROAD_NEAR:\n",
      "  No spans found\n",
      "\n",
      "XX_TO_YY:\n",
      "  No spans found\n",
      "\n",
      "AND_SPACE_N_FLATS:\n",
      "  [ 91- 94] unit_id         '701'\n",
      "  [163-166] unit_id         '601'\n",
      "  [369-372] unit_id         '901'\n",
      "  [449-452] unit_id         '601'\n",
      "  [527-530] unit_id         '601'\n",
      "  [599-602] unit_id         '601'\n",
      "  [741-744] unit_id         '601'\n",
      "  [804-807] unit_id         '402'\n",
      "  [836-839] unit_id         '101'\n",
      "  [905-908] unit_id         '501'\n",
      "\n",
      "BEGINS_WITH_NUMBER_FLAT:\n",
      "  No spans found\n",
      "\n",
      "NUMBER_BEFORE_BUILDING:\n",
      "  No spans found\n",
      "\n",
      "CARPARK_ID_FN:\n",
      "  No spans found\n",
      "\n",
      "COMMA_SPACE_NUMBER_COMMA_FLAT:\n",
      "  No spans found\n",
      "\n",
      "FLAT_LETTER:\n",
      "  No spans found\n",
      "\n",
      "FLAT_LETTER_COMPLEX:\n",
      "  No spans found\n",
      "\n",
      "FLAT_NUMBER:\n",
      "  No spans found\n",
      "\n",
      "NUMBER_SPACE_AND_FLAT:\n",
      "  No spans found\n",
      "\n",
      "UNIT_ID_FN:\n",
      "  No spans found\n",
      "\n",
      "UNIT_LETTER:\n",
      "  No spans found\n",
      "\n",
      "XX_TO_YY_FLAT:\n",
      "  [ 34- 41] unit_id         '101-114'\n",
      "  [ 43- 50] unit_id         '201-214'\n",
      "  [ 52- 59] unit_id         '301-314'\n",
      "  [ 61- 68] unit_id         '401-414'\n",
      "  [ 70- 77] unit_id         '501-514'\n",
      "  [ 79- 86] unit_id         '601-613'\n",
      "  [ 91- 98] unit_id         '701-704'\n",
      "  [116-123] unit_id         '101-114'\n",
      "  [125-132] unit_id         '201-214'\n",
      "  [133-140] unit_id         '301-314'\n",
      "  [142-149] unit_id         '401-412'\n",
      "  [151-158] unit_id         '501-506'\n",
      "  [163-170] unit_id         '601-605'\n",
      "  [189-196] unit_id         '101-114'\n",
      "  [198-205] unit_id         '201-214'\n",
      "  [207-214] unit_id         '301-314'\n",
      "  [216-223] unit_id         '401-414'\n",
      "  [225-232] unit_id         '501-514'\n",
      "  [234-241] unit_id         '601-614'\n",
      "  [243-250] unit_id         '701-708'\n",
      "  [252-259] unit_id         '801-804'\n",
      "  [261-268] unit_id         '901-903'\n",
      "  [290-297] unit_id         '101-108'\n",
      "  [303-310] unit_id         '201-208'\n",
      "  [312-319] unit_id         '301-307'\n",
      "  [321-328] unit_id         '401-408'\n",
      "  [330-337] unit_id         '501-508'\n",
      "  [339-346] unit_id         '601-608'\n",
      "  [348-355] unit_id         '701-708'\n",
      "  [357-364] unit_id         '801-808'\n",
      "  [369-376] unit_id         '901-903'\n",
      "  [396-399] unit_id         '1-4'\n",
      "  [401-408] unit_id         '101-109'\n",
      "  [410-417] unit_id         '201-210'\n",
      "  [419-426] unit_id         '301-310'\n",
      "  [428-435] unit_id         '401-410'\n",
      "  [437-444] unit_id         '501-510'\n",
      "  [449-456] unit_id         '601-605'\n",
      "  [474-477] unit_id         '1-7'\n",
      "  [479-486] unit_id         '101-108'\n",
      "  [488-495] unit_id         '201-208'\n",
      "  [497-504] unit_id         '301-308'\n",
      "  [506-513] unit_id         '401-408'\n",
      "  [515-522] unit_id         '501-506'\n",
      "  [527-534] unit_id         '601-604'\n",
      "  [551-558] unit_id         '102-112'\n",
      "  [560-567] unit_id         '201-212'\n",
      "  [569-576] unit_id         '301-312'\n",
      "  [578-585] unit_id         '401-412'\n",
      "  [587-594] unit_id         '501-508'\n",
      "  [599-606] unit_id         '601-604'\n",
      "  [625-629] unit_id         '1-15'\n",
      "  [631-638] unit_id         '101-116'\n",
      "  [640-647] unit_id         '201-216'\n",
      "  [649-656] unit_id         '301-315'\n",
      "  [658-665] unit_id         '401-416'\n",
      "  [667-674] unit_id         '501-510'\n",
      "  [693-700] unit_id         '101-108'\n",
      "  [702-709] unit_id         '201-208'\n",
      "  [711-718] unit_id         '301-308'\n",
      "  [720-727] unit_id         '401-408'\n",
      "  [729-736] unit_id         '501-506'\n",
      "  [741-748] unit_id         '601-604'\n",
      "  [768-772] unit_id         '1-10'\n",
      "  [774-781] unit_id         '101-110'\n",
      "  [783-790] unit_id         '201-210'\n",
      "  [792-799] unit_id         '301-310'\n",
      "  [804-811] unit_id         '402-403'\n",
      "  [827-831] unit_id         '1-10'\n",
      "  [836-843] unit_id         '101-110'\n",
      "  [861-864] unit_id         '1-6'\n",
      "  [866-873] unit_id         '101-106'\n",
      "  [875-882] unit_id         '201-206'\n",
      "  [884-891] unit_id         '301-306'\n",
      "  [893-900] unit_id         '401-408'\n",
      "  [905-912] unit_id         '501-507'\n",
      "\n",
      "FLAT_UNIT:\n",
      "  No spans found\n",
      "\n",
      "IS_CARPARK:\n",
      "  No spans found\n",
      "\n",
      "IS_OTHER_CLASS:\n",
      "  No spans found\n",
      "\n",
      "================================================================================\n",
      "OVERLAP ANALYSIS:\n",
      "Found 11 overlaps:\n",
      "  OVERLAP:\n",
      "    and_space_n_flats    [ 91- 94] unit_id         '701'\n",
      "    xx_to_yy_flat        [ 91- 98] unit_id         '701-704'\n",
      "\n",
      "  OVERLAP:\n",
      "    and_space_n_flats    [163-166] unit_id         '601'\n",
      "    xx_to_yy_flat        [163-170] unit_id         '601-605'\n",
      "\n",
      "  OVERLAP:\n",
      "    and_space_n_flats    [369-372] unit_id         '901'\n",
      "    xx_to_yy_flat        [369-376] unit_id         '901-903'\n",
      "\n",
      "  OVERLAP:\n",
      "    and_space_n_flats    [449-452] unit_id         '601'\n",
      "    xx_to_yy_flat        [449-456] unit_id         '601-605'\n",
      "\n",
      "  OVERLAP:\n",
      "    and_space_n_flats    [527-530] unit_id         '601'\n",
      "    xx_to_yy_flat        [527-534] unit_id         '601-604'\n",
      "\n",
      "  OVERLAP:\n",
      "    and_space_n_flats    [599-602] unit_id         '601'\n",
      "    xx_to_yy_flat        [599-606] unit_id         '601-604'\n",
      "\n",
      "  OVERLAP:\n",
      "    and_space_n_flats    [741-744] unit_id         '601'\n",
      "    xx_to_yy_flat        [741-748] unit_id         '601-604'\n",
      "\n",
      "  OVERLAP:\n",
      "    and_space_n_flats    [804-807] unit_id         '402'\n",
      "    xx_to_yy_flat        [804-811] unit_id         '402-403'\n",
      "\n",
      "  OVERLAP:\n",
      "    and_space_n_flats    [836-839] unit_id         '101'\n",
      "    xx_to_yy_flat        [836-843] unit_id         '101-110'\n",
      "\n",
      "  OVERLAP:\n",
      "    and_space_n_flats    [905-908] unit_id         '501'\n",
      "    xx_to_yy_flat        [905-912] unit_id         '501-507'\n",
      "\n",
      "  OVERLAP:\n",
      "    road_names_basic     [931-944] street_name     'Deals Gateway'\n",
      "    words_waygate_fn     [931-944] street_name     'Deals Gateway'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_text = 'Ground to ninth Floor Flats being 101-114, 201-214, 301-314, 401-414, 501-514, 601-613 and 701-704 Alaska Building, 101-114, 201-214,301-314, 401-412, 501-506 and 601-605 Arizona Building, 101-114, 201-214, 301-314, 401-414, 501-514, 601-614, 701-708, 801-804, 901-903 California Building, 101-108,     201-208, 301-307, 401-408, 501-508, 601-608, 701-708, 801-808 and 901-903 Colorado Building, 1-4, 101-109, 201-210, 301-310, 401-410, 501-510 and 601-605 Dakota Building, 1-7, 101-108, 201-208, 301-308, 401-408, 501-506 and 601-604 Idaho Building, 102-112, 201-212, 301-312, 401-412, 501-508 and 601-604 Indiana Building, 1-15, 101-116, 201-216, 301-315, 401-416, 501-510 Montana Building, 101-108, 201-208, 301-308, 401-408, 501-506 and 601-604 Nebraska Building, 1-10, 101-110, 201-210, 301-310 and 402-403 Utah Building, 1-10 and 101-110 Boston Building, 1-6, 101-106, 201-206, 301-306, 401-408 and 501-507 Madison Building, Deals Gateway, London'\n",
    "\n",
    "debug_labelling_functions(target_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [01:32<00:00,  4.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing complete:\n",
      "  - Processed 94091 rows successfully\n",
      "  - Found 476207 total spans\n",
      "  - Average 5.06 spans per row\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "weakly_labelled_dict = process_dataframe_batch(df, \n",
    "                           batch_size = 5000,\n",
    "                           text_column  = 'Property Address',\n",
    "                           include_function_name  = False,\n",
    "                           save_intermediate  = False,\n",
    "                           verbose  = True)\n",
    "                           \n",
    "remove_overlapping_spans(weakly_labelled_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "def convert_weakly_labelled_list_to_dataframe(results: List[Dict], batch_size: int = 50000) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert parsing results to structured entity DataFrame.\n",
    "\n",
    "    Works with data produced by the weak labelling function `process_dataframe_batch`.\n",
    "    This function exists due to a structural mismatch with the `convert_to_entity_dataframe` function from the inference module. This should be resolved in future versions.\n",
    "    Creates a long-format DataFrame where each row represents one extracted entity,\n",
    "    suitable for further analysis or database storage.\n",
    "\n",
    "    Args:\n",
    "        results: List of dictionaries with 'text' and 'spans' fields\n",
    "        batch_size: Batch size for progress reporting during processing\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with columns: datapoint_id, label, start, end, text, label_text, label_id_count\n",
    "\n",
    "    Example:\n",
    "    ```python\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Create example DataFrame with property addresses\n",
    "    df = pd.DataFrame({\n",
    "        'property_address': [\n",
    "            'Westleigh Lodge Care Home, Nel Pan Lane, Leigh (WN7 5JT)',\n",
    "            'Flat 1, 1a Canal Street, Manchester (M1 3HE)',\n",
    "            'Flat 201, 1 Regent Road, Manchester (M3 4AY)',\n",
    "            '15 Oak Avenue, Birmingham (B12 9QR)'\n",
    "        ]\n",
    "    })\n",
    "\n",
    "    # Process DataFrame with property addresses\n",
    "    processed_list = process_dataframe_batch(df, \n",
    "                                batch_size=5000,\n",
    "                                text_column='property_address')\n",
    "                                \n",
    "    # Remove any overlapping spans\n",
    "    remove_overlapping_spans(processed_list)\n",
    "\n",
    "    # Convert to standard DataFrame format\n",
    "    processed_list = convert_weak_labels_to_standard_format(processed_list)\n",
    "\n",
    "\n",
    "    processed_df = convert_weakly_labelled_list_to_dataframe(processed_list)\n",
    "\n",
    "    print(processed_df)\n",
    "    ```\n",
    "    \n",
    "    Note: Input DataFrame (df) must contain a 'property_address' column.\n",
    "    \"\"\"\n",
    "    total_entities = sum(len(result[\"spans\"]) for result in results)\n",
    "    \n",
    "    if total_entities == 0:\n",
    "        print(\"Warning: No entities found in results!\")\n",
    "        return pd.DataFrame(columns=['datapoint_id', 'label', 'start', 'end', 'text', 'label_text', 'label_id_count'])\n",
    "    \n",
    "    print(f'Processing {total_entities:,} entities into DataFrame...')\n",
    "    \n",
    "    # Pre-allocate arrays for efficiency\n",
    "    datapoint_ids = []\n",
    "    labels = []\n",
    "    starts = []\n",
    "    ends = []\n",
    "    texts = []\n",
    "    label_texts = []\n",
    "    \n",
    "    processed = 0\n",
    "    \n",
    "    for idx, result in enumerate(results):\n",
    "        datapoint_id = idx  # Using list index as datapoint_id\n",
    "        original_address = result[\"text\"]\n",
    "        spans = result[\"spans\"]\n",
    "        \n",
    "        entity_count = len(spans)\n",
    "        if entity_count > 0:\n",
    "            datapoint_ids.extend([datapoint_id] * entity_count)\n",
    "            labels.extend([span['label'] for span in spans])\n",
    "            starts.extend([span['start'] for span in spans])\n",
    "            ends.extend([span['end'] for span in spans])\n",
    "            texts.extend([original_address] * entity_count)\n",
    "            # Extract text using start/end positions\n",
    "            label_texts.extend([original_address[span['start']:span['end']] for span in spans])\n",
    "            \n",
    "            processed += entity_count\n",
    "            if processed % batch_size == 0:\n",
    "                print(f'Processed {processed:,}/{total_entities:,} entities')\n",
    "    \n",
    "    all_entities = pd.DataFrame({\n",
    "        'datapoint_id': datapoint_ids,\n",
    "        'label': labels,\n",
    "        'start': starts,\n",
    "        'end': ends,\n",
    "        'text': texts,\n",
    "        'label_text': label_texts\n",
    "    })\n",
    "    \n",
    "    print('Computing label counts...')\n",
    "    # Add counter for multiple entities of same type within same address\n",
    "    all_entities['label_id_count'] = all_entities.groupby(['datapoint_id', 'label'], sort=False).cumcount()\n",
    "    \n",
    "    print('âœ“ Named Entity Recognition processing complete')\n",
    "    print(f'Total entities extracted: {len(all_entities):,}')\n",
    "    \n",
    "    return all_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 424,440 entities into DataFrame...\n",
      "Processed 350,000/424,440 entities\n",
      "Processed 400,000/424,440 entities\n",
      "Computing label counts...\n",
      "âœ“ Named Entity Recognition processing complete\n",
      "Total entities extracted: 424,440\n"
     ]
    }
   ],
   "source": [
    "from enhance_ocod.inference import convert_to_entity_dataframe\n",
    "\n",
    "temp = convert_weakly_labelled_list_to_dataframe(weakly_labelled_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    424440.000000\n",
       "mean          0.274265\n",
       "std           2.502990\n",
       "min           0.000000\n",
       "25%           0.000000\n",
       "50%           0.000000\n",
       "75%           0.000000\n",
       "max         109.000000\n",
       "Name: label_id_count, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp['label_id_count'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 136.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing complete:\n",
      "  - Processed 4 rows successfully\n",
      "  - Found 19 total spans\n",
      "  - Average 4.75 spans per row\n",
      "Processing 18 entities into DataFrame...\n",
      "Computing label counts...\n",
      "âœ“ Named Entity Recognition processing complete\n",
      "Total entities extracted: 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "processed_list = process_dataframe_batch(df, \n",
    "                           batch_size = 5000,\n",
    "                           text_column  = 'property_address')\n",
    "                           \n",
    "remove_overlapping_spans(processed_list)\n",
    "\n",
    "processed_list = convert_weak_labels_to_standard_format(processed_list)\n",
    "\n",
    "\n",
    "processed_df = convert_weakly_labelled_list_to_dataframe(processed_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test = convert_weak_labels_to_standard_format(weakly_labelled_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m temp \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_to_entity_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/enhance_ocod/src/enhance_ocod/inference.py:371\u001b[0m, in \u001b[0;36mconvert_to_entity_dataframe\u001b[0;34m(results, batch_size)\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mconvert_to_entity_dataframe\u001b[39m(results: Dict, batch_size: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50000\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame:\n\u001b[1;32m    357\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;124;03m    Convert parsing results to structured entity DataFrame.\u001b[39;00m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;124;03m        DataFrame with columns: datapoint_id, label, start, end, text, label_text, label_id_count\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 371\u001b[0m     total_entities \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mlen\u001b[39m(result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mentities\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m \u001b[43mresults\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresults\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m    373\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m total_entities \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    374\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWarning: No entities found in results!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "temp = convert_to_entity_dataframe(test)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
