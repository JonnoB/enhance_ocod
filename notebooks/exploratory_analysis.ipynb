{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from plotnine import *\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from enhance_ocod.analysis import create_summarised_stats, create_mean_difference_by_groups\n",
    "\n",
    "data_folder = Path('../data') \n",
    "figures_folder = Path('../figures/figures_new')\n",
    "figures_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "OCOD_history_path = data_folder / 'ocod_history_processed_new' \n",
    "\n",
    "list_of_files = list(OCOD_history_path.iterdir())\n",
    "\n",
    "active_class_var = 'class'\n",
    "\n",
    "\n",
    "LAD_COLUMN_CODE = 'LAD22CD' # change this according to the shapefile you are using\n",
    "LAD_COLUMN_NAME = \"LAD22NM\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_file = OCOD_history_path /'OCOD_FULL_2022_02.parquet'\n",
    "\n",
    "reference_year = pd.read_parquet(target_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postcode_test = reference_year.loc[ reference_year[active_class_var].isin(['residential'])].copy()\n",
    "postcode_test['has_postcode'] = postcode_test['postcode'].notna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postcode_test.groupby('has_postcode').size()/postcode_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the missing LSOA records\n",
    "missing_lsoa_addresses = reference_year.loc[reference_year['lsoa11cd'].isna(), 'property_address']\n",
    "\n",
    "# Count how many contain each keyword\n",
    "keyword_counts = {}\n",
    "keywords = ['airspace', 'air space', 'land', 'plot', 'car park', 'carpark', 'unit', 'industrial', 'centre' ]\n",
    "\n",
    "for keyword in keywords:\n",
    "    count = missing_lsoa_addresses.str.contains(keyword, case=False, na=False).sum()\n",
    "    keyword_counts[keyword] = count\n",
    "    print(f\"'{keyword}': {count}\")\n",
    "\n",
    "# If you want to see which addresses contain ANY of these keywords\n",
    "any_keyword_mask = missing_lsoa_addresses.str.contains('|'.join(keywords), case=False, na=False)\n",
    "any_keyword_count = any_keyword_mask.sum()\n",
    "print(f\"\\nAddresses containing any of {keywords}: {any_keyword_count}\")\n",
    "\n",
    "# If you want to see the actual addresses that contain these keywords\n",
    "addresses_with_keywords = missing_lsoa_addresses[any_keyword_mask]\n",
    "print(f\"\\nSample addresses containing keywords:\")\n",
    "print(addresses_with_keywords.head(10).tolist())\n",
    "\n",
    "# Summary counts\n",
    "print(f\"\\nSummary:\")\n",
    "print(f\"Total missing LSOA: {len(missing_lsoa_addresses)}\")\n",
    "print(f\"Contains keywords: {any_keyword_count}\")\n",
    "print(f\"Percentage: {any_keyword_count/len(missing_lsoa_addresses)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_years = []\n",
    "\n",
    "for file in list_of_files:\n",
    "\n",
    "    target_file = pd.read_parquet(file)\n",
    "\n",
    "    target_file = target_file.loc[target_file['lsoa11cd'].isna(),:].groupby('class').size().reset_index()\n",
    "\n",
    "    target_file = target_file.rename(columns = {0:'values'})\n",
    "    target_file['file'] = Path(file).stem\n",
    "\n",
    "    all_years.append(target_file)\n",
    "\n",
    "all_years = pd.concat(all_years, ignore_index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data without LSOA\n",
    "\n",
    "WE can see from below that across time the values are relatively stable, and that residential data makes up about 74% of the missing geo location. This means that this data is not getting priced in reducing the overall value of the offshore owned estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "all_years['date'] = all_years['file'].str.extract(r'(\\d{4}_\\d{2})$')[0]\n",
    "all_years['date'] = all_years['date'].str.replace('_', '-') + '-01'\n",
    "all_years['date'] = pd.to_datetime(all_years['date'])\n",
    "# Pivot the dataframe to spread 'class' values as columns\n",
    "all_years_pivot = all_years.pivot(index='file', columns='class', values='values')\n",
    "\n",
    "# Reset index if you want 'file' as a regular column instead of index\n",
    "all_years_pivot = all_years.pivot_table(index='date', columns='class', values='values', fill_value=0)\n",
    "all_years_pivot = all_years_pivot.reset_index()\n",
    "\n",
    "\n",
    "\n",
    "# Calculate the fraction each class makes up of the total\n",
    "class_fractions = all_years.groupby('class')['values'].sum()\n",
    "class_fractions = (class_fractions / class_fractions.sum()).round(2)\n",
    "class_fractions = class_fractions.reset_index()\n",
    "class_fractions.columns = ['class', 'fraction']\n",
    "\n",
    "class_fractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ggplot(all_years_pivot, aes(x = 'date', y = 'residential'))  + geom_line() + labs(\n",
    "    title = 'The total residential property which has not been\\nassigned an LSOA') + scale_x_date(\n",
    "     breaks=pd.date_range(start=all_years_pivot['date'].min(), end=all_years_pivot['date'].max(), freq='Y'),\n",
    "     date_labels='%Y'  # Format to show only year\n",
    " ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Total over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_residential_df, total_per_region_df, total_incorp_df, total_resi_lad_df = create_summarised_stats(list_of_files, active_class_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_per_region_df.groupby(active_class_var)['counts'].sum()/total_per_region_df['counts'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "total_residential_df['type'] = np.where(total_residential_df['is_multi'], 'Multi', 'Single')\n",
    "\n",
    "p = ggplot(total_residential_df, aes(x = 'date', \n",
    "y = 'counts', \n",
    "color = 'type') )+ geom_line() + labs(\n",
    "    title = \"Total number of residential properties\") +   scale_x_date(\n",
    "     breaks=pd.date_range(start=total_residential_df['date'].min(), end=total_residential_df['date'].max(), freq='Y'),\n",
    "     date_labels='%Y'  # Format to show only year\n",
    " )\n",
    "\n",
    "p.save(filename = figures_folder /'total_properties.png')\n",
    "\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "p = ggplot(total_per_region_df.groupby(['region', 'date'])['counts'].sum().reset_index(), aes(x='date', y='counts', color='region')) + \\\n",
    "    geom_line() + \\\n",
    "    labs(title=\"Total number of properties\") + \\\n",
    "    scale_x_date(\n",
    "        breaks=pd.date_range(start=total_residential_df['date'].min(), end=total_residential_df['date'].max(), freq='Y'),\n",
    "        date_labels='%Y'\n",
    "    ) \n",
    "\n",
    "p.save(filename = figures_folder /'properties_by_region.png')\n",
    "\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_per_class_df = total_per_region_df.groupby([active_class_var, 'date'])['counts'].sum().reset_index()\n",
    "\n",
    "p = ggplot(total_per_class_df.loc[total_per_class_df[active_class_var]!='residential'], aes(x = 'date', \n",
    "y = 'counts', \n",
    "color = active_class_var) )+ geom_line() + labs(\n",
    "    title = \"Total properties by class excluding residential\") +   scale_x_date(\n",
    "     breaks=pd.date_range(start=total_residential_df['date'].min(), end=total_residential_df['date'].max(), freq='Y'),\n",
    "     date_labels='%Y'  # Format to show only year\n",
    " )\n",
    "\n",
    "p.save(filename = figures_folder /'properties_by_class.png')\n",
    "\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_class = total_per_class_df.pivot(index = 'date', columns = active_class_var, values = 'counts')\n",
    "\n",
    "temp_class['res_perc'] = temp_class['residential']/temp_class.sum(axis = 1)\n",
    "temp_class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Country of incorporation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_incorp_df['country_incorporated'] = total_incorp_df['country_incorporated'].str.replace(\"BRITISH VIRGIN ISLANDS\", \"BVI\")\n",
    "\n",
    "country_totals = total_incorp_df.groupby('country_incorporated')['counts'].mean()\n",
    "\n",
    "top_10_countries = country_totals.nlargest(20).index\n",
    "\n",
    "# The top four dominate so massively there is no point in having anyone else\n",
    "top_4_countries = country_totals.nlargest(4).index\n",
    "\n",
    "filtered_df = total_incorp_df[total_incorp_df['country_incorporated'].isin(top_4_countries)]\n",
    "\n",
    "print(\"Top 10 countries by total counts:\")\n",
    "print(country_totals.nlargest(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total Fraction the top four make up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_incorp_df.loc[total_incorp_df['country_incorporated'].isin(['BVI', 'JERSEY', 'GUERNSEY', 'ISLO OF MAN']),'counts'].sum()/total_incorp_df['counts'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = ggplot(filtered_df, aes(x = 'date', \n",
    "y = 'counts', \n",
    "color = 'country_incorporated') )+ geom_line() + labs(\n",
    "    title = \"Total number of properties\\nby country of incorporation\",\n",
    "    color = 'Country'\n",
    "    ) +   scale_x_date(\n",
    "     breaks=pd.date_range(start=filtered_df['date'].min(), end=filtered_df['date'].max(), freq='Y'),\n",
    "     date_labels='%Y'  # Format to show only year\n",
    " ) \n",
    "\n",
    "p.save(filename = figures_folder /'properties_by_incorporation.png')\n",
    "\n",
    "p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = total_incorp_df.copy()\n",
    "\n",
    "test['diff'] = (test['counts'] - test.groupby('country_incorporated')['counts'].shift())/test.groupby('country_incorporated')['counts'].shift()\n",
    "\n",
    "country_totals = test.groupby('country_incorporated')['diff'].mean()\n",
    "\n",
    "# Step 2: Get the top 10 countries\n",
    "top_10_countries = country_totals.nlargest(20).index\n",
    "\n",
    "print(country_totals.nlargest(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = ggplot(test[test['country_incorporated'].isin(['LUXEMBOURG', 'NETHERLANDS', 'GERMANY'])], aes(x = 'date', \n",
    "y = 'counts', \n",
    "color = 'country_incorporated') )+ geom_line() + labs(\n",
    "    title = \"Total number of properties\\nby country of incorporation\",\n",
    "    ) +   scale_x_date(\n",
    "     breaks=pd.date_range(start=test['date'].min(), end=test['date'].max(), freq='Y'),\n",
    "     date_labels='%Y'  # Format to show only year\n",
    " ) \n",
    "\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sorted_all = total_resi_lad_df.groupby(['lad11cd', 'date'])['counts'].sum().reset_index()\n",
    "df_sorted_all = df_sorted_all.sort_values(['lad11cd', 'date'])\n",
    "df_sorted_all['counts_diff'] = df_sorted_all.groupby(['lad11cd'])['counts'].diff()\n",
    "df_sorted_all['is_multi'] = 'all'\n",
    "\n",
    "\n",
    "df_sorted = total_resi_lad_df.sort_values(['lad11cd', 'date'])\n",
    "\n",
    "# Calculate the difference in counts between consecutive dates for each LAD11cd\n",
    "df_sorted['counts_diff'] = df_sorted.groupby(['lad11cd','is_multi'])['counts'].diff()\n",
    "\n",
    "\n",
    "df_sorted['is_multi'] = np.where(df_sorted['is_multi'], 'multi', 'single')\n",
    "\n",
    "df_sorted = pd.concat([df_sorted, df_sorted_all], ignore_index = True)\n",
    "\n",
    "df_diff_change = df_sorted.groupby(['is_multi', 'lad11cd' ])['counts_diff'].mean().reset_index()\n",
    "df_diff_change['total_mean_change'] = df_diff_change['counts_diff'] * total_resi_lad_df['date'].nunique()\n",
    "\n",
    "df_diff_change.sort_values('total_mean_change')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diff_change = df_sorted.groupby(['is_multi', 'lad11cd' ])['counts_diff'].mean().reset_index()\n",
    "df_diff_change['total_mean_change'] = df_diff_change['counts_diff'] * total_resi_lad_df['date'].nunique()\n",
    "\n",
    "df_diff_change.sort_values('total_mean_change')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "# Direct WFS URL with correct parameters\n",
    "wfs_url = \"https://dservices1.arcgis.com/ESMARspQHYMw9BZ9/arcgis/services/Local_Authority_Districts_May_2022_UK_BSC_V3/WFSServer?service=WFS&version=2.0.0&request=GetFeature&typeName=Local_Authority_Districts_May_2022_UK_BSC_V3:LAD_MAY_2022_UK_BSC_V3&outputFormat=GEOJSON&srsName=EPSG:4326\"\n",
    "\n",
    "try:\n",
    "    print(\"Loading Local Authority Districts data...\")\n",
    "    gdf = gpd.read_file(wfs_url)\n",
    "    print(f\"Successfully loaded {len(gdf)} Local Authority Districts\")\n",
    "    print(f\"Columns: {list(gdf.columns)}\")\n",
    "    print(f\"CRS: {gdf.crs}\")\n",
    "    \n",
    "    # Display first few rows\n",
    "    print(\"\\nFirst 3 rows:\")\n",
    "    print(gdf[[LAD_COLUMN_CODE, LAD_COLUMN_NAME]].head(3))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to calculate months between dates\n",
    "def months_between(date1, date2):\n",
    "    return (date2.year - date1.year) * 12 + (date2.month - date1.month)\n",
    "\n",
    "# Process 'all' group\n",
    "df_sorted_all = total_resi_lad_df.groupby(['lad11cd', 'date'])['counts'].sum().reset_index()\n",
    "df_sorted_all = df_sorted_all.sort_values(['lad11cd', 'date'])\n",
    "df_sorted_all['counts_diff'] = df_sorted_all.groupby(['lad11cd'])['counts'].diff()\n",
    "\n",
    "# Calculate months between consecutive dates\n",
    "df_sorted_all['prev_date'] = df_sorted_all.groupby(['lad11cd'])['date'].shift(1)\n",
    "df_sorted_all['months_diff'] = df_sorted_all.apply(\n",
    "    lambda row: months_between(row['prev_date'], row['date']) if pd.notna(row['prev_date']) else np.nan, \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Calculate monthly change (divide by number of months)\n",
    "df_sorted_all['monthly_change'] = df_sorted_all['counts_diff'] / df_sorted_all['months_diff']\n",
    "df_sorted_all['is_multi'] = 'all'\n",
    "\n",
    "# Process main dataframe\n",
    "df_sorted = total_resi_lad_df.sort_values(['lad11cd', 'date'])\n",
    "\n",
    "# Calculate the difference in counts between consecutive dates for each LAD11cd\n",
    "df_sorted['counts_diff'] = df_sorted.groupby(['lad11cd','is_multi'])['counts'].diff()\n",
    "\n",
    "# Calculate months between consecutive dates\n",
    "df_sorted['prev_date'] = df_sorted.groupby(['lad11cd', 'is_multi'])['date'].shift(1)\n",
    "df_sorted['months_diff'] = df_sorted.apply(\n",
    "    lambda row: months_between(row['prev_date'], row['date']) if pd.notna(row['prev_date']) else np.nan, \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Calculate monthly change (divide by number of months)\n",
    "df_sorted['monthly_change'] = df_sorted['counts_diff'] / df_sorted['months_diff']\n",
    "\n",
    "df_sorted['is_multi'] = np.where(df_sorted['is_multi'], 'multi', 'single')\n",
    "\n",
    "# Use monthly_change instead of counts_diff for concatenation\n",
    "df_sorted_all_final = df_sorted_all[['lad11cd', 'date', 'monthly_change', 'is_multi']]\n",
    "df_sorted_final = df_sorted[['lad11cd', 'date', 'monthly_change', 'is_multi']]\n",
    "\n",
    "df_sorted_combined = pd.concat([df_sorted_final, df_sorted_all_final], ignore_index=True)\n",
    "\n",
    "# Calculate mean monthly change\n",
    "df_diff_change = df_sorted_combined.groupby(['is_multi', 'lad11cd'])['monthly_change'].mean().reset_index()\n",
    "\n",
    "# Calculate total months in dataset\n",
    "min_date = total_resi_lad_df['date'].min()\n",
    "max_date = total_resi_lad_df['date'].max()\n",
    "total_months = months_between(min_date, max_date)\n",
    "\n",
    "# Multiply by total months instead of unique dates\n",
    "df_diff_change['total_mean_change'] = df_diff_change['monthly_change'] * total_months\n",
    "\n",
    "df_diff_change.sort_values('total_mean_change')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = df_diff_change.copy()\n",
    "\n",
    "temp['increase'] = (temp['total_mean_change']>0).astype(int)\n",
    "\n",
    "temp.groupby('is_multi')['increase'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Filter gdf to only include areas with LAD23CD containing \"E\" or \"W\" Remove silly isles for visual compactness\n",
    "gdf_filtered = gdf[gdf[LAD_COLUMN_CODE].str.contains('E|W', na=False) & (gdf[LAD_COLUMN_CODE]!='E06000053')]\n",
    "\n",
    "# Create subplots with light blue background\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 10), facecolor='#E6F3FF')\n",
    "\n",
    "# Categories to plot\n",
    "categories = ['multi', 'single']\n",
    "\n",
    "# Plot each category\n",
    "for i, category in enumerate(categories):\n",
    "    # Merge for each category\n",
    "    gdf_temp = gdf_filtered.merge(df_diff_change.loc[df_diff_change['is_multi']==category], \n",
    "                                  left_on=LAD_COLUMN_CODE, \n",
    "                                  right_on='lad11cd', \n",
    "                                  how='left')\n",
    "    \n",
    "    # Fill missing values with 0\n",
    "    gdf_temp['total_mean_change'] = gdf_temp['total_mean_change'].fillna(0)\n",
    "    \n",
    "    # Apply transformation\n",
    "    gdf_temp['total_mean_change'] = np.where(gdf_temp['total_mean_change'] < -500, -500, gdf_temp['total_mean_change'])\n",
    "    \n",
    "    # Set light blue background for axes\n",
    "    axes[i].set_facecolor('#E6F3FF')\n",
    "    \n",
    "    # Plot base map with light gray fill and black borders\n",
    "    gdf_filtered.plot(ax=axes[i], \n",
    "                      facecolor='lightgray', \n",
    "                      edgecolor='black', \n",
    "                      linewidth=0.3,\n",
    "                      alpha=0.3)\n",
    "    \n",
    "    # Remove rows with NaN values for ranking\n",
    "    gdf_temp_clean = gdf_temp.dropna(subset=['total_mean_change'])\n",
    "    \n",
    "    # Get top 10 highest values\n",
    "    top_10_highest = gdf_temp_clean.nlargest(20, 'total_mean_change')\n",
    "    \n",
    "    # Get top 10 lowest values  \n",
    "    top_10_lowest = gdf_temp_clean.nsmallest(20, 'total_mean_change')\n",
    "    \n",
    "    # Get centroids for point plotting\n",
    "    top_10_highest_centroids = top_10_highest.geometry.centroid\n",
    "    top_10_lowest_centroids = top_10_lowest.geometry.centroid\n",
    "    \n",
    "    # Plot highest values as red circles\n",
    "    axes[i].scatter(top_10_highest_centroids.x, \n",
    "                    top_10_highest_centroids.y,\n",
    "                    c='red', \n",
    "                    s=100, \n",
    "                    alpha=0.8,\n",
    "                    edgecolor='black',\n",
    "                    linewidth=1,\n",
    "                    label='Increase',\n",
    "                    zorder=5)\n",
    "    \n",
    "    # Plot lowest values as blue circles\n",
    "    axes[i].scatter(top_10_lowest_centroids.x, \n",
    "                    top_10_lowest_centroids.y,\n",
    "                    c='blue', \n",
    "                    s=100, \n",
    "                    alpha=0.8,\n",
    "                    edgecolor='black',\n",
    "                    linewidth=1,\n",
    "                    label='Decrease',\n",
    "                    zorder=5)\n",
    "    \n",
    "    # Add legend only to the second plot\n",
    "    if i == 1:\n",
    "        axes[i].legend(loc='upper right', \n",
    "                      bbox_to_anchor=(1.15, 1),\n",
    "                      fontsize=12,\n",
    "                      frameon=True,\n",
    "                      fancybox=True,\n",
    "                      shadow=True)\n",
    "    \n",
    "    # Turn off axis\n",
    "    axes[i].set_axis_off()\n",
    "\n",
    "# Add titles manually using fig.text for perfect alignment\n",
    "fig.text(0.25, 0.92, 'Multi', fontsize=14, fontweight='bold', ha='center')\n",
    "fig.text(0.75, 0.92, 'Single', fontsize=14, fontweight='bold', ha='center')\n",
    "\n",
    "# Overall title\n",
    "fig.suptitle('Top 20 Highest and Lowest Change in property counts at local authority level', \n",
    "             fontsize=16, fontweight='bold', y=0.98)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(figures_folder /'change_maps.png')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_filtered.merge(df_diff_change.loc[df_diff_change['is_multi']=='multi'], \n",
    "                                  left_on=LAD_COLUMN_CODE, \n",
    "                                  right_on='lad11cd', \n",
    "                                  how='left')['total_mean_change'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = gdf_filtered.merge(df_diff_change.loc[df_diff_change['is_multi']=='all'], \n",
    "                                  left_on=LAD_COLUMN_CODE, \n",
    "                                  right_on='lad11cd', \n",
    "                                  how='left').sort_values('total_mean_change')\n",
    "\n",
    "temp.loc[temp['total_mean_change'].notna(), [\"LAD22NM\", \"total_mean_change\"]].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# concentration\n",
    "\n",
    "As we can see the majority of change is concentrated into a very small number of local authorities. The below shows the fraction the 20 LADS with the biggest increases make up of the total increase, and the 20 with the biggest decrease make up of all decreases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.loc[temp['total_mean_change'].notna() & (temp['total_mean_change']<0), [\"LAD22NM\", \"total_mean_change\"]].head(20)['total_mean_change'].sum()/ \\\n",
    "    temp.loc[temp['total_mean_change'].notna() & (temp['total_mean_change']<0),'total_mean_change'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.loc[temp['total_mean_change'].notna() & (temp['total_mean_change']>0), [\"LAD22NM\", \"total_mean_change\"]].tail(20)['total_mean_change'].sum()/ \\\n",
    "    temp.loc[temp['total_mean_change'].notna() & (temp['total_mean_change']>0),'total_mean_change'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lad_price_df = create_mean_difference_by_groups( ['lad11cd', 'is_multi'], \n",
    "ocod_path = OCOD_history_path, \n",
    "class_var = active_class_var)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lad_price_all_df = create_mean_difference_by_groups( ['lad11cd'], ocod_path = OCOD_history_path,\n",
    "class_var = active_class_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lad_price_df.groupby(['ratio_significantly_different', 'is_multi']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lad_price_df.pivot(columns = 'is_multi', \n",
    "index = 'lad11cd', \n",
    "values = 'mean_weighted_difference').rename(columns ={False:\"single_price\", True:\"nested_price\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lad_price_all_df.rename(columns = {'mean_weighted_difference':'all_offshore_change', 'mean_unweighted_difference':'general_property_change'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_change_data = lad_price_df.pivot(columns = 'is_multi', \n",
    "index = 'lad11cd', \n",
    "values = 'mean_weighted_difference').rename(columns ={False:\"single_price\", True:\"nested_price\"})\n",
    "\n",
    "price_change_data_all = lad_price_all_df.rename(columns = {'mean_weighted_difference':'all_offshore_change', 'mean_unweighted_difference':'general_property_change'})\n",
    "price_change_data_all['price_ratio'] = price_change_data_all['all_offshore_change']/price_change_data_all['general_property_change']\n",
    "\n",
    "volume_change_data = df_diff_change.loc[:, ['is_multi','lad11cd', 'total_mean_change']].pivot(\n",
    "    columns = 'is_multi', index ='lad11cd', values = 'total_mean_change')\n",
    "\n",
    "full_data_compare = price_change_data.merge(volume_change_data, on = 'lad11cd').merge(\n",
    "price_change_data_all, on = 'lad11cd'\n",
    ").fillna(0)\n",
    "\n",
    "conditions = [\n",
    "    full_data_compare['price_ratio'] > 1.05,\n",
    "    full_data_compare['price_ratio'] < 0.95\n",
    "]\n",
    "choices = ['increase', 'decrease']\n",
    "\n",
    "# Create categorical column\n",
    "full_data_compare['price_change'] = np.select(conditions, choices, default='stable')\n",
    "\n",
    "conditions = [\n",
    "    full_data_compare['all'] > 10,\n",
    "    full_data_compare['all'] < -10\n",
    "]\n",
    "choices = ['increase', 'decrease']\n",
    "\n",
    "# Create categorical column\n",
    "full_data_compare['volume_change'] = np.select(conditions, choices, default='stable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(full_data_compare[['all', 'price_ratio']]).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create crosstab with better labels\n",
    "crosstab = pd.crosstab(\n",
    "    full_data_compare['price_change'] ,\n",
    "    full_data_compare['volume_change'] ,\n",
    "    rownames=['volume increase'],\n",
    "    colnames=['relative_price increase'],\n",
    "    margins=False,\n",
    "    normalize = False\n",
    ")\n",
    "print(crosstab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create crosstab with better labels\n",
    "crosstab = pd.crosstab(\n",
    "    full_data_compare['price_change'] ,\n",
    "    full_data_compare['volume_change'] ,\n",
    "    rownames=['volume increase'],\n",
    "    colnames=['relative_price increase'],\n",
    "    margins=False,\n",
    "    normalize = True\n",
    ")\n",
    "print(crosstab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  model analayis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "# Find all 'overall' CSVs in the model_performance directory\n",
    "performance_dir = Path(\"../data/model_performance\")\n",
    "overall_files = list(performance_dir.glob(\"*overall*.csv\"))\n",
    "\n",
    "# Load and concatenate all relevant CSVs\n",
    "dfs = []\n",
    "for file in overall_files:\n",
    "    df = pd.read_csv(file)\n",
    "    df['source_file'] = file.name  # Optionally add a column to indicate the source\n",
    "    dfs.append(df)\n",
    "\n",
    "\n",
    "combined = pd.concat(dfs, ignore_index=True)[['precision', 'recall', 'f1_score', 'source_file']]\n",
    "combined['prepocessed'] = combined['source_file'].str.contains('preprocessed')\n",
    "combined[['precision', 'recall', 'f1_score',]] = combined[['precision', 'recall', 'f1_score',]].round(2)\n",
    "combined['model_type'] = np.where(\n",
    "    combined['source_file'].str.contains('devset'), \n",
    "    'conventional', \n",
    "    np.where(\n",
    "        combined['source_file'].str.contains('regex'), \n",
    "        'regex', \n",
    "        'weak-learning'\n",
    "    )\n",
    ")\n",
    "combined = combined[['model_type',  'prepocessed', 'f1_score','precision', 'recall']].sort_values('model_type')\n",
    "combined.rename(columns={'f1_score': 'F1', 'model_type': 'Model Type',\n",
    " 'prepocessed': 'Preprocessed', 'precision': 'Precision', 'recall': 'Recall'}, inplace=True)\n",
    "# Print the LaTeX table\n",
    "latex_table = combined.to_latex(\n",
    "    index=False, \n",
    "    float_format='{:.2f}'.format,\n",
    "    caption='Performance Comparison of Different Model Configurations',\n",
    "    label='tab:model_performance',\n",
    "    position='htbp',\n",
    "    escape=False\n",
    ")\n",
    "\n",
    "table_file = figures_folder / 'model_performance_table.tex'\n",
    "with open(table_file, 'w') as f:\n",
    "    f.write(latex_table)\n",
    "\n",
    "print(latex_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class_table = pd.read_csv('../data/model_performance/test_original_fullset_class_performance.csv')\n",
    "class_table = class_table[['class_name', 'precision', 'recall', 'f1_score', 'support']]\n",
    "class_table.rename(columns={'f1_score': 'F1', 'precision': 'Precision', 'recall': 'Recall', 'class_name': active_class_var }, inplace=True)\n",
    "class_table[active_class_var] = class_table[active_class_var].str.replace(\"_\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "latex_table = class_table.to_latex(\n",
    "    index=False, \n",
    "    float_format='{:.2f}'.format,\n",
    "    caption='Performance is high across all classes indicating a well trained, reliable model',\n",
    "    label='tab:class_performance',\n",
    "    position='htbp',\n",
    "    escape=False\n",
    ")\n",
    "\n",
    "\n",
    "table_file = figures_folder / 'class_performance_table.tex'\n",
    "with open(table_file, 'w') as f:\n",
    "    f.write(latex_table)\n",
    "\n",
    "print(latex_table)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
