{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from plotnine import *\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from enhance_ocod.analysis import create_summarised_stats, create_mean_difference_by_groups\n",
    "\n",
    "data_folder = Path('../data') \n",
    "figures_folder = Path('../figures/figures')\n",
    "figures_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "OCOD_history_path = data_folder / 'ocod_history_processed' \n",
    "\n",
    "list_of_files = list(OCOD_history_path.iterdir())\n",
    "\n",
    "active_class_var = 'class'\n",
    "\n",
    "\n",
    "LAD_COLUMN_CODE = 'LAD22CD' # change this according to the shapefile you are using\n",
    "LAD_COLUMN_NAME = \"LAD22NM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expansion_size_check = []\n",
    "\n",
    "for file in list_of_files:\n",
    "\n",
    "    target_file = pd.read_parquet(file)\n",
    "\n",
    "    target_file = target_file.drop_duplicates(['title_number', 'expansion_size'])\n",
    "    target_file = target_file['expansion_size'].value_counts().reset_index()\n",
    "\n",
    "    target_file.columns = ['expansion_size', 'count']\n",
    "\n",
    "    target_file['file'] = Path(file).stem\n",
    "\n",
    "    expansion_size_check.append(target_file[['file', 'expansion_size', 'count']])\n",
    "\n",
    "expansion_size_check = pd.concat(expansion_size_check, ignore_index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_expansion_counts = expansion_size_check.groupby('expansion_size')['count'].sum().reset_index()\n",
    "del(expansion_size_check)\n",
    "total_expansion_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Erroneous expansion\n",
    "\n",
    "Looking at the distribution and addresses and the close inspection of the relevant addresses, it appears that only a small number of addresses are incorrectly parsed and these are obvious as they start at 1 and go upto over 1000. As such they can be easily filtered out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot(total_expansion_counts.loc[total_expansion_counts['expansion_size']>50], aes(x = 'expansion_size', y = 'count')) + geom_point()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data without LSOA\n",
    "\n",
    "WE can see from below that across time the values are relatively stable, and that residential data makes up about 74% of the missing geo location. This means that this data is not getting priced in reducing the overall value of the offshore owned estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_years = []\n",
    "\n",
    "for file in list_of_files:\n",
    "\n",
    "    target_file = pd.read_parquet(file)\n",
    "    # filter to only known lsoa and expansions less than 1000\n",
    "    target_file = target_file.loc[target_file['lsoa11cd'].isna() & (target_file['expansion_size']<1000),:].groupby('class').size().reset_index()\n",
    "\n",
    "    target_file = target_file.rename(columns = {0:'values'})\n",
    "    target_file['file'] = Path(file).stem\n",
    "\n",
    "    all_years.append(target_file)\n",
    "\n",
    "all_years = pd.concat(all_years, ignore_index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "all_years['date'] = all_years['file'].str.extract(r'(\\d{4}_\\d{2})$')[0]\n",
    "all_years['date'] = all_years['date'].str.replace('_', '-') + '-01'\n",
    "all_years['date'] = pd.to_datetime(all_years['date'])\n",
    "\n",
    "# Keep as first 10 years only\n",
    "all_years = all_years.loc[all_years['date']< pd.to_datetime('2025-11-01')]\n",
    "# Pivot the dataframe to spread 'class' values as columns\n",
    "all_years_pivot = all_years.pivot(index='file', columns='class', values='values')\n",
    "\n",
    "# Reset index if you want 'file' as a regular column instead of index\n",
    "all_years_pivot = all_years.pivot_table(index='date', columns='class', values='values', fill_value=0)\n",
    "all_years_pivot = all_years_pivot.reset_index()\n",
    "\n",
    "\n",
    "\n",
    "# Calculate the fraction each class makes up of the total\n",
    "class_fractions = all_years.groupby('class')['values'].sum()\n",
    "class_fractions = (class_fractions / class_fractions.sum()).round(2)\n",
    "class_fractions = class_fractions.reset_index()\n",
    "class_fractions.columns = ['class', 'fraction']\n",
    "\n",
    "class_fractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_years_pivot.loc[all_years_pivot['date']==pd.to_datetime('2022-02-01')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ggplot(all_years_pivot, aes(x = 'date', y = 'residential'))  + geom_line() + labs(\n",
    "    title = 'The total residential property which has not been\\nassigned an LSOA') + scale_x_date(\n",
    "     breaks=pd.date_range(start=all_years_pivot['date'].min(), end=all_years_pivot['date'].max(), freq='YE'),\n",
    "     date_labels='%Y'  # Format to show only year\n",
    " ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Total over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_residential_df, total_per_region_df, total_incorp_df, total_resi_lad_df = create_summarised_stats(list_of_files, active_class_var,\n",
    "expansion_threshold=1000)\n",
    "\n",
    "# keep only the first 10 years\n",
    "total_residential_df = total_residential_df.loc[total_residential_df['date']< pd.to_datetime('2025-11-01')]\n",
    "total_per_region_df  = total_per_region_df.loc[total_per_region_df['date']< pd.to_datetime('2025-11-01')]\n",
    "total_incorp_df  = total_incorp_df.loc[total_incorp_df['date']< pd.to_datetime('2025-11-01')]\n",
    "total_resi_lad_df  = total_resi_lad_df.loc[total_resi_lad_df['date']< pd.to_datetime('2025-11-01')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_resi_lad_df.sort_values('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_resi_lad_df.loc[total_resi_lad_df['date']==pd.to_datetime('2025-08-01')].groupby('lad11cd')['counts'].sum().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perc_per_lad = total_resi_lad_df.loc[total_resi_lad_df['date']==pd.to_datetime('2025-08-01')].groupby('lad11cd')['counts'].sum().sort_values().reset_index()\n",
    "\n",
    "perc_per_lad['perc'] = perc_per_lad['counts']/perc_per_lad['counts'].sum()\n",
    "\n",
    "perc_per_lad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_per_region_df.groupby(active_class_var)['counts'].sum()/total_per_region_df['counts'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearly_df = total_residential_df.groupby('date')['counts'].sum().reset_index()\n",
    "\n",
    "ggplot(yearly_df, \n",
    "aes(x = 'date', y = 'counts')) + geom_line()  +   scale_x_date(\n",
    "     breaks=pd.date_range(start=total_residential_df['date'].min(), end=total_residential_df['date'].max(), freq='YE'),\n",
    "     date_labels='%Y'  # Format to show only year\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearly_df.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "total_residential_df['type'] = np.where(total_residential_df['is_multi'], 'Multi', 'Single')\n",
    "\n",
    "p = ggplot(total_residential_df, aes(x = 'date', \n",
    "y = 'counts', \n",
    "color = 'type') )+ geom_line() + labs(\n",
    "    title = \"Total number of residential properties\") +   scale_x_date(\n",
    "     breaks=pd.date_range(start=total_residential_df['date'].min(), end=total_residential_df['date'].max(), freq='Y'),\n",
    "     date_labels='%Y'  # Format to show only year\n",
    " )\n",
    "\n",
    "p.save(filename = figures_folder /'total_properties.png')\n",
    "\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fract_total = total_per_region_df.loc[total_per_region_df['class']=='residential', ].groupby('region')['counts'].mean().reset_index()\n",
    "print(fract_total)\n",
    "\n",
    "# Create a copy and add percentage column\n",
    "result_df = fract_total.copy()\n",
    "result_df['percentage'] = (fract_total['counts'] / fract_total['counts'].sum()).round(2) * 100\n",
    "print(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "p = ggplot(total_per_region_df.groupby(['region', 'date'])['counts'].sum().reset_index(), aes(x='date', y='counts', color='region')) + \\\n",
    "    geom_line() + \\\n",
    "    labs(title=\"Total number of properties\") + \\\n",
    "    scale_x_date(\n",
    "        breaks=pd.date_range(start=total_residential_df['date'].min(), end=total_residential_df['date'].max(), freq='Y'),\n",
    "        date_labels='%Y'\n",
    "    ) \n",
    "\n",
    "p.save(filename = figures_folder /'properties_by_region.png')\n",
    "\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_per_class_df = total_per_region_df.groupby([active_class_var, 'date'])['counts'].sum().reset_index()\n",
    "\n",
    "p = ggplot(total_per_class_df.loc[total_per_class_df[active_class_var]!='residential'], aes(x = 'date', \n",
    "y = 'counts', \n",
    "color = active_class_var) )+ geom_line() + labs(\n",
    "    title = \"Total properties by class excluding residential\") +   scale_x_date(\n",
    "     breaks=pd.date_range(start=total_residential_df['date'].min(), end=total_residential_df['date'].max(), freq='Y'),\n",
    "     date_labels='%Y'  # Format to show only year\n",
    " )\n",
    "\n",
    "p.save(filename = figures_folder /'properties_by_class.png')\n",
    "\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_class = total_per_class_df.pivot(index = 'date', columns = active_class_var, values = 'counts')\n",
    "\n",
    "temp_class['res_perc'] = temp_class['residential']/temp_class.sum(axis = 1)\n",
    "temp_class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Country of incorporation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_incorp_df['country_incorporated'] = total_incorp_df['country_incorporated'].str.replace(\"BRITISH VIRGIN ISLANDS\", \"BVI\")\n",
    "\n",
    "country_totals = total_incorp_df.groupby('country_incorporated')['counts'].mean()\n",
    "\n",
    "top_10_countries = country_totals.nlargest(20).index\n",
    "\n",
    "# The top four dominate so massively there is no point in having anyone else\n",
    "top_4_countries = country_totals.nlargest(4).index\n",
    "\n",
    "filtered_df = total_incorp_df[total_incorp_df['country_incorporated'].isin(top_4_countries)]\n",
    "\n",
    "print(\"Top 10 countries by total counts:\")\n",
    "print(country_totals.nlargest(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total Fraction the top four make up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_incorp_df.loc[total_incorp_df['country_incorporated'].isin(['BVI', 'JERSEY', 'GUERNSEY', 'ISLO OF MAN']),'counts'].sum()/total_incorp_df['counts'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = ggplot(filtered_df, aes(x = 'date', \n",
    "y = 'counts', \n",
    "color = 'country_incorporated') )+ geom_line() + labs(\n",
    "    title = \"Total number of properties\\nby country of incorporation\",\n",
    "    color = 'Country'\n",
    "    ) +   scale_x_date(\n",
    "     breaks=pd.date_range(start=filtered_df['date'].min(), end=filtered_df['date'].max(), freq='Y'),\n",
    "     date_labels='%Y'  # Format to show only year\n",
    " ) \n",
    "\n",
    "p.save(filename = figures_folder /'properties_by_incorporation.png')\n",
    "\n",
    "p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = total_incorp_df.copy()\n",
    "\n",
    "test['diff'] = (test['counts'] - test.groupby('country_incorporated')['counts'].shift())/test.groupby('country_incorporated')['counts'].shift()\n",
    "\n",
    "country_totals = test.groupby('country_incorporated')['diff'].mean()\n",
    "\n",
    "# Step 2: Get the top 10 countries\n",
    "top_10_countries = country_totals.nlargest(20).index\n",
    "\n",
    "print(country_totals.nlargest(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = ggplot(test[test['country_incorporated'].isin(['LUXEMBOURG', 'NETHERLANDS', 'GERMANY'])], aes(x = 'date', \n",
    "y = 'counts', \n",
    "color = 'country_incorporated') )+ geom_line() + labs(\n",
    "    title = \"Total number of properties\\nby country of incorporation\",\n",
    "    ) +   scale_x_date(\n",
    "     breaks=pd.date_range(start=test['date'].min(), end=test['date'].max(), freq='Y'),\n",
    "     date_labels='%Y'  # Format to show only year\n",
    " ) \n",
    "\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sorted_all = total_resi_lad_df.groupby(['lad11cd', 'date'])['counts'].sum().reset_index()\n",
    "df_sorted_all = df_sorted_all.sort_values(['lad11cd', 'date'])\n",
    "df_sorted_all['counts_diff'] = df_sorted_all.groupby(['lad11cd'])['counts'].diff()\n",
    "df_sorted_all['is_multi'] = 'all'\n",
    "\n",
    "\n",
    "df_sorted = total_resi_lad_df.sort_values(['lad11cd', 'date'])\n",
    "\n",
    "# Calculate the difference in counts between consecutive dates for each LAD11cd\n",
    "df_sorted['counts_diff'] = df_sorted.groupby(['lad11cd','is_multi'])['counts'].diff()\n",
    "\n",
    "\n",
    "df_sorted['is_multi'] = np.where(df_sorted['is_multi'], 'multi', 'single')\n",
    "\n",
    "df_sorted = pd.concat([df_sorted, df_sorted_all], ignore_index = True)\n",
    "\n",
    "df_diff_change = df_sorted.groupby(['is_multi', 'lad11cd' ])['counts_diff'].mean().reset_index()\n",
    "df_diff_change['total_mean_change'] = df_diff_change['counts_diff'] * total_resi_lad_df['date'].nunique()\n",
    "\n",
    "df_diff_change.sort_values('total_mean_change')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diff_change = df_sorted.groupby(['is_multi', 'lad11cd' ])['counts_diff'].mean().reset_index()\n",
    "df_diff_change['total_mean_change'] = df_diff_change['counts_diff'] * total_resi_lad_df['date'].nunique()\n",
    "\n",
    "df_diff_change.sort_values('total_mean_change')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import time \n",
    "\n",
    "def load_with_retry(url, retries=3):\n",
    "    for i in range(retries):\n",
    "        try:\n",
    "            print(f\"Attempt {i+1}/{retries}\")\n",
    "            gdf = gpd.read_file(url)\n",
    "            if len(gdf) > 0:\n",
    "                return gdf\n",
    "        except Exception as e:\n",
    "            print(f\"Failed: {e}\")\n",
    "            if i < retries-1:\n",
    "                time.sleep(2 ** i)  # 1, 2, 4 second delays\n",
    "    raise Exception(\"All retry attempts failed\")\n",
    "\n",
    "# Direct WFS URL with correct parameters\n",
    "wfs_url = \"https://dservices1.arcgis.com/ESMARspQHYMw9BZ9/arcgis/services/Local_Authority_Districts_May_2022_UK_BSC_V3/WFSServer?service=WFS&version=2.0.0&request=GetFeature&typeName=Local_Authority_Districts_May_2022_UK_BSC_V3:LAD_MAY_2022_UK_BSC_V3&outputFormat=GEOJSON&srsName=EPSG:4326\"\n",
    "\n",
    "try:\n",
    "    gdf = load_with_retry(wfs_url)\n",
    "    print(f\"Success! Loaded {len(gdf)} records\")\n",
    "except Exception as e:\n",
    "    print(f\"Final failure: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to calculate months between dates\n",
    "def months_between(date1, date2):\n",
    "    return (date2.year - date1.year) * 12 + (date2.month - date1.month)\n",
    "\n",
    "# Process 'all' group\n",
    "df_sorted_all = total_resi_lad_df.groupby(['lad11cd', 'date'])['counts'].sum().reset_index()\n",
    "df_sorted_all = df_sorted_all.sort_values(['lad11cd', 'date'])\n",
    "df_sorted_all['counts_diff'] = df_sorted_all.groupby(['lad11cd'])['counts'].diff()\n",
    "\n",
    "# Calculate months between consecutive dates\n",
    "df_sorted_all['prev_date'] = df_sorted_all.groupby(['lad11cd'])['date'].shift(1)\n",
    "df_sorted_all['months_diff'] = df_sorted_all.apply(\n",
    "    lambda row: months_between(row['prev_date'], row['date']) if pd.notna(row['prev_date']) else np.nan, \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Calculate monthly change (divide by number of months)\n",
    "df_sorted_all['monthly_change'] = df_sorted_all['counts_diff'] / df_sorted_all['months_diff']\n",
    "df_sorted_all['is_multi'] = 'all'\n",
    "\n",
    "# Process main dataframe\n",
    "df_sorted = total_resi_lad_df.sort_values(['lad11cd', 'date'])\n",
    "\n",
    "# Calculate the difference in counts between consecutive dates for each LAD11cd\n",
    "df_sorted['counts_diff'] = df_sorted.groupby(['lad11cd','is_multi'])['counts'].diff()\n",
    "\n",
    "# Calculate months between consecutive dates\n",
    "df_sorted['prev_date'] = df_sorted.groupby(['lad11cd', 'is_multi'])['date'].shift(1)\n",
    "df_sorted['months_diff'] = df_sorted.apply(\n",
    "    lambda row: months_between(row['prev_date'], row['date']) if pd.notna(row['prev_date']) else np.nan, \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Calculate monthly change (divide by number of months)\n",
    "df_sorted['monthly_change'] = df_sorted['counts_diff'] / df_sorted['months_diff']\n",
    "\n",
    "df_sorted['is_multi'] = np.where(df_sorted['is_multi'], 'multi', 'single')\n",
    "\n",
    "# Use monthly_change instead of counts_diff for concatenation\n",
    "df_sorted_all_final = df_sorted_all[['lad11cd', 'date', 'monthly_change', 'is_multi']]\n",
    "df_sorted_final = df_sorted[['lad11cd', 'date', 'monthly_change', 'is_multi']]\n",
    "\n",
    "df_sorted_combined = pd.concat([df_sorted_final, df_sorted_all_final], ignore_index=True)\n",
    "\n",
    "# Calculate mean monthly change\n",
    "df_diff_change = df_sorted_combined.groupby(['is_multi', 'lad11cd'])['monthly_change'].mean().reset_index()\n",
    "\n",
    "# Calculate total months in dataset\n",
    "min_date = total_resi_lad_df['date'].min()\n",
    "max_date = total_resi_lad_df['date'].max()\n",
    "total_months = months_between(min_date, max_date)\n",
    "\n",
    "# Multiply by total months instead of unique dates\n",
    "df_diff_change['total_mean_change'] = df_diff_change['monthly_change'] * total_months\n",
    "\n",
    "df_diff_change.sort_values('total_mean_change')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = df_diff_change.copy()\n",
    "\n",
    "temp['increase'] = (temp['total_mean_change']>0).astype(int)\n",
    "\n",
    "temp.groupby('is_multi')['increase'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Filter gdf to only include areas with LAD23CD containing \"E\" or \"W\" Remove silly isles for visual compactness\n",
    "gdf_filtered = gdf[gdf[LAD_COLUMN_CODE].str.contains('E|W', na=False) & (gdf[LAD_COLUMN_CODE]!='E06000053')]\n",
    "\n",
    "# Create subplots with light blue background\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 10), facecolor='#E6F3FF')\n",
    "\n",
    "# Categories to plot\n",
    "categories = ['multi', 'single']\n",
    "\n",
    "# Plot each category\n",
    "for i, category in enumerate(categories):\n",
    "    # Merge for each category\n",
    "    gdf_temp = gdf_filtered.merge(df_diff_change.loc[df_diff_change['is_multi']==category], \n",
    "                                  left_on=LAD_COLUMN_CODE, \n",
    "                                  right_on='lad11cd', \n",
    "                                  how='left')\n",
    "    \n",
    "    # Fill missing values with 0\n",
    "    gdf_temp['total_mean_change'] = gdf_temp['total_mean_change'].fillna(0)\n",
    "    \n",
    "    # Apply transformation\n",
    "    gdf_temp['total_mean_change'] = np.where(gdf_temp['total_mean_change'] < -500, -500, gdf_temp['total_mean_change'])\n",
    "    \n",
    "    # Set light blue background for axes\n",
    "    axes[i].set_facecolor('#E6F3FF')\n",
    "    \n",
    "    # Plot base map with light gray fill and black borders\n",
    "    gdf_filtered.plot(ax=axes[i], \n",
    "                      facecolor='lightgray', \n",
    "                      edgecolor='black', \n",
    "                      linewidth=0.3,\n",
    "                      alpha=0.3)\n",
    "    \n",
    "    # Remove rows with NaN values for ranking\n",
    "    gdf_temp_clean = gdf_temp.dropna(subset=['total_mean_change'])\n",
    "    \n",
    "    # Get top 10 highest values\n",
    "    top_10_highest = gdf_temp_clean.nlargest(20, 'total_mean_change')\n",
    "    \n",
    "    # Get top 10 lowest values  \n",
    "    top_10_lowest = gdf_temp_clean.nsmallest(20, 'total_mean_change')\n",
    "    \n",
    "    # Get centroids for point plotting\n",
    "    top_10_highest_centroids = top_10_highest.geometry.centroid\n",
    "    top_10_lowest_centroids = top_10_lowest.geometry.centroid\n",
    "    \n",
    "    # Plot highest values as red circles\n",
    "    axes[i].scatter(top_10_highest_centroids.x, \n",
    "                    top_10_highest_centroids.y,\n",
    "                    c='red', \n",
    "                    s=100, \n",
    "                    alpha=0.8,\n",
    "                    edgecolor='black',\n",
    "                    linewidth=1,\n",
    "                    label='Increase',\n",
    "                    zorder=5)\n",
    "    \n",
    "    # Plot lowest values as blue circles\n",
    "    axes[i].scatter(top_10_lowest_centroids.x, \n",
    "                    top_10_lowest_centroids.y,\n",
    "                    c='blue', \n",
    "                    s=100, \n",
    "                    alpha=0.8,\n",
    "                    edgecolor='black',\n",
    "                    linewidth=1,\n",
    "                    label='Decrease',\n",
    "                    zorder=5)\n",
    "    \n",
    "    # Add legend only to the second plot\n",
    "    if i == 1:\n",
    "        axes[i].legend(loc='upper right', \n",
    "                      bbox_to_anchor=(1.15, 1),\n",
    "                      fontsize=12,\n",
    "                      frameon=True,\n",
    "                      fancybox=True,\n",
    "                      shadow=True)\n",
    "    \n",
    "    # Turn off axis\n",
    "    axes[i].set_axis_off()\n",
    "\n",
    "# Add titles manually using fig.text for perfect alignment\n",
    "fig.text(0.25, 0.92, 'Multi', fontsize=14, fontweight='bold', ha='center')\n",
    "fig.text(0.75, 0.92, 'Single', fontsize=14, fontweight='bold', ha='center')\n",
    "\n",
    "# Overall title\n",
    "fig.suptitle('Top 20 Highest and Lowest Change in property counts at local authority level', \n",
    "             fontsize=16, fontweight='bold', y=0.98)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(figures_folder /'change_maps.png')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_filtered.merge(df_diff_change.loc[df_diff_change['is_multi']=='multi'], \n",
    "                                  left_on=LAD_COLUMN_CODE, \n",
    "                                  right_on='lad11cd', \n",
    "                                  how='left')['total_mean_change'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = gdf_filtered.merge(df_diff_change.loc[df_diff_change['is_multi']=='all'], \n",
    "                                  left_on=LAD_COLUMN_CODE, \n",
    "                                  right_on='lad11cd', \n",
    "                                  how='left').sort_values('total_mean_change')\n",
    "\n",
    "temp.loc[temp['total_mean_change'].notna(), [\"LAD22NM\", \"total_mean_change\"]].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# concentration\n",
    "\n",
    "As we can see the majority of change is concentrated into a very small number of local authorities. The below shows the fraction the 20 LADS with the biggest increases make up of the total increase, and the 20 with the biggest decrease make up of all decreases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.loc[temp['total_mean_change'].notna() & (temp['total_mean_change']<0), [\"LAD22NM\", \"total_mean_change\"]].head(20)['total_mean_change'].sum()/ \\\n",
    "    temp.loc[temp['total_mean_change'].notna() & (temp['total_mean_change']<0),'total_mean_change'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.loc[temp['total_mean_change'].notna() & (temp['total_mean_change']>0), [\"LAD22NM\", \"total_mean_change\"]].tail(20)['total_mean_change'].sum()/ \\\n",
    "    temp.loc[temp['total_mean_change'].notna() & (temp['total_mean_change']>0),'total_mean_change'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lad_price_df = create_mean_difference_by_groups( ['lad11cd', 'is_multi'], \n",
    "ocod_path = OCOD_history_path, \n",
    "class_var = active_class_var,\n",
    "expansion_threshold=1000,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lad_price_all_df = create_mean_difference_by_groups( ['lad11cd'], ocod_path = OCOD_history_path,\n",
    "class_var = active_class_var,\n",
    "expansion_threshold=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lad_price_df.groupby(['ratio_significantly_different', 'is_multi']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lad_price_df.pivot(columns = 'is_multi', \n",
    "index = 'lad11cd', \n",
    "values = 'mean_weighted_difference').rename(columns ={False:\"single_price\", True:\"nested_price\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lad_price_all_df.rename(columns = {'mean_weighted_difference':'all_offshore_change', 'mean_unweighted_difference':'general_property_change'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_change_data = lad_price_df.pivot(columns = 'is_multi', \n",
    "index = 'lad11cd', \n",
    "values = 'mean_weighted_difference').rename(columns ={False:\"single_price\", True:\"nested_price\"})\n",
    "\n",
    "price_change_data_all = lad_price_all_df.rename(columns = {'mean_weighted_difference':'all_offshore_change', 'mean_unweighted_difference':'general_property_change'})\n",
    "price_change_data_all['price_ratio'] = price_change_data_all['all_offshore_change']/price_change_data_all['general_property_change']\n",
    "\n",
    "volume_change_data = df_diff_change.loc[:, ['is_multi','lad11cd', 'total_mean_change']].pivot(\n",
    "    columns = 'is_multi', index ='lad11cd', values = 'total_mean_change')\n",
    "\n",
    "full_data_compare = price_change_data.merge(volume_change_data, on = 'lad11cd').merge(\n",
    "price_change_data_all, on = 'lad11cd'\n",
    ").fillna(0)\n",
    "\n",
    "conditions = [\n",
    "    full_data_compare['price_ratio'] > 1.05,\n",
    "    full_data_compare['price_ratio'] < 0.95\n",
    "]\n",
    "choices = ['increase', 'decrease']\n",
    "\n",
    "# Create categorical column\n",
    "full_data_compare['price_change'] = np.select(conditions, choices, default='stable')\n",
    "\n",
    "conditions = [\n",
    "    full_data_compare['all'] > 10,\n",
    "    full_data_compare['all'] < -10\n",
    "]\n",
    "choices = ['increase', 'decrease']\n",
    "\n",
    "# Create categorical column\n",
    "full_data_compare['volume_change'] = np.select(conditions, choices, default='stable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(full_data_compare[['all', 'price_ratio']]).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create crosstab with better labels\n",
    "crosstab = pd.crosstab(\n",
    "    full_data_compare['price_change'] ,\n",
    "    full_data_compare['volume_change'] ,\n",
    "    rownames=['volume increase'],\n",
    "    colnames=['relative_price increase'],\n",
    "    margins=False,\n",
    "    normalize = False\n",
    ")\n",
    "print(crosstab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create crosstab with better labels\n",
    "crosstab = pd.crosstab(\n",
    "    full_data_compare['price_change'] ,\n",
    "    full_data_compare['volume_change'] ,\n",
    "    rownames=['volume increase'],\n",
    "    colnames=['relative_price increase'],\n",
    "    margins=False,\n",
    "    normalize = True\n",
    ")\n",
    "print(crosstab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  model analayis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "# Find all 'overall' CSVs in the model_performance directory\n",
    "performance_dir = Path(\"../data/model_performance\")\n",
    "overall_files = list(performance_dir.glob(\"*overall*.csv\"))\n",
    "\n",
    "# Load and concatenate all relevant CSVs\n",
    "dfs = []\n",
    "for file in overall_files:\n",
    "    df = pd.read_csv(file)\n",
    "    df['source_file'] = file.name  # Optionally add a column to indicate the source\n",
    "    dfs.append(df)\n",
    "\n",
    "\n",
    "combined = pd.concat(dfs, ignore_index=True)[['precision', 'recall', 'f1_score', 'source_file']]\n",
    "combined['prepocessed'] = combined['source_file'].str.contains('preprocessed')\n",
    "combined[['precision', 'recall', 'f1_score',]] = combined[['precision', 'recall', 'f1_score',]].round(2)\n",
    "combined['model_type'] = np.where(\n",
    "    combined['source_file'].str.contains('devset'), \n",
    "    'conventional', \n",
    "    np.where(\n",
    "        combined['source_file'].str.contains('regex'), \n",
    "        'regex', \n",
    "        'weak-learning'\n",
    "    )\n",
    ")\n",
    "combined = combined[['model_type',  'prepocessed', 'f1_score','precision', 'recall']].sort_values('model_type')\n",
    "combined.rename(columns={'f1_score': 'F1', 'model_type': 'Model Type',\n",
    " 'prepocessed': 'Preprocessed', 'precision': 'Precision', 'recall': 'Recall'}, inplace=True)\n",
    "# Print the LaTeX table\n",
    "latex_table = combined.to_latex(\n",
    "    index=False, \n",
    "    float_format='{:.2f}'.format,\n",
    "    caption='Performance Comparison of Different Model Configurations',\n",
    "    label='tab:model_performance',\n",
    "    position='htbp',\n",
    "    escape=False\n",
    ")\n",
    "\n",
    "table_file = figures_folder / 'model_performance_table.tex'\n",
    "with open(table_file, 'w') as f:\n",
    "    f.write(latex_table)\n",
    "\n",
    "print(latex_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class_table = pd.read_csv('../data/model_performance/test_original_fullset_class_performance.csv')\n",
    "class_table = class_table[['class_name', 'precision', 'recall', 'f1_score', 'support']]\n",
    "class_table.rename(columns={'f1_score': 'F1', 'precision': 'Precision', 'recall': 'Recall', 'class_name': active_class_var }, inplace=True)\n",
    "class_table[active_class_var] = class_table[active_class_var].str.replace(\"_\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "latex_table = class_table.to_latex(\n",
    "    index=False, \n",
    "    float_format='{:.2f}'.format,\n",
    "    caption='Performance is high across all classes indicating a well trained, reliable model',\n",
    "    label='tab:class_performance',\n",
    "    position='htbp',\n",
    "    escape=False\n",
    ")\n",
    "\n",
    "\n",
    "table_file = figures_folder / 'class_performance_table.tex'\n",
    "with open(table_file, 'w') as f:\n",
    "    f.write(latex_table)\n",
    "\n",
    "print(latex_table)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classification_predictions(gt_df, pred_df):\n",
    "    \"\"\"\n",
    "    Evaluate classification predictions and return classification report.\n",
    "\n",
    "    Args:\n",
    "        gt_df: DataFrame with columns ['title_number', 'class'] containing ground truth\n",
    "        pred_df: DataFrame with columns ['title_number', 'class'] containing predictions\n",
    "\n",
    "    Returns:\n",
    "        str: Classification report\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import (\n",
    "        f1_score, \n",
    "        precision_score, \n",
    "        recall_score, \n",
    "        classification_report\n",
    "    )\n",
    "    \n",
    "    # Join the dataframes on title_number\n",
    "    merged_df = gt_df.merge(pred_df, on='title_number', suffixes=('_true', '_pred'))\n",
    "    \n",
    "    # Check if we lost any examples in the merge\n",
    "    if len(merged_df) != len(gt_df):\n",
    "        print(f\"Warning: Ground truth has {len(gt_df)} examples, but only {len(merged_df)} matched predictions\")\n",
    "    \n",
    "    # Extract true and predicted labels\n",
    "    y_true = merged_df['class_true'].tolist()\n",
    "    y_pred = merged_df['class_pred'].tolist()\n",
    "    \n",
    "    # Calculate metrics (using macro average to match typical classification evaluation)\n",
    "    overall_f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    overall_precision = precision_score(y_true, y_pred, average='macro')\n",
    "    overall_recall = recall_score(y_true, y_pred, average='macro')\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Overall F1: {overall_f1:.4f}\")\n",
    "    print(f\"Overall Precision: {overall_precision:.4f}\")\n",
    "    print(f\"Overall Recall: {overall_recall:.4f}\")\n",
    "    print(\"\\nPer-class results:\")\n",
    "    print(classification_report(y_true, y_pred))\n",
    "\n",
    "    class_dict = classification_report(y_true, y_pred, output_dict=True )\n",
    "    df = pd.DataFrame(class_dict).transpose()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_gt = pd.read_csv(data_folder / 'training_data' / 'parsed_ground_truth_complete.csv').drop_duplicates(['title_number'])\n",
    "class_gt['class'] = class_gt['truth']\n",
    "class_gt['class'] = class_gt['class'].str.replace('domestic', 'residential')\n",
    "\n",
    "class_pred_data_raw = pd.read_parquet(OCOD_history_path /'OCOD_FULL_2022_01.parquet')\n",
    "class_pred_data = class_pred_data_raw.loc[class_pred_data_raw['title_number'].isin(class_gt['title_number']), \n",
    "['title_number', 'class']].drop_duplicates('title_number')\n",
    "class_pred_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " evaluate_classification_predictions(\n",
    "    class_gt, class_pred_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = class_gt.merge(class_pred_data, on= 'title_number', suffixes = ['_gt', '_pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.loc[temp['class_gt']!= temp['class_pred']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def property_class_no_match(df):\n",
    "\n",
    "    df[\"class_no_match\"] = np.where(\n",
    "        (df[\"class\"] == \"unknown\") & \n",
    "        df['street_number'].notna(),\n",
    "        \"residential\",\n",
    "        df[\"class\"]\n",
    "    )\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_pred_data2 =property_class_no_match(class_pred_data_raw)\n",
    "class_pred_data2['class2'] = class_pred_data2['class_no_match']\n",
    "class_pred_data3 = class_pred_data2.loc[class_pred_data2['title_number'].isin(class_gt['title_number']), \n",
    "['title_number', 'class']].drop_duplicates('title_number')\n",
    "\n",
    "evaluate_classification_predictions(\n",
    "    class_gt, class_pred_data3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_pred_data_raw.loc[class_pred_data_raw['property_address'].str.contains('vault', case = False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = class_pred_data2.loc[class_pred_data2['class']=='residential'].merge(class_gt[['title_number', 'class']].drop_duplicates(), on = ['title_number'],\n",
    "suffixes = ['_pred', '_gt'])\n",
    "\n",
    "temp['business'] = temp['property_address'].str.contains(\n",
    "                r\"cinema|hotel|office|centre|\\bpub|holiday(?:\\s)?inn|travel lodge|travelodge|medical|business|cafe|^shop| shop|service|logistics|building supplies|restaurant|home|^store(?:s)?\\b|^storage\\b|company|ltd|limited|plc|retail|leisure|industrial|hall of|trading|commercial|technology|works|club,|advertising|school|church|(?:^room)\",\n",
    "                case=False,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp['business'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.loc[temp['class_gt'] == 'business'].to_csv(data_folder /'errors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crosstab = pd.crosstab(temp['class_gt'], temp['class_pred'])\n",
    "print(crosstab)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
