{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inference_utils import parse_addresses_from_csv, convert_to_entity_dataframe\n",
    "from address_parsing_helper_functions import (load_and_prep_OCOD_data, parsing_and_expansion_process, post_process_expanded_data, load_postocde_district_lookup)\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "model_path = \"models/address_parser/checkpoint-750\"\n",
    "csv_path = \"data/ocod_history/OCOD_FULL_2015_10.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocod_data = load_and_prep_OCOD_data(\"data/ocod_history/OCOD_FULL_2015_10.zip\", csv_filename=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from models/address_parser/checkpoint-750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cuda. Labels: ['B-building_name', 'B-city', 'B-filter_type', 'B-postcode', 'B-street_name', 'B-street_number', 'B-unit_id', 'B-unit_type', 'I-building_name', 'I-city', 'I-filter_type', 'I-postcode', 'I-street_name', 'I-street_number', 'I-unit_id', 'I-unit_type', 'O']\n",
      "Processing 99349 addresses in batches of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 49/49 [09:43<00:00, 11.91s/batch]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from models/address_parser/checkpoint-750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cuda. Labels: ['B-building_name', 'B-city', 'B-filter_type', 'B-postcode', 'B-street_name', 'B-street_number', 'B-unit_id', 'B-unit_type', 'I-building_name', 'I-city', 'I-filter_type', 'I-postcode', 'I-street_name', 'I-street_number', 'I-unit_id', 'I-unit_type', 'O']\n",
      "Processing 99349 addresses in batches of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|          | 0/49 [00:06<?, ?batch/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m      4\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 6\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mparse_addresses_from_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mocod_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m\u001b[49m\u001b[43mtarget_column\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mproperty_address\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2048\u001b[39;49m\n\u001b[1;32m     11\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Load existing file\u001b[39;00m\n",
      "File \u001b[0;32m~/enhance_ocod/inference_utils.py:257\u001b[0m, in \u001b[0;36mparse_addresses_from_csv\u001b[0;34m(df, model_path, target_column, index_column, csv_filename, batch_size)\u001b[0m\n\u001b[1;32m    255\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m][j]\n\u001b[1;32m    256\u001b[0m tokens \u001b[38;5;241m=\u001b[39m parser\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mconvert_ids_to_tokens(input_ids)\n\u001b[0;32m--> 257\u001b[0m predicted_labels \u001b[38;5;241m=\u001b[39m [parser\u001b[38;5;241m.\u001b[39mid2label[pred\u001b[38;5;241m.\u001b[39mitem()] \u001b[38;5;28;01mfor\u001b[39;00m pred \u001b[38;5;129;01min\u001b[39;00m predicted_token_class_ids[j]]\n\u001b[1;32m    258\u001b[0m offset_mapping \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moffset_mapping\u001b[39m\u001b[38;5;124m\"\u001b[39m][j]\n\u001b[1;32m    260\u001b[0m \u001b[38;5;66;03m# Extract entities\u001b[39;00m\n",
      "File \u001b[0;32m~/enhance_ocod/inference_utils.py:257\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    255\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m][j]\n\u001b[1;32m    256\u001b[0m tokens \u001b[38;5;241m=\u001b[39m parser\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mconvert_ids_to_tokens(input_ids)\n\u001b[0;32m--> 257\u001b[0m predicted_labels \u001b[38;5;241m=\u001b[39m [parser\u001b[38;5;241m.\u001b[39mid2label[\u001b[43mpred\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m] \u001b[38;5;28;01mfor\u001b[39;00m pred \u001b[38;5;129;01min\u001b[39;00m predicted_token_class_ids[j]]\n\u001b[1;32m    258\u001b[0m offset_mapping \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moffset_mapping\u001b[39m\u001b[38;5;124m\"\u001b[39m][j]\n\u001b[1;32m    260\u001b[0m \u001b[38;5;66;03m# Extract entities\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if os.path.exists('data/test_results_df.parquet'):\n",
    "\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "\n",
    "    results = parse_addresses_from_csv(\n",
    "    df = ocod_data,\n",
    "    model_path=model_path,\n",
    "    target_column=\"property_address\",\n",
    "    batch_size=2048\n",
    "    )\n",
    "\n",
    "    end_time = time.time()\n",
    "    # Load existing file\n",
    "    test = pd.read_parquet('data/test_results_df.parquet')\n",
    "    print(\"Loaded existing test results from file\")\n",
    "else:\n",
    "    # Create new dataframe and save it\n",
    "    test = convert_to_entity_dataframe(results)\n",
    "    test.to_parquet('data/test_results_df.parquet')\n",
    "    print(\"Created new test results and saved to file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added missing columns: ['street_name', 'filter_type', 'city']\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test = parsing_and_expansion_process(all_entities = test)\n",
    "ocod_data2 = post_process_expanded_data(test, ocod_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import re\n",
    "ONSPD_path = \"data/ONSPD_FEB_2025.zip\"\n",
    "zip_file = zipfile.ZipFile(ONSPD_path)\n",
    "target_zipped_file = [i for i in zip_file.namelist() if re.search(r'^Data/ONSPD.+csv$',i)][0]\n",
    "postcode_district_lookup = load_postocde_district_lookup(ONSPD_path, target_zipped_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MODEL LABEL CONFIGURATION ===\n",
      "Number of labels: 17\n",
      "id2label mapping: {0: 'O', 1: 'B-building_name', 2: 'I-building_name', 3: 'B-street_name', 4: 'I-street_name', 5: 'B-street_number', 6: 'I-street_number', 7: 'B-filter_type', 8: 'I-filter_type', 9: 'B-unit_id', 10: 'I-unit_id', 11: 'B-unit_type', 12: 'I-unit_type', 13: 'B-city', 14: 'I-city', 15: 'B-postcode', 16: 'I-postcode'}\n",
      "label2id mapping: {'B-building_name': 1, 'B-city': 13, 'B-filter_type': 7, 'B-postcode': 15, 'B-street_name': 3, 'B-street_number': 5, 'B-unit_id': 9, 'B-unit_type': 11, 'I-building_name': 2, 'I-city': 14, 'I-filter_type': 8, 'I-postcode': 16, 'I-street_name': 4, 'I-street_number': 6, 'I-unit_id': 10, 'I-unit_type': 12, 'O': 0}\n",
      "\n",
      "=== LABEL COMPARISON ===\n",
      "Expected labels: ['building_name', 'street_name', 'street_number', 'filter_type', 'unit_id', 'unit_type', 'city', 'postcode']\n",
      "Model entity types: ['building_name', 'city', 'filter_type', 'postcode', 'street_name', 'street_number', 'unit_id', 'unit_type']\n",
      "Missing from model: set()\n",
      "Extra in model: set()\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "def check_model_labels(model_path):\n",
    "    \"\"\"Check what labels your trained model actually contains\"\"\"\n",
    "    \n",
    "    # Load the model\n",
    "    model = AutoModelForTokenClassification.from_pretrained(model_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    \n",
    "    print(\"=== MODEL LABEL CONFIGURATION ===\")\n",
    "    print(f\"Number of labels: {model.config.num_labels}\")\n",
    "    print(f\"id2label mapping: {model.config.id2label}\")\n",
    "    print(f\"label2id mapping: {model.config.label2id}\")\n",
    "    \n",
    "    expected_labels = [\n",
    "        \"building_name\", \"street_name\", \"street_number\", \"filter_type\",\n",
    "        \"unit_id\", \"unit_type\", \"city\", \"postcode\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\n=== LABEL COMPARISON ===\")\n",
    "    model_labels = set(model.config.label2id.keys())\n",
    "    \n",
    "    # Remove BIO prefixes and 'O' for comparison\n",
    "    model_entity_types = set()\n",
    "    for label in model_labels:\n",
    "        if label.startswith('B-') or label.startswith('I-'):\n",
    "            model_entity_types.add(label[2:])  # Remove B- or I- prefix\n",
    "    \n",
    "    print(f\"Expected labels: {expected_labels}\")\n",
    "    print(f\"Model entity types: {sorted(model_entity_types)}\")\n",
    "    print(f\"Missing from model: {set(expected_labels) - model_entity_types}\")\n",
    "    print(f\"Extra in model: {model_entity_types - set(expected_labels)}\")\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "# Check your model\n",
    "model, tokenizer = check_model_labels(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TRAINING DATA ANALYSIS ===\n",
      "Total examples: 2000\n",
      "Total spans: 11176\n",
      "\n",
      "Label distribution:\n",
      "  building_name: 797 (7.1%)\n",
      "  city: 1937 (17.3%)\n",
      "  number_filter: 50 (0.4%)\n",
      "  postcode: 1618 (14.5%)\n",
      "  street_name: 2133 (19.1%)\n",
      "  street_number: 2939 (26.3%)\n",
      "  unit_id: 743 (6.6%)\n",
      "  unit_type: 959 (8.6%)\n",
      "\n",
      "✅ All expected labels found in training data\n"
     ]
    }
   ],
   "source": [
    "def analyze_training_data(json_path):\n",
    "    \"\"\"Analyze what labels are actually in your training data\"\"\"\n",
    "    \n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    print(\"=== TRAINING DATA ANALYSIS ===\")\n",
    "    print(f\"Total examples: {len(data)}\")\n",
    "    \n",
    "    # Count all labels\n",
    "    label_counts = Counter()\n",
    "    total_spans = 0\n",
    "    \n",
    "    for example in data:\n",
    "        for span in example.get('spans', []):\n",
    "            label_counts[span['label']] += 1\n",
    "            total_spans += 1\n",
    "    \n",
    "    print(f\"Total spans: {total_spans}\")\n",
    "    print(\"\\nLabel distribution:\")\n",
    "    for label, count in sorted(label_counts.items()):\n",
    "        percentage = (count / total_spans) * 100\n",
    "        print(f\"  {label}: {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Check for missing expected labels\n",
    "    expected_labels = [\n",
    "        \"building_name\", \"street_name\", \"street_number\", \"number_filter\",\n",
    "        \"unit_id\", \"unit_type\", \"city\", \"postcode\"\n",
    "    ]\n",
    "    \n",
    "    found_labels = set(label_counts.keys())\n",
    "    missing_labels = set(expected_labels) - found_labels\n",
    "    \n",
    "    if missing_labels:\n",
    "        print(f\"\\n⚠️  MISSING LABELS IN TRAINING DATA: {missing_labels}\")\n",
    "    else:\n",
    "        print(f\"\\n✅ All expected labels found in training data\")\n",
    "    \n",
    "    return label_counts\n",
    "\n",
    "# Analyze your training data\n",
    "label_counts = analyze_training_data(\"data/training_data/training_data_dev.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (973226428.py, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[17], line 11\u001b[0;36m\u001b[0m\n\u001b[0;31m    return_offsets_mapping=True,test\u001b[0m\n\u001b[0m                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "def debug_single_prediction(model, tokenizer, address):\n",
    "    \"\"\"Debug what the model predicts for a single address\"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(\n",
    "        address,\n",
    "        return_tensors=\"pt\",\n",
    "        return_offsets_mapping=True,test\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=128\n",
    "    )\n",
    "    \n",
    "    # Move to device (excluding offset_mapping)\n",
    "    model_inputs = {k: v.to(device) for k, v in inputs.items() if k != 'offset_mapping'}\n",
    "    \n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**model_inputs)\n",
    "        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "        predicted_ids = predictions.argmax(dim=-1)\n",
    "    \n",
    "    # Get tokens and labels\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "    predicted_labels = [model.config.id2label[pred.item()] for pred in predicted_ids[0]]\n",
    "    offset_mapping = inputs[\"offset_mapping\"][0]\n",
    "    \n",
    "    print(f\"=== DEBUGGING: {address} ===\")\n",
    "    print(\"Token -> Label -> Text_Span\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for i, (token, label, offset) in enumerate(zip(tokens, predicted_labels, offset_mapping)):\n",
    "        if token in ['[CLS]', '[SEP]', '[PAD]']:\n",
    "            continue\n",
    "        \n",
    "        start_pos, end_pos = offset.tolist()\n",
    "        if start_pos == 0 and end_pos == 0 and i > 0:\n",
    "            text_span = \"[SPECIAL]\"\n",
    "        else:\n",
    "            text_span = address[start_pos:end_pos]\n",
    "        \n",
    "        print(f\"{token:15} -> {label:15} -> '{text_span}'\")\n",
    "\n",
    "# Test with an address that should have street_name\n",
    "test_address = \"161, 163, 165, 167 and 169 uxbridge road, ealing\"\n",
    "debug_single_prediction(model, tokenizer, test_address)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
