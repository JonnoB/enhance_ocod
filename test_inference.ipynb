{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing parser...\n",
      "Loading model and tokenizer...\n",
      "AddressParserInference initialized: cuda, FP16=True\n",
      "✓ Parser initialized successfully\n",
      "\n",
      "Testing address: '36 - 49, chapel street, London, se45 6pq'\n",
      "\n",
      "==================================================\n",
      "RESULTS:\n",
      "==================================================\n",
      "✓ Original address: 36 - 49, chapel street, London, se45 6pq\n",
      "✓ Number of entities found: 3\n",
      "\n",
      "Entities found:\n",
      "  1. street_number: '36 - 49' (confidence: 1.000)\n",
      "  2. street_name: 'chapel street' (confidence: 1.000)\n",
      "  3. postcode: 'se45 6pq' (confidence: 1.000)\n",
      "\n",
      "Parsed components:\n",
      "  street_number: 36 - 49\n",
      "  street_name: chapel street\n",
      "  postcode: se45 6pq\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# test_single_address.py\n",
    "\n",
    "def test_single_address():\n",
    "    # Initialize your parser\n",
    "    model_path = \"models/address_parser_dev/final_model\"  # Replace with your actual model path\n",
    "    \n",
    "    try:\n",
    "        print(\"Initializing parser...\")\n",
    "        parser = AddressParserInference(\n",
    "            model_path=model_path,\n",
    "            max_length=512,\n",
    "            stride=50,\n",
    "            use_fp16=True  # Set to False for debugging to avoid GPU issues\n",
    "        )\n",
    "        print(\"✓ Parser initialized successfully\")\n",
    "        \n",
    "        # Test address\n",
    "        test_address = \"36 - 49, chapel street, London, se45 6pq\"\n",
    "        print(f\"\\nTesting address: '{test_address}'\")\n",
    "        \n",
    "        # Make prediction\n",
    "        result = parser.predict_single_address(test_address, row_index=0)\n",
    "        \n",
    "        # Print results\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"RESULTS:\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        if \"error\" in result:\n",
    "            print(f\"❌ ERROR: {result['error']}\")\n",
    "        else:\n",
    "            print(f\"✓ Original address: {result['original_address']}\")\n",
    "            print(f\"✓ Number of entities found: {len(result['entities'])}\")\n",
    "            \n",
    "            if result['entities']:\n",
    "                print(\"\\nEntities found:\")\n",
    "                for i, entity in enumerate(result['entities']):\n",
    "                    print(f\"  {i+1}. {entity['type']}: '{entity['text']}' (confidence: {entity['confidence']:.3f})\")\n",
    "                \n",
    "                print(\"\\nParsed components:\")\n",
    "                for key, value in result['parsed_components'].items():\n",
    "                    print(f\"  {key}: {value}\")\n",
    "            else:\n",
    "                print(\"⚠️  No entities found\")\n",
    "        \n",
    "        print(\"=\"*50)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to initialize or run parser: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "test_single_address()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_parquet(\"data/ocod_history_processed/OCOD_FULL_2015_10.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([None, 'great titchfield house', ' london house', ...,\n",
       "       'royal pavilion', ' the business village', 'afe'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['building_name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading common reference data...\n",
      "Postcode district lookup shape: (2406510, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/teamspace/studios/this_studio/enhance_ocod/src/enhance_ocod/locate_and_classify.py:376: DtypeWarning: Columns (13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  voa_businesses = pd.read_csv(csv_file,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial rows: 2282385\n",
      "VOA businesses shape: (2094735, 19)\n",
      "\n",
      "==================================================\n",
      "STEP 1: Load and prep OCOD data\n",
      "==================================================\n",
      "Initial OCOD data shape: (5, 7)\n",
      "Initial OCOD data columns: ['property_address', 'title_number', 'tenure', 'district', 'county', 'region', 'price_paid']\n",
      "Sample property_address values:\n",
      "0       5 to 9(odds only) odd road, london  (pj10 8df)\n",
      "1          5 to 9 (even) even road, london  (pj10 8df)\n",
      "2           5 to 9 everything road, london  (pj10 8df)\n",
      "3            6,7,10 selective road, london, (pj10 8df)\n",
      "4    Flats 5 to 9, all apartments road, london  (pj...\n",
      "Name: property_address, dtype: object\n",
      "\n",
      "==================================================\n",
      "STEP 2: Parse addresses\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing address lengths: 100%|██████████| 5/5 [00:00<00:00, 2349.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Short addresses: 5 | Long addresses: 0\n",
      "Processing 5 short addresses (batch_size=2048)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Completed 5 short addresses\n",
      "No long addresses to process\n",
      "Address parsing took 5.39 seconds\n",
      "Success rate: 100.0%\n",
      "Results keys: dict_keys(['summary', 'results'])\n",
      "Processing 21 entities into DataFrame...\n",
      "Computing label counts...\n",
      "✓ Named Entity Recognition processing complete\n",
      "Total entities extracted: 21\n",
      "Added missing columns: ['building_name', 'filter_type', 'unit_id', 'unit_type']\n"
     ]
    }
   ],
   "source": [
    "from enhance_ocod.inference import parse_addresses_pipeline, convert_to_entity_dataframe\n",
    "from enhance_ocod.address_parsing import (\n",
    "    load_and_prep_OCOD_data, parsing_and_expansion_process, post_process_expanded_data\n",
    ")\n",
    "from enhance_ocod.locate_and_classify import (\n",
    "    load_postcode_district_lookup, preprocess_expandaded_ocod_data, \n",
    "    add_missing_lads_ocod, load_voa_ratinglist, street_and_building_matching, substreet_matching,\n",
    "    counts_of_businesses_per_oa_lsoa, voa_address_match_all_data, classification_type1, classification_type2,\n",
    "    contract_ocod_after_classification\n",
    ")\n",
    "from enhance_ocod.price_paid_process import load_and_process_pricepaid_data\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "# ====== SET YOUR PATHS HERE ======\n",
    "test_file_path = \"data/simple_example.csv\"  # UPDATE THIS PATH\n",
    "model_path = \"models/address_parser_original_fullset/final_model\"\n",
    "ONSPD_path = \"data/ONSPD_FEB_2025.zip\"\n",
    "price_paid_path = \"data/price_paid_data/price_paid_complete_may_2025.csv\"\n",
    "processed_price_paid_dir = \"data/processed_price_paid\"\n",
    "voa_path = \"data/2023_non_domestic_rating_list_entries.zip\"\n",
    "\n",
    "print(\"Loading common reference data...\")\n",
    "postcode_district_lookup = load_postcode_district_lookup(str(ONSPD_path))\n",
    "print(f\"Postcode district lookup shape: {postcode_district_lookup.shape}\")\n",
    "\n",
    "voa_businesses = load_voa_ratinglist(str(voa_path), postcode_district_lookup)\n",
    "print(f\"VOA businesses shape: {voa_businesses.shape}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"STEP 1: Load and prep OCOD data\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Load and process the OCOD data\n",
    "ocod_data_1_initial = load_and_prep_OCOD_data(str(test_file_path))\n",
    "print(f\"Initial OCOD data shape: {ocod_data_1_initial.shape}\")\n",
    "print(f\"Initial OCOD data columns: {list(ocod_data_1_initial.columns)}\")\n",
    "print(f\"Sample property_address values:\\n{ocod_data_1_initial['property_address'].head()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"STEP 2: Parse addresses\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "start_time = time.time()\n",
    "results = parse_addresses_pipeline(\n",
    "    df=ocod_data_1_initial,\n",
    "    model_path=str(model_path),\n",
    "    target_column=\"property_address\",\n",
    ")\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Address parsing took {end_time - start_time:.2f} seconds\")\n",
    "print(f\"Success rate: {results['summary']['success_rate']:.1%}\")\n",
    "print(f\"Results keys: {results.keys()}\")\n",
    "\n",
    "\n",
    "\n",
    "ocod_data_2_entities = convert_to_entity_dataframe(results)\n",
    "\n",
    "\n",
    "ocod_data_3_expanded = parsing_and_expansion_process(all_entities=ocod_data_2_entities)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enhance_ocod.address_parsing import identify_multi_addresses\n",
    "import numpy as np\n",
    "\n",
    "multi_unit_id, multi_property, all_multi_ids = identify_multi_addresses(ocod_data_2_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_entities = ocod_data_2_entities\n",
    "\n",
    "# Define regex pattern to match number ranges like \"1-5\", \"10 to 20\", \"3-7\" etc.\n",
    "xx_to_yy_regex = r'^\\d+\\s?(?:-|to)\\s?\\d+$'\n",
    "\n",
    "# Create working dataframe with unique address records only\n",
    "multi_check_df = all_entities[['datapoint_id', 'text', ]].drop_duplicates()\n",
    "\n",
    "# Count commas in address text (multiple commas may indicate complex addresses)\n",
    "multi_check_df['comma_count'] = multi_check_df['text'].str.count(',')\n",
    "\n",
    "# Flag addresses that start with land-related keywords (these are typically single properties)\n",
    "multi_check_df['land'] = multi_check_df['text'].str.contains(r\"^(?:land|plot|airspace|car|parking)\", case = False)\n",
    "\n",
    "# Flag addresses containing business-related keywords (these are typically single commercial properties)\n",
    "multi_check_df['business'] = multi_check_df['text'].str.contains(r\"cinema|hotel|office|centre|\\bpub|holiday\\s?inn|travel\\s?lodge|business|cafe|^shop| shop|restaurant|home|^stores?\\b|^storage\\b|company|ltd|limited|plc|retail|leisure|industrial|hall of|trading|commercial|works\", case = False)\n",
    "\n",
    "# Create pivot table showing count of each entity label type per datapoint_id\n",
    "# This gives us counts of building_name, unit_id, street_number etc. for each address\n",
    "temp_df = all_entities[['datapoint_id', 'label']].groupby(['datapoint_id', 'label']).value_counts().to_frame(name = \"counts\").reset_index().pivot(index = 'datapoint_id', columns = 'label', values = 'counts').fillna(0)\n",
    "\n",
    "# Count how many street numbers contain range patterns (e.g., \"1-5 Main St\")\n",
    "# This indicates multiple properties at consecutive street numbers\n",
    "xx_to_yy_street_counts = all_entities['datapoint_id'][all_entities['label_text'].str.contains(\n",
    "    xx_to_yy_regex)& (all_entities['label']==\"street_number\")\n",
    "                        ].to_frame(name = 'datapoint_id').groupby('datapoint_id').size().to_frame(name = 'xx_to_yy_street_counts')\n",
    "\n",
    "# Count how many unit IDs contain range patterns (e.g., \"Units 1-5\")\n",
    "# This indicates multiple units within a single building\n",
    "xx_to_yy_unit_counts = all_entities['datapoint_id'][all_entities['label_text'].str.contains(\n",
    "    xx_to_yy_regex)& (all_entities['label']==\"unit_id\")\n",
    "                        ].to_frame(name = 'datapoint_id').groupby('datapoint_id').size().to_frame(name = 'xx_to_yy_unit_counts')\n",
    "\n",
    "# Merge all the feature dataframes together\n",
    "multi_check_df = multi_check_df.merge(temp_df, how = 'left', left_on = \"datapoint_id\", right_index = True).\\\n",
    "merge(xx_to_yy_street_counts, how = 'left', left_on = \"datapoint_id\", right_index = True).\\\n",
    "merge(xx_to_yy_unit_counts, how = 'left', left_on = \"datapoint_id\", right_index = True).fillna(0)\n",
    "\n",
    "# Ensure the necessary columns are present (some addresses may not have these entity types)\n",
    "required_columns = ['building_name', 'unit_id', 'street_number']\n",
    "for col in required_columns:\n",
    "    if col not in multi_check_df.columns:\n",
    "        multi_check_df[col] = 0\n",
    "\n",
    "\n",
    "# Classify addresses as single/multi/unknown using hierarchical logical rules\n",
    "# Order matters here - more specific conditions should come first\n",
    "multi_check_df['class'] = np.select(\n",
    "    [\n",
    "        multi_check_df['land'],  # Land/plot addresses are single properties\n",
    "        multi_check_df['business'],  # Business addresses are typically single properties\n",
    "        (multi_check_df['building_name']==1) & (multi_check_df['unit_id'] == 0),  # Single building name without units = single property\n",
    "        (multi_check_df['xx_to_yy_unit_counts']>0) | (multi_check_df['xx_to_yy_street_counts']>0),  # Range patterns in unit IDs = multiple units\n",
    "        multi_check_df['street_number']>1,  # Multiple street numbers = multiple properties\n",
    "        multi_check_df['unit_id']>1,  # Multiple unit IDs = multiple units\n",
    "        (multi_check_df['street_number']<=1) & (multi_check_df['xx_to_yy_street_counts']==0) & (multi_check_df['unit_id']<=1)  # Single street number, no ranges, single/no unit = single property\n",
    "    ], \n",
    "    [\n",
    "        'single',\n",
    "        'single', \n",
    "        'single',\n",
    "        'multi',\n",
    "        'multi',\n",
    "        'multi',\n",
    "        'single',\n",
    "    ], \n",
    "    default='unknown'  # Fallback for edge cases\n",
    ")\n",
    "\n",
    "# With the classification complete, extract the required ID lists\n",
    "\n",
    "# Multi-unit addresses: multiple properties that are individual units (apartments, flats, etc.)\n",
    "multi_unit_id = set(multi_check_df['datapoint_id'][(multi_check_df['class']=='multi') &( multi_check_df['unit_id']>0)].tolist())\n",
    "\n",
    "# Multi-property addresses: multiple properties but not individual units (e.g., multiple houses)\n",
    "multi_property = set(multi_check_df['datapoint_id'][(multi_check_df['class']=='multi') &( multi_check_df['unit_id']==0)].tolist())\n",
    "\n",
    "# Combined list of all multi-address IDs\n",
    "all_multi_ids = list(multi_unit_id) +list(multi_property)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['5 to 9', ' odd road', ' london', 'pj10 8df', '5 to 9', ' even road',\\n       ' london', 'pj10 8df', '5 to 9', ' everything road', ' london',\\n       'pj10 8df', '6', '7', '10', ' selective road', 'pj10 8df', ' 5 to 9',\\n       ' all apartments road', ' london', 'pj10 8df'],\\n      dtype='object')] are in the [index]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mall_entities\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdatapoint_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mall_entities\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabel_text\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/pandas/core/series.py:1072\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1069\u001b[0m     key \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(key, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m)\n\u001b[1;32m   1070\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_rows_with_mask(key)\n\u001b[0;32m-> 1072\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_with\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/pandas/core/series.py:1113\u001b[0m, in \u001b[0;36mSeries._get_with\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1110\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miloc[key]\n\u001b[1;32m   1112\u001b[0m \u001b[38;5;66;03m# handle the dup indexing case GH#4246\u001b[39;00m\n\u001b[0;32m-> 1113\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/pandas/core/indexing.py:1153\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1150\u001b[0m axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1152\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[0;32m-> 1153\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/pandas/core/indexing.py:1382\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1379\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mndim\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1380\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot index with multidimensional key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1382\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_iterable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;66;03m# nested tuple slicing\u001b[39;00m\n\u001b[1;32m   1385\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_nested_tuple(key, labels):\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/pandas/core/indexing.py:1322\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_iterable\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_key(key, axis)\n\u001b[1;32m   1321\u001b[0m \u001b[38;5;66;03m# A collection of keys\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m keyarr, indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_listlike_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_reindex_with_indexers(\n\u001b[1;32m   1324\u001b[0m     {axis: [keyarr, indexer]}, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_dups\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1325\u001b[0m )\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/pandas/core/indexing.py:1520\u001b[0m, in \u001b[0;36m_LocIndexer._get_listlike_indexer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1517\u001b[0m ax \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis(axis)\n\u001b[1;32m   1518\u001b[0m axis_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis_name(axis)\n\u001b[0;32m-> 1520\u001b[0m keyarr, indexer \u001b[38;5;241m=\u001b[39m \u001b[43max\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m keyarr, indexer\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/pandas/core/indexes/base.py:6115\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6112\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6113\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6115\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6117\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6119\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/pandas/core/indexes/base.py:6176\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6174\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_interval_msg:\n\u001b[1;32m   6175\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 6176\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6178\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m   6179\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Index(['5 to 9', ' odd road', ' london', 'pj10 8df', '5 to 9', ' even road',\\n       ' london', 'pj10 8df', '5 to 9', ' everything road', ' london',\\n       'pj10 8df', '6', '7', '10', ' selective road', 'pj10 8df', ' 5 to 9',\\n       ' all apartments road', ' london', 'pj10 8df'],\\n      dtype='object')] are in the [index]\""
     ]
    }
   ],
   "source": [
    "all_entities['datapoint_id'][all_entities['label_text']]\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocod_data_4_postprocessed = post_process_expanded_data(ocod_data_3_expanded, ocod_data_1_initial)\n",
    "\n",
    "# Clean up intermediate results\n",
    "del results, ocod_data_2_entities, ocod_data_3_expanded\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "ocod_data_5_preprocessed = preprocess_expandaded_ocod_data(ocod_data_4_postprocessed, postcode_district_lookup)\n",
    "\n",
    "\n",
    "price_paid_df = load_and_process_pricepaid_data(\n",
    "    file_path=str(price_paid_path), \n",
    "    processed_dir=processed_price_paid_dir,\n",
    "    postcode_district_lookup=postcode_district_lookup, \n",
    "    years_needed=[2017, 2018, 2019]\n",
    ")\n",
    "\n",
    "\n",
    "ocod_data_6_with_lads = add_missing_lads_ocod(ocod_data_5_preprocessed, price_paid_df)\n",
    "\n",
    "\n",
    "ocod_data_7_street_matched = street_and_building_matching(ocod_data_6_with_lads, price_paid_df, voa_businesses)\n",
    "\n",
    "\n",
    "ocod_data_8_substreet_matched = substreet_matching(ocod_data_7_street_matched, price_paid_df, voa_businesses)\n",
    "\n",
    "# Clean up price paid data\n",
    "del price_paid_df\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "ocod_data_9_with_counts = counts_of_businesses_per_oa_lsoa(ocod_data_8_substreet_matched, voa_businesses)\n",
    "\n",
    "\n",
    "ocod_data_10_voa_matched = voa_address_match_all_data(ocod_data_9_with_counts, voa_businesses)\n",
    "\n",
    "ocod_data_11_class1 = classification_type1(ocod_data_10_voa_matched)\n",
    "\n",
    "ocod_data_12_class2 = classification_type2(ocod_data_11_class1)\n",
    "\n",
    "ocod_data_13_contracted = contract_ocod_after_classification(ocod_data_12_class2, class_type='class2', classes=['residential'])\n",
    "\n",
    "\n",
    "columns = ['title_number', 'within_title_id', 'within_larger_title', 'unique_id', \n",
    "           'unit_id', 'unit_type', 'building_name', 'street_number', 'street_name', \n",
    "           'postcode', 'city', 'district', 'region', 'property_address', 'oa11cd', \n",
    "           'lsoa11cd', 'msoa11cd', 'lad11cd', 'class', 'class2']\n",
    "\n",
    "ocod_data_14_final = ocod_data_13_contracted.loc[:, columns].rename(columns={\n",
    "    'within_title_id': 'nested_id',\n",
    "    'within_larger_title': 'nested_title'\n",
    "})\n",
    "\n",
    "print(f\"Final data shape: {ocod_data_14_final.shape}\")\n",
    "print(f\"Final data columns: {list(ocod_data_14_final.columns)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"PROCESSING COMPLETE!\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mresults\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
