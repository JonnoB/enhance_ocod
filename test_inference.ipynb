{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enhance_ocod.inference_utils import parse_addresses_from_csv, convert_to_entity_dataframe\n",
    "from enhance_ocod.address_parsing_helper_functions import (load_and_prep_OCOD_data, parsing_and_expansion_process, post_process_expanded_data)\n",
    "from enhance_ocod.locate_and_classify_helper_functions import (load_postcode_district_lookup, preprocess_expandaded_ocod_data, \n",
    "                                                  add_missing_lads_ocod, load_voa_ratinglist, street_and_building_matching, substreet_matching,\n",
    "                                                  counts_of_businesses_per_oa_lsoa, voa_address_match_all_data, classification_type1, classification_type2,\n",
    "                                                  contract_ocod_after_classification)\n",
    "from enhance_ocod.price_paid_process import load_and_process_pricepaid_data\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "model_path = \"models/address_parser_full/checkpoint-35286\" #checkpoint-750\"\n",
    "model_path = \"models/address_parser/checkpoint-750\" \n",
    "csv_path = \"data/training_data/ground_truth_test_set_labels.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enhance_ocod.address_parsing_helper_functions import load_cleaned_labels\n",
    "temp = load_cleaned_labels(\"data/training_data/full_dataset_no_overlaps.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>label_text</th>\n",
       "      <th>property_address</th>\n",
       "      <th>datapoint_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>street_number</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>161</td>\n",
       "      <td>161, 163, 165, 167 and 169 uxbridge road, ealing</td>\n",
       "      <td>13755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>street_number</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>163</td>\n",
       "      <td>161, 163, 165, 167 and 169 uxbridge road, ealing</td>\n",
       "      <td>13755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>street_number</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>165</td>\n",
       "      <td>161, 163, 165, 167 and 169 uxbridge road, ealing</td>\n",
       "      <td>13755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>street_number</td>\n",
       "      <td>15</td>\n",
       "      <td>18</td>\n",
       "      <td>167</td>\n",
       "      <td>161, 163, 165, 167 and 169 uxbridge road, ealing</td>\n",
       "      <td>13755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>street_number</td>\n",
       "      <td>23</td>\n",
       "      <td>26</td>\n",
       "      <td>169</td>\n",
       "      <td>161, 163, 165, 167 and 169 uxbridge road, ealing</td>\n",
       "      <td>13755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33343</th>\n",
       "      <td>street_name</td>\n",
       "      <td>20</td>\n",
       "      <td>24</td>\n",
       "      <td>john</td>\n",
       "      <td>118 metcalfe court, john harrison way, london ...</td>\n",
       "      <td>67996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33344</th>\n",
       "      <td>street_number</td>\n",
       "      <td>25</td>\n",
       "      <td>33</td>\n",
       "      <td>harrison</td>\n",
       "      <td>118 metcalfe court, john harrison way, london ...</td>\n",
       "      <td>67996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33345</th>\n",
       "      <td>street_name</td>\n",
       "      <td>34</td>\n",
       "      <td>37</td>\n",
       "      <td>way</td>\n",
       "      <td>118 metcalfe court, john harrison way, london ...</td>\n",
       "      <td>67996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33346</th>\n",
       "      <td>city</td>\n",
       "      <td>39</td>\n",
       "      <td>45</td>\n",
       "      <td>london</td>\n",
       "      <td>118 metcalfe court, john harrison way, london ...</td>\n",
       "      <td>67996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33347</th>\n",
       "      <td>postcode</td>\n",
       "      <td>47</td>\n",
       "      <td>55</td>\n",
       "      <td>se10 0bz</td>\n",
       "      <td>118 metcalfe court, john harrison way, london ...</td>\n",
       "      <td>67996</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33348 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               label  start  end label_text  \\\n",
       "0      street_number      0    3        161   \n",
       "1      street_number      5    8        163   \n",
       "2      street_number     10   13        165   \n",
       "3      street_number     15   18        167   \n",
       "4      street_number     23   26        169   \n",
       "...              ...    ...  ...        ...   \n",
       "33343    street_name     20   24       john   \n",
       "33344  street_number     25   33   harrison   \n",
       "33345    street_name     34   37        way   \n",
       "33346           city     39   45     london   \n",
       "33347       postcode     47   55   se10 0bz   \n",
       "\n",
       "                                        property_address  datapoint_id  \n",
       "0       161, 163, 165, 167 and 169 uxbridge road, ealing         13755  \n",
       "1       161, 163, 165, 167 and 169 uxbridge road, ealing         13755  \n",
       "2       161, 163, 165, 167 and 169 uxbridge road, ealing         13755  \n",
       "3       161, 163, 165, 167 and 169 uxbridge road, ealing         13755  \n",
       "4       161, 163, 165, 167 and 169 uxbridge road, ealing         13755  \n",
       "...                                                  ...           ...  \n",
       "33343  118 metcalfe court, john harrison way, london ...         67996  \n",
       "33344  118 metcalfe court, john harrison way, london ...         67996  \n",
       "33345  118 metcalfe court, john harrison way, london ...         67996  \n",
       "33346  118 metcalfe court, john harrison way, london ...         67996  \n",
       "33347  118 metcalfe court, john harrison way, london ...         67996  \n",
       "\n",
       "[33348 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth_df = pd.read_csv(csv_path)\n",
    "\n",
    "#I only need a small number of the columns to be able to calculate the F1 score\n",
    "#Everything else just makes it confusing. \n",
    "#renaming is for consistancy\n",
    "ground_truth_df = ground_truth_df.rename(\n",
    "    columns = {'input:text':'property_address',\n",
    "              'input:datapoint_id':'datapoint_id',\n",
    "              'text':'label_text'})\n",
    "\n",
    "ground_truth_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 33348 addresses in batches of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|          | 0/17 [00:00<?, ?batch/s]/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:236: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.\n",
      "  warnings.warn(\n",
      "W0615 06:07:58.803000 19446 /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_inductor/utils.py:1250] [1/0] Not enough SMs to use max_autotune_gemm mode\n",
      "Processing batches: 100%|██████████| 17/17 [01:32<00:00,  5.45s/batch]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "results = parse_addresses_from_csv(\n",
    "    df=ground_truth_df,\n",
    "    model_path=str(model_path),\n",
    "    target_column=\"property_address\",\n",
    "    batch_size=2048\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 33348 addresses in batches of 6144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 6/6 [01:28<00:00, 14.82s/batch]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "results = parse_addresses_from_csv(\n",
    "    df=ground_truth_df,\n",
    "    model_path=str(model_path),\n",
    "    target_column=\"property_address\",\n",
    "    batch_size=6144\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import zipfile\n",
    "import json\n",
    "from pathlib import Path\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "from datasets import Dataset\n",
    "from typing import Dict, List, Optional, Union, Callable\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from enhance_ocod.inference_utils import AddressParserInference\n",
    "from itertools import islice\n",
    "import json\n",
    "import time\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "import torch.multiprocessing as mp\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, Optional\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_addresses_from_csv_with_profiling(\n",
    "    df: pd.DataFrame,\n",
    "    model_path: str,\n",
    "    target_column: str = \"address\",\n",
    "    index_column: Optional[str] = None,\n",
    "    batch_size: int = 64,\n",
    "    stream_results_path: Optional[str] = None\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Complete function with timing to identify bottlenecks\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    parser = AddressParserInference(model_path)\n",
    "    \n",
    "    # Validation\n",
    "    if target_column not in df.columns:\n",
    "        raise ValueError(f\"Column '{target_column}' not found. Available columns: {list(df.columns)}\")\n",
    "    \n",
    "    if index_column is None and \"datapoint_id\" in df.columns:\n",
    "        index_column = \"datapoint_id\"\n",
    "    \n",
    "    if index_column:\n",
    "        if index_column not in df.columns:\n",
    "            raise ValueError(f\"Index column '{index_column}' not found. Available columns: {list(df.columns)}\")\n",
    "    \n",
    "    # Use numpy arrays\n",
    "    addresses = df[target_column].fillna(\"\").astype(str).values\n",
    "    indices = df[index_column].values if index_column else df.index.values\n",
    "    has_datapoint_id = \"datapoint_id\" in df.columns\n",
    "    datapoint_ids = df[\"datapoint_id\"].values if has_datapoint_id else None\n",
    "    \n",
    "    total_addresses = len(addresses)\n",
    "    print(f\"Processing {total_addresses} addresses in batches of {batch_size}\")\n",
    "    \n",
    "    # Streaming setup\n",
    "    all_results = [] if stream_results_path is None else None\n",
    "    result_file = open(stream_results_path, \"w\") if stream_results_path else None\n",
    "    successful_parses = 0\n",
    "    \n",
    "    # Timing variables\n",
    "    time_tokenization = 0\n",
    "    time_gpu_inference = 0\n",
    "    time_cpu_transfer = 0\n",
    "    time_entity_extraction = 0\n",
    "    time_result_building = 0\n",
    "    time_cleanup = 0\n",
    "    \n",
    "    # Clean batching\n",
    "    def batch_arrays(*arrays):\n",
    "        for i in range(0, len(arrays[0]), batch_size):\n",
    "            yield tuple(arr[i:i+batch_size] for arr in arrays)\n",
    "    \n",
    "    arrays_to_batch = [addresses, indices]\n",
    "    if has_datapoint_id:\n",
    "        arrays_to_batch.append(datapoint_ids)\n",
    "    \n",
    "    for batch in tqdm(batch_arrays(*arrays_to_batch), \n",
    "                     total=(total_addresses + batch_size - 1) // batch_size,\n",
    "                     desc=\"Processing batches\", unit=\"batch\"):\n",
    "        \n",
    "        if has_datapoint_id:\n",
    "            batch_addresses, batch_indices, batch_datapoint_ids = batch\n",
    "        else:\n",
    "            batch_addresses, batch_indices = batch\n",
    "            batch_datapoint_ids = None\n",
    "        \n",
    "        # TIME: Tokenization\n",
    "        start_time = time.time()\n",
    "        inputs = parser.tokenizer(\n",
    "            list(batch_addresses),\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=128,  \n",
    "            return_tensors=\"pt\",\n",
    "            return_offsets_mapping=True\n",
    "        )\n",
    "        model_inputs = {k: v.to(parser.device) for k, v in inputs.items() if k != 'offset_mapping'}\n",
    "        time_tokenization += time.time() - start_time\n",
    "        \n",
    "        # TIME: GPU Inference\n",
    "        start_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            outputs = parser.model(**model_inputs)\n",
    "            predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "            predicted_token_class_ids = predictions.argmax(dim=-1)\n",
    "        time_gpu_inference += time.time() - start_time\n",
    "        \n",
    "        # TIME: CPU Transfer\n",
    "        start_time = time.time()\n",
    "        predicted_token_class_ids_cpu = predicted_token_class_ids.cpu()\n",
    "        input_ids_cpu = inputs[\"input_ids\"].cpu()\n",
    "        offset_mapping_cpu = inputs[\"offset_mapping\"]\n",
    "        \n",
    "        # Cache frequently accessed objects\n",
    "        id2label = parser.id2label\n",
    "        tokenizer = parser.tokenizer\n",
    "        time_cpu_transfer += time.time() - start_time\n",
    "        \n",
    "        # TIME: Entity Extraction (individual processing)\n",
    "        start_time = time.time()\n",
    "        batch_results = []\n",
    "        for j, address in enumerate(batch_addresses):\n",
    "            index = batch_indices[j]\n",
    "            \n",
    "            try:\n",
    "                input_ids = input_ids_cpu[j]\n",
    "                tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "                predicted_labels = [id2label[pred.item()] \n",
    "                                  for pred in predicted_token_class_ids_cpu[j]]\n",
    "                offset_mapping = offset_mapping_cpu[j]\n",
    "                \n",
    "                # This is likely the bottleneck\n",
    "                entities = parser._extract_entities(address, tokens, predicted_labels, offset_mapping)\n",
    "                if len(entities) > 0:\n",
    "                    successful_parses += 1\n",
    "                    \n",
    "            except Exception as e:\n",
    "                entities = []\n",
    "                print(f\"Warning: Failed to parse address at index {index}: {str(e)}\")\n",
    "            \n",
    "            batch_results.append((index, address, entities, batch_datapoint_ids[j] if has_datapoint_id else None))\n",
    "        \n",
    "        time_entity_extraction += time.time() - start_time\n",
    "        \n",
    "        # TIME: Result Building\n",
    "        start_time = time.time()\n",
    "        for index, address, entities, datapoint_id in batch_results:\n",
    "            result = {\n",
    "                \"row_index\": index,\n",
    "                \"original_address\": address,\n",
    "                \"entities\": entities,\n",
    "                \"parsed_components\": parser._group_entities_by_type(entities)\n",
    "            }\n",
    "            \n",
    "            if has_datapoint_id:\n",
    "                result[\"datapoint_id\"] = datapoint_id\n",
    "            \n",
    "            # Store or stream result\n",
    "            if all_results is not None:\n",
    "                all_results.append(result)\n",
    "            if result_file:\n",
    "                result_file.write(json.dumps(result) + \"\\n\")\n",
    "        \n",
    "        time_result_building += time.time() - start_time\n",
    "        \n",
    "        # TIME: GPU Cleanup\n",
    "        start_time = time.time()\n",
    "        del outputs, predictions, predicted_token_class_ids, predicted_token_class_ids_cpu, input_ids_cpu\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        time_cleanup += time.time() - start_time\n",
    "    \n",
    "    if result_file:\n",
    "        result_file.close()\n",
    "    \n",
    "    # Print timing results\n",
    "    total_time = time_tokenization + time_gpu_inference + time_cpu_transfer + time_entity_extraction + time_result_building + time_cleanup\n",
    "    print(f\"\\n=== TIMING BREAKDOWN ===\")\n",
    "    print(f\"Tokenization:      {time_tokenization:.2f}s ({time_tokenization/total_time*100:.1f}%)\")\n",
    "    print(f\"GPU Inference:     {time_gpu_inference:.2f}s ({time_gpu_inference/total_time*100:.1f}%)\")\n",
    "    print(f\"CPU Transfer:      {time_cpu_transfer:.2f}s ({time_cpu_transfer/total_time*100:.1f}%)\")\n",
    "    print(f\"Entity Extraction: {time_entity_extraction:.2f}s ({time_entity_extraction/total_time*100:.1f}%)\")\n",
    "    print(f\"Result Building:   {time_result_building:.2f}s ({time_result_building/total_time*100:.1f}%)\")\n",
    "    print(f\"Cleanup:           {time_cleanup:.2f}s ({time_cleanup/total_time*100:.1f}%)\")\n",
    "    print(f\"Total:             {total_time:.2f}s\")\n",
    "    \n",
    "    # Build summary\n",
    "    summary = {\n",
    "        \"total_addresses\": total_addresses,\n",
    "        \"successful_parses\": successful_parses,\n",
    "        \"failed_parses\": total_addresses - successful_parses,\n",
    "        \"success_rate\": successful_parses / total_addresses if total_addresses > 0 else 0,\n",
    "        \"timing\": {\n",
    "            \"tokenization\": time_tokenization,\n",
    "            \"gpu_inference\": time_gpu_inference,\n",
    "            \"cpu_transfer\": time_cpu_transfer,\n",
    "            \"entity_extraction\": time_entity_extraction,\n",
    "            \"result_building\": time_result_building,\n",
    "            \"cleanup\": time_cleanup,\n",
    "            \"total\": total_time\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        \"summary\": summary,\n",
    "        \"results\": all_results if all_results is not None else f\"Streamed to {stream_results_path}\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 33348 addresses in batches of 6144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 6/6 [01:29<00:00, 14.87s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TIMING BREAKDOWN ===\n",
      "Tokenization:      3.74s (4.2%)\n",
      "GPU Inference:     0.25s (0.3%)\n",
      "CPU Transfer:      73.73s (82.7%)\n",
      "Entity Extraction: 11.30s (12.7%)\n",
      "Result Building:   0.06s (0.1%)\n",
      "Cleanup:           0.12s (0.1%)\n",
      "Total:             89.20s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "results = parse_addresses_from_csv_with_profiling(\n",
    "    df=ground_truth_df,\n",
    "    model_path=str(model_path),\n",
    "    target_column=\"property_address\",\n",
    "    batch_size=6144\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Optional' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mparse_addresses_from_csv_gpu_optimized\u001b[39m(\n\u001b[1;32m      2\u001b[0m     df: pd\u001b[38;5;241m.\u001b[39mDataFrame,\n\u001b[1;32m      3\u001b[0m     model_path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m      4\u001b[0m     target_column: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maddress\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m----> 5\u001b[0m     index_column: \u001b[43mOptional\u001b[49m[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m      6\u001b[0m     batch_size: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m,\n\u001b[1;32m      7\u001b[0m     stream_results_path: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m      8\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict:\n\u001b[1;32m      9\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03m    GPU-optimized version - minimal CPU transfers\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mitertools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m islice\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Optional' is not defined"
     ]
    }
   ],
   "source": [
    "def parse_addresses_from_csv_gpu_optimized(\n",
    "    df: pd.DataFrame,\n",
    "    model_path: str,\n",
    "    target_column: str = \"address\",\n",
    "    index_column: Optional[str] = None,\n",
    "    batch_size: int = 64,\n",
    "    stream_results_path: Optional[str] = None\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    GPU-optimized version - minimal CPU transfers\n",
    "    \"\"\"\n",
    "    from itertools import islice\n",
    "    import json\n",
    "    import time\n",
    "    \n",
    "    parser = AddressParserInference(model_path)\n",
    "    \n",
    "    # Validation (same as before)\n",
    "    if target_column not in df.columns:\n",
    "        raise ValueError(f\"Column '{target_column}' not found. Available columns: {list(df.columns)}\")\n",
    "    \n",
    "    if index_column is None and \"datapoint_id\" in df.columns:\n",
    "        index_column = \"datapoint_id\"\n",
    "    \n",
    "    if index_column and index_column not in df.columns:\n",
    "        raise ValueError(f\"Index column '{index_column}' not found. Available columns: {list(df.columns)}\")\n",
    "    \n",
    "    # Setup (same as before)\n",
    "    addresses = df[target_column].fillna(\"\").astype(str).values\n",
    "    indices = df[index_column].values if index_column else df.index.values\n",
    "    has_datapoint_id = \"datapoint_id\" in df.columns\n",
    "    datapoint_ids = df[\"datapoint_id\"].values if has_datapoint_id else None\n",
    "    \n",
    "    total_addresses = len(addresses)\n",
    "    print(f\"Processing {total_addresses} addresses in batches of {batch_size}\")\n",
    "    \n",
    "    all_results = [] if stream_results_path is None else None\n",
    "    result_file = open(stream_results_path, \"w\") if stream_results_path else None\n",
    "    successful_parses = 0\n",
    "    \n",
    "    # Timing\n",
    "    time_tokenization = 0\n",
    "    time_gpu_inference = 0\n",
    "    time_individual_processing = 0\n",
    "    time_result_building = 0\n",
    "    \n",
    "    def batch_arrays(*arrays):\n",
    "        for i in range(0, len(arrays[0]), batch_size):\n",
    "            yield tuple(arr[i:i+batch_size] for arr in arrays)\n",
    "    \n",
    "    arrays_to_batch = [addresses, indices]\n",
    "    if has_datapoint_id:\n",
    "        arrays_to_batch.append(datapoint_ids)\n",
    "    \n",
    "    for batch in tqdm(batch_arrays(*arrays_to_batch), \n",
    "                     total=(total_addresses + batch_size - 1) // batch_size,\n",
    "                     desc=\"Processing batches\", unit=\"batch\"):\n",
    "        \n",
    "        if has_datapoint_id:\n",
    "            batch_addresses, batch_indices, batch_datapoint_ids = batch\n",
    "        else:\n",
    "            batch_addresses, batch_indices = batch\n",
    "            batch_datapoint_ids = None\n",
    "        \n",
    "        # Tokenization\n",
    "        start_time = time.time()\n",
    "        inputs = parser.tokenizer(\n",
    "            list(batch_addresses),\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=128,  \n",
    "            return_tensors=\"pt\",\n",
    "            return_offsets_mapping=True\n",
    "        )\n",
    "        model_inputs = {k: v.to(parser.device) for k, v in inputs.items() if k != 'offset_mapping'}\n",
    "        time_tokenization += time.time() - start_time\n",
    "        \n",
    "        # GPU Inference\n",
    "        start_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            outputs = parser.model(**model_inputs)\n",
    "            predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "            predicted_token_class_ids = predictions.argmax(dim=-1)\n",
    "        time_gpu_inference += time.time() - start_time\n",
    "        \n",
    "        # CRITICAL: Process one item at a time to avoid massive CPU transfers\n",
    "        start_time = time.time()\n",
    "        for j, address in enumerate(batch_addresses):\n",
    "            index = batch_indices[j]\n",
    "            \n",
    "            try:\n",
    "                # Transfer only single item tensors to CPU\n",
    "                input_ids_single = inputs[\"input_ids\"][j].cpu()\n",
    "                predicted_labels_single = predicted_token_class_ids[j].cpu()\n",
    "                offset_mapping_single = inputs[\"offset_mapping\"][j]\n",
    "                \n",
    "                # Convert to tokens and labels\n",
    "                tokens = parser.tokenizer.convert_ids_to_tokens(input_ids_single)\n",
    "                predicted_labels = [parser.id2label[pred.item()] for pred in predicted_labels_single]\n",
    "                \n",
    "                # Extract entities\n",
    "                entities = parser._extract_entities(address, tokens, predicted_labels, offset_mapping_single)\n",
    "                if len(entities) > 0:\n",
    "                    successful_parses += 1\n",
    "                    \n",
    "            except Exception as e:\n",
    "                entities = []\n",
    "                print(f\"Warning: Failed to parse address at index {index}: {str(e)}\")\n",
    "            \n",
    "            # Build and store result immediately\n",
    "            result = {\n",
    "                \"row_index\": index,\n",
    "                \"original_address\": address,\n",
    "                \"entities\": entities,\n",
    "                \"parsed_components\": parser._group_entities_by_type(entities)\n",
    "            }\n",
    "            \n",
    "            if has_datapoint_id:\n",
    "                result[\"datapoint_id\"] = batch_datapoint_ids[j]\n",
    "            \n",
    "            if all_results is not None:\n",
    "                all_results.append(result)\n",
    "            if result_file:\n",
    "                result_file.write(json.dumps(result) + \"\\n\")\n",
    "        \n",
    "        time_individual_processing += time.time() - start_time\n",
    "        \n",
    "        # Cleanup GPU memory\n",
    "        del outputs, predictions, predicted_token_class_ids\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    if result_file:\n",
    "        result_file.close()\n",
    "    \n",
    "    # Timing results\n",
    "    total_time = time_tokenization + time_gpu_inference + time_individual_processing\n",
    "    print(f\"\\n=== OPTIMIZED TIMING ===\")\n",
    "    print(f\"Tokenization:         {time_tokenization:.2f}s ({time_tokenization/total_time*100:.1f}%)\")\n",
    "    print(f\"GPU Inference:        {time_gpu_inference:.2f}s ({time_gpu_inference/total_time*100:.1f}%)\")\n",
    "    print(f\"Individual Processing: {time_individual_processing:.2f}s ({time_individual_processing/total_time*100:.1f}%)\")\n",
    "    print(f\"Total:                {total_time:.2f}s\")\n",
    "    \n",
    "    summary = {\n",
    "        \"total_addresses\": total_addresses,\n",
    "        \"successful_parses\": successful_parses,\n",
    "        \"failed_parses\": total_addresses - successful_parses,\n",
    "        \"success_rate\": successful_parses / total_addresses if total_addresses > 0 else 0\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        \"summary\": summary,\n",
    "        \"results\": all_results if all_results is not None else f\"Streamed to {stream_results_path}\"\n",
    "    }\n",
    "\n",
    "\n",
    "results = parse_addresses_from_csv_gpu_optimized(\n",
    "    df=ground_truth_df,\n",
    "    model_path=str(model_path),\n",
    "    target_column=\"property_address\",\n",
    "    batch_size=6144\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 33348 addresses in batches of 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 9/9 [01:30<00:00, 10.11s/batch]\n"
     ]
    }
   ],
   "source": [
    "def parse_addresses_from_csv_streaming(\n",
    "    df: pd.DataFrame,\n",
    "    model_path: str,\n",
    "    target_column: str = \"address\",\n",
    "    index_column: Optional[str] = None,\n",
    "    batch_size: int = 512,  # Much smaller!\n",
    "    stream_results_path: Optional[str] = None\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Streaming approach with smaller batches\n",
    "    \"\"\"\n",
    "    from itertools import islice\n",
    "    import json\n",
    "    import time\n",
    "    \n",
    "    parser = AddressParserInference(model_path)\n",
    "    \n",
    "    # Validation\n",
    "    if target_column not in df.columns:\n",
    "        raise ValueError(f\"Column '{target_column}' not found. Available columns: {list(df.columns)}\")\n",
    "    \n",
    "    if index_column is None and \"datapoint_id\" in df.columns:\n",
    "        index_column = \"datapoint_id\"\n",
    "    \n",
    "    if index_column and index_column not in df.columns:\n",
    "        raise ValueError(f\"Index column '{index_column}' not found. Available columns: {list(df.columns)}\")\n",
    "    \n",
    "    # Setup\n",
    "    addresses = df[target_column].fillna(\"\").astype(str).values\n",
    "    indices = df[index_column].values if index_column else df.index.values\n",
    "    has_datapoint_id = \"datapoint_id\" in df.columns\n",
    "    datapoint_ids = df[\"datapoint_id\"].values if has_datapoint_id else None\n",
    "    \n",
    "    total_addresses = len(addresses)\n",
    "    print(f\"Processing {total_addresses} addresses in batches of {batch_size}\")\n",
    "    \n",
    "    all_results = [] if stream_results_path is None else None\n",
    "    result_file = open(stream_results_path, \"w\") if stream_results_path else None\n",
    "    successful_parses = 0\n",
    "    \n",
    "    # Process in smaller chunks\n",
    "    def batch_arrays(*arrays):\n",
    "        for i in range(0, len(arrays[0]), batch_size):\n",
    "            yield tuple(arr[i:i+batch_size] for arr in arrays)\n",
    "    \n",
    "    arrays_to_batch = [addresses, indices]\n",
    "    if has_datapoint_id:\n",
    "        arrays_to_batch.append(datapoint_ids)\n",
    "    \n",
    "    for batch in tqdm(batch_arrays(*arrays_to_batch), \n",
    "                     total=(total_addresses + batch_size - 1) // batch_size,\n",
    "                     desc=\"Processing batches\", unit=\"batch\"):\n",
    "        \n",
    "        if has_datapoint_id:\n",
    "            batch_addresses, batch_indices, batch_datapoint_ids = batch\n",
    "        else:\n",
    "            batch_addresses, batch_indices = batch\n",
    "            batch_datapoint_ids = None\n",
    "        \n",
    "        # Tokenize smaller batch\n",
    "        inputs = parser.tokenizer(\n",
    "            list(batch_addresses),\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=128,  \n",
    "            return_tensors=\"pt\",\n",
    "            return_offsets_mapping=True\n",
    "        )\n",
    "        \n",
    "        model_inputs = {k: v.to(parser.device) for k, v in inputs.items() if k != 'offset_mapping'}\n",
    "        \n",
    "        # GPU inference on smaller batch\n",
    "        with torch.no_grad():\n",
    "            outputs = parser.model(**model_inputs)\n",
    "            predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "            predicted_token_class_ids = predictions.argmax(dim=-1)\n",
    "        \n",
    "        # Now CPU transfer is much smaller and manageable\n",
    "        predicted_token_class_ids_cpu = predicted_token_class_ids.cpu()\n",
    "        input_ids_cpu = inputs[\"input_ids\"].cpu()\n",
    "        offset_mapping_cpu = inputs[\"offset_mapping\"]\n",
    "        \n",
    "        # Process the smaller batch\n",
    "        for j, address in enumerate(batch_addresses):\n",
    "            index = batch_indices[j]\n",
    "            \n",
    "            try:\n",
    "                input_ids = input_ids_cpu[j]\n",
    "                tokens = parser.tokenizer.convert_ids_to_tokens(input_ids)\n",
    "                predicted_labels = [parser.id2label[pred.item()] \n",
    "                                  for pred in predicted_token_class_ids_cpu[j]]\n",
    "                offset_mapping = offset_mapping_cpu[j]\n",
    "                \n",
    "                entities = parser._extract_entities(address, tokens, predicted_labels, offset_mapping)\n",
    "                if len(entities) > 0:\n",
    "                    successful_parses += 1\n",
    "                    \n",
    "            except Exception as e:\n",
    "                entities = []\n",
    "                print(f\"Warning: Failed to parse address at index {index}: {str(e)}\")\n",
    "            \n",
    "            # Build result\n",
    "            result = {\n",
    "                \"row_index\": index,\n",
    "                \"original_address\": address,\n",
    "                \"entities\": entities,\n",
    "                \"parsed_components\": parser._group_entities_by_type(entities)\n",
    "            }\n",
    "            \n",
    "            if has_datapoint_id:\n",
    "                result[\"datapoint_id\"] = batch_datapoint_ids[j]\n",
    "            \n",
    "            # Stream immediately\n",
    "            if all_results is not None:\n",
    "                all_results.append(result)\n",
    "            if result_file:\n",
    "                result_file.write(json.dumps(result) + \"\\n\")\n",
    "        \n",
    "        # Cleanup smaller tensors\n",
    "        del outputs, predictions, predicted_token_class_ids, predicted_token_class_ids_cpu, input_ids_cpu\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    if result_file:\n",
    "        result_file.close()\n",
    "    \n",
    "    summary = {\n",
    "        \"total_addresses\": total_addresses,\n",
    "        \"successful_parses\": successful_parses,\n",
    "        \"failed_parses\": total_addresses - successful_parses,\n",
    "        \"success_rate\": successful_parses / total_addresses if total_addresses > 0 else 0\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        \"summary\": summary,\n",
    "        \"results\": all_results if all_results is not None else f\"Streamed to {stream_results_path}\"\n",
    "    }\n",
    "\n",
    "results = parse_addresses_from_csv_streaming(\n",
    "    df=ground_truth_df,\n",
    "    model_path=str(model_path),\n",
    "    target_column=\"property_address\",\n",
    "    batch_size=512\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_addresses_from_csv_streaming_with_profiling(\n",
    "    df: pd.DataFrame,\n",
    "    model_path: str,\n",
    "    target_column: str = \"address\",\n",
    "    index_column: Optional[str] = None,\n",
    "    batch_size: int = 512,\n",
    "    stream_results_path: Optional[str] = None\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Streaming approach with detailed timing\n",
    "    \"\"\"\n",
    "    from itertools import islice\n",
    "    import json\n",
    "    import time\n",
    "    \n",
    "    parser = AddressParserInference(model_path)\n",
    "    \n",
    "    # Validation\n",
    "    if target_column not in df.columns:\n",
    "        raise ValueError(f\"Column '{target_column}' not found. Available columns: {list(df.columns)}\")\n",
    "    \n",
    "    if index_column is None and \"datapoint_id\" in df.columns:\n",
    "        index_column = \"datapoint_id\"\n",
    "    \n",
    "    if index_column and index_column not in df.columns:\n",
    "        raise ValueError(f\"Index column '{index_column}' not found. Available columns: {list(df.columns)}\")\n",
    "    \n",
    "    # Setup\n",
    "    addresses = df[target_column].fillna(\"\").astype(str).values\n",
    "    indices = df[index_column].values if index_column else df.index.values\n",
    "    has_datapoint_id = \"datapoint_id\" in df.columns\n",
    "    datapoint_ids = df[\"datapoint_id\"].values if has_datapoint_id else None\n",
    "    \n",
    "    total_addresses = len(addresses)\n",
    "    print(f\"Processing {total_addresses} addresses in batches of {batch_size}\")\n",
    "    \n",
    "    all_results = [] if stream_results_path is None else None\n",
    "    result_file = open(stream_results_path, \"w\") if stream_results_path else None\n",
    "    successful_parses = 0\n",
    "    \n",
    "    # Timing variables\n",
    "    time_tokenization = 0\n",
    "    time_gpu_inference = 0\n",
    "    time_cpu_transfer = 0\n",
    "    time_entity_extraction = 0\n",
    "    time_result_building = 0\n",
    "    time_cleanup = 0\n",
    "    \n",
    "    # Process in smaller chunks\n",
    "    def batch_arrays(*arrays):\n",
    "        for i in range(0, len(arrays[0]), batch_size):\n",
    "            yield tuple(arr[i:i+batch_size] for arr in arrays)\n",
    "    \n",
    "    arrays_to_batch = [addresses, indices]\n",
    "    if has_datapoint_id:\n",
    "        arrays_to_batch.append(datapoint_ids)\n",
    "    \n",
    "    for batch in tqdm(batch_arrays(*arrays_to_batch), \n",
    "                     total=(total_addresses + batch_size - 1) // batch_size,\n",
    "                     desc=\"Processing batches\", unit=\"batch\"):\n",
    "        \n",
    "        if has_datapoint_id:\n",
    "            batch_addresses, batch_indices, batch_datapoint_ids = batch\n",
    "        else:\n",
    "            batch_addresses, batch_indices = batch\n",
    "            batch_datapoint_ids = None\n",
    "        \n",
    "        # TIME: Tokenization\n",
    "        start_time = time.time()\n",
    "        inputs = parser.tokenizer(\n",
    "            list(batch_addresses),\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=128,  \n",
    "            return_tensors=\"pt\",\n",
    "            return_offsets_mapping=True\n",
    "        )\n",
    "        model_inputs = {k: v.to(parser.device) for k, v in inputs.items() if k != 'offset_mapping'}\n",
    "        time_tokenization += time.time() - start_time\n",
    "        \n",
    "        # TIME: GPU inference\n",
    "        start_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            outputs = parser.model(**model_inputs)\n",
    "            predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "            predicted_token_class_ids = predictions.argmax(dim=-1)\n",
    "        time_gpu_inference += time.time() - start_time\n",
    "        \n",
    "        # TIME: CPU transfer (now smaller)\n",
    "        start_time = time.time()\n",
    "        predicted_token_class_ids_cpu = predicted_token_class_ids.cpu()\n",
    "        input_ids_cpu = inputs[\"input_ids\"].cpu()\n",
    "        offset_mapping_cpu = inputs[\"offset_mapping\"]\n",
    "        \n",
    "        # Cache frequently accessed objects\n",
    "        id2label = parser.id2label\n",
    "        tokenizer = parser.tokenizer\n",
    "        time_cpu_transfer += time.time() - start_time\n",
    "        \n",
    "        # TIME: Entity extraction\n",
    "        start_time = time.time()\n",
    "        batch_results = []\n",
    "        for j, address in enumerate(batch_addresses):\n",
    "            index = batch_indices[j]\n",
    "            \n",
    "            try:\n",
    "                input_ids = input_ids_cpu[j]\n",
    "                tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "                predicted_labels = [id2label[pred.item()] \n",
    "                                  for pred in predicted_token_class_ids_cpu[j]]\n",
    "                offset_mapping = offset_mapping_cpu[j]\n",
    "                \n",
    "                entities = parser._extract_entities(address, tokens, predicted_labels, offset_mapping)\n",
    "                if len(entities) > 0:\n",
    "                    successful_parses += 1\n",
    "                    \n",
    "            except Exception as e:\n",
    "                entities = []\n",
    "                print(f\"Warning: Failed to parse address at index {index}: {str(e)}\")\n",
    "            \n",
    "            batch_results.append((index, address, entities, batch_datapoint_ids[j] if has_datapoint_id else None))\n",
    "        \n",
    "        time_entity_extraction += time.time() - start_time\n",
    "        \n",
    "        # TIME: Result building\n",
    "        start_time = time.time()\n",
    "        for index, address, entities, datapoint_id in batch_results:\n",
    "            result = {\n",
    "                \"row_index\": index,\n",
    "                \"original_address\": address,\n",
    "                \"entities\": entities,\n",
    "                \"parsed_components\": parser._group_entities_by_type(entities)\n",
    "            }\n",
    "            \n",
    "            if has_datapoint_id:\n",
    "                result[\"datapoint_id\"] = datapoint_id\n",
    "            \n",
    "            # Stream immediately\n",
    "            if all_results is not None:\n",
    "                all_results.append(result)\n",
    "            if result_file:\n",
    "                result_file.write(json.dumps(result) + \"\\n\")\n",
    "        \n",
    "        time_result_building += time.time() - start_time\n",
    "        \n",
    "        # TIME: Cleanup\n",
    "        start_time = time.time()\n",
    "        del outputs, predictions, predicted_token_class_ids, predicted_token_class_ids_cpu, input_ids_cpu\n",
    "        torch.cuda.empty_cache()\n",
    "        time_cleanup += time.time() - start_time\n",
    "    \n",
    "    if result_file:\n",
    "        result_file.close()\n",
    "    \n",
    "    # Print timing results\n",
    "    total_time = time_tokenization + time_gpu_inference + time_cpu_transfer + time_entity_extraction + time_result_building + time_cleanup\n",
    "    print(f\"\\n=== STREAMING TIMING BREAKDOWN ===\")\n",
    "    print(f\"Tokenization:      {time_tokenization:.2f}s ({time_tokenization/total_time*100:.1f}%)\")\n",
    "    print(f\"GPU Inference:     {time_gpu_inference:.2f}s ({time_gpu_inference/total_time*100:.1f}%)\")\n",
    "    print(f\"CPU Transfer:      {time_cpu_transfer:.2f}s ({time_cpu_transfer/total_time*100:.1f}%)\")\n",
    "    print(f\"Entity Extraction: {time_entity_extraction:.2f}s ({time_entity_extraction/total_time*100:.1f}%)\")\n",
    "    print(f\"Result Building:   {time_result_building:.2f}s ({time_result_building/total_time*100:.1f}%)\")\n",
    "    print(f\"Cleanup:           {time_cleanup:.2f}s ({time_cleanup/total_time*100:.1f}%)\")\n",
    "    print(f\"Total:             {total_time:.2f}s\")\n",
    "    print(f\"Average per batch: {total_time/((total_addresses + batch_size - 1) // batch_size):.3f}s\")\n",
    "    print(f\"Items per second:  {total_addresses/total_time:.1f}\")\n",
    "    \n",
    "    summary = {\n",
    "        \"total_addresses\": total_addresses,\n",
    "        \"successful_parses\": successful_parses,\n",
    "        \"failed_parses\": total_addresses - successful_parses,\n",
    "        \"success_rate\": successful_parses / total_addresses if total_addresses > 0 else 0,\n",
    "        \"timing\": {\n",
    "            \"tokenization\": time_tokenization,\n",
    "            \"gpu_inference\": time_gpu_inference,\n",
    "            \"cpu_transfer\": time_cpu_transfer,\n",
    "            \"entity_extraction\": time_entity_extraction,\n",
    "            \"result_building\": time_result_building,\n",
    "            \"cleanup\": time_cleanup,\n",
    "            \"total\": total_time,\n",
    "            \"items_per_second\": total_addresses/total_time\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        \"summary\": summary,\n",
    "        \"results\": all_results if all_results is not None else f\"Streamed to {stream_results_path}\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 33348 addresses in batches of 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 261/261 [00:40<00:00,  6.48batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== STREAMING TIMING BREAKDOWN ===\n",
      "Tokenization:      2.55s (6.4%)\n",
      "GPU Inference:     8.12s (20.3%)\n",
      "CPU Transfer:      21.65s (54.2%)\n",
      "Entity Extraction: 6.82s (17.1%)\n",
      "Result Building:   0.06s (0.2%)\n",
      "Cleanup:           0.77s (1.9%)\n",
      "Total:             39.98s\n",
      "Average per batch: 0.153s\n",
      "Items per second:  834.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = parse_addresses_from_csv_streaming_with_profiling(\n",
    "    df=ground_truth_df,\n",
    "    model_path=str(model_path),\n",
    "    target_column=\"property_address\",\n",
    "    batch_size=128\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_addresses_from_csv_streaming_with_cpu_transfer_optimization(\n",
    "    df: pd.DataFrame,\n",
    "    model_path: str,\n",
    "    target_column: str = \"address\",\n",
    "    index_column: Optional[str] = None,\n",
    "    batch_size: int = 512,\n",
    "    stream_results_path: Optional[str] = None\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Streaming approach with optimized CPU transfer.\n",
    "    \"\"\"\n",
    "    import json\n",
    "    import time\n",
    "\n",
    "    parser = AddressParserInference(model_path)\n",
    "\n",
    "    # Validation\n",
    "    if target_column not in df.columns:\n",
    "        raise ValueError(f\"Column '{target_column}' not found. Available columns: {list(df.columns)}\")\n",
    "\n",
    "    if index_column is None and \"datapoint_id\" in df.columns:\n",
    "        index_column = \"datapoint_id\"\n",
    "\n",
    "    if index_column and index_column not in df.columns:\n",
    "        raise ValueError(f\"Index column '{index_column}' not found. Available columns: {list(df.columns)}\")\n",
    "\n",
    "    addresses = df[target_column].fillna(\"\").astype(str).values\n",
    "    indices = df[index_column].values if index_column else df.index.values\n",
    "    has_datapoint_id = \"datapoint_id\" in df.columns\n",
    "    datapoint_ids = df[\"datapoint_id\"].values if has_datapoint_id else None\n",
    "\n",
    "    total_addresses = len(addresses)\n",
    "    print(f\"Processing {total_addresses} addresses in batches of {batch_size}\")\n",
    "\n",
    "    all_results = [] if stream_results_path is None else None\n",
    "    result_file = open(stream_results_path, \"w\") if stream_results_path else None\n",
    "    successful_parses = 0\n",
    "\n",
    "    # Timing variables\n",
    "    time_tokenization = 0\n",
    "    time_gpu_inference = 0\n",
    "    time_cpu_transfer = 0\n",
    "    time_entity_extraction = 0\n",
    "    time_result_building = 0\n",
    "    time_cleanup = 0\n",
    "\n",
    "    def batch_arrays(*arrays):\n",
    "        for i in range(0, len(arrays[0]), batch_size):\n",
    "            yield tuple(arr[i:i+batch_size] for arr in arrays)\n",
    "\n",
    "    arrays_to_batch = [addresses, indices]\n",
    "    if has_datapoint_id:\n",
    "        arrays_to_batch.append(datapoint_ids)\n",
    "\n",
    "    for batch in tqdm(batch_arrays(*arrays_to_batch),\n",
    "                      total=(total_addresses + batch_size - 1) // batch_size,\n",
    "                      desc=\"Processing batches\", unit=\"batch\"):\n",
    "\n",
    "        if has_datapoint_id:\n",
    "            batch_addresses, batch_indices, batch_datapoint_ids = batch\n",
    "        else:\n",
    "            batch_addresses, batch_indices = batch\n",
    "            batch_datapoint_ids = None\n",
    "\n",
    "        # TIME: Tokenization\n",
    "        start_time = time.time()\n",
    "        inputs = parser.tokenizer(\n",
    "            list(batch_addresses),\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=128,\n",
    "            return_tensors=\"pt\",\n",
    "            return_offsets_mapping=True\n",
    "        )\n",
    "        model_inputs = {k: v.to(parser.device) for k, v in inputs.items() if k != 'offset_mapping'}\n",
    "        time_tokenization += time.time() - start_time\n",
    "\n",
    "        # TIME: GPU inference\n",
    "        start_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            outputs = parser.model(**model_inputs)\n",
    "            predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "            predicted_token_class_ids = predictions.argmax(dim=-1)\n",
    "        time_gpu_inference += time.time() - start_time\n",
    "\n",
    "        # Cache frequently accessed objects\n",
    "        id2label = parser.id2label\n",
    "        tokenizer = parser.tokenizer\n",
    "\n",
    "        # TIME: Entity extraction + CPU transfer (optimized)\n",
    "        start_time = time.time()\n",
    "        batch_results = []\n",
    "        for j, address in enumerate(batch_addresses):\n",
    "            index = batch_indices[j]\n",
    "            try:\n",
    "                # Transfer only the relevant slices to CPU, not the whole batch\n",
    "                input_ids = inputs[\"input_ids\"][j].cpu()\n",
    "                offset_mapping = inputs[\"offset_mapping\"][j]  # already on CPU\n",
    "                pred_token_class_ids = predicted_token_class_ids[j].cpu()\n",
    "                tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "                predicted_labels = [id2label[pred.item()] for pred in pred_token_class_ids]\n",
    "                entities = parser._extract_entities(address, tokens, predicted_labels, offset_mapping)\n",
    "                if len(entities) > 0:\n",
    "                    successful_parses += 1\n",
    "            except Exception as e:\n",
    "                entities = []\n",
    "                print(f\"Warning: Failed to parse address at index {index}: {str(e)}\")\n",
    "            batch_results.append((index, address, entities, batch_datapoint_ids[j] if has_datapoint_id else None))\n",
    "        time_cpu_transfer += time.time() - start_time  # now includes both transfer and extraction\n",
    "\n",
    "        # TIME: Result building\n",
    "        start_time = time.time()\n",
    "        for index, address, entities, datapoint_id in batch_results:\n",
    "            result = {\n",
    "                \"row_index\": index,\n",
    "                \"original_address\": address,\n",
    "                \"entities\": entities,\n",
    "                \"parsed_components\": parser._group_entities_by_type(entities)\n",
    "            }\n",
    "            if has_datapoint_id:\n",
    "                result[\"datapoint_id\"] = datapoint_id\n",
    "            if all_results is not None:\n",
    "                all_results.append(result)\n",
    "            if result_file:\n",
    "                result_file.write(json.dumps(result) + \"\\n\")\n",
    "        time_result_building += time.time() - start_time\n",
    "\n",
    "        # TIME: Cleanup\n",
    "        start_time = time.time()\n",
    "        del outputs, predictions, predicted_token_class_ids\n",
    "        torch.cuda.empty_cache()\n",
    "        time_cleanup += time.time() - start_time\n",
    "\n",
    "    if result_file:\n",
    "        result_file.close()\n",
    "\n",
    "    total_time = time_tokenization + time_gpu_inference + time_cpu_transfer + time_result_building + time_cleanup\n",
    "    print(f\"\\n=== STREAMING TIMING BREAKDOWN (CPU Transfer Optimized) ===\")\n",
    "    print(f\"Tokenization:      {time_tokenization:.2f}s ({time_tokenization/total_time*100:.1f}%)\")\n",
    "    print(f\"GPU Inference:     {time_gpu_inference:.2f}s ({time_gpu_inference/total_time*100:.1f}%)\")\n",
    "    print(f\"CPU Transfer+Entity Extraction: {time_cpu_transfer:.2f}s ({time_cpu_transfer/total_time*100:.1f}%)\")\n",
    "    print(f\"Result Building:   {time_result_building:.2f}s ({time_result_building/total_time*100:.1f}%)\")\n",
    "    print(f\"Cleanup:           {time_cleanup:.2f}s ({time_cleanup/total_time*100:.1f}%)\")\n",
    "    print(f\"Total:             {total_time:.2f}s\")\n",
    "    print(f\"Average per batch: {total_time/((total_addresses + batch_size - 1) // batch_size):.3f}s\")\n",
    "    print(f\"Items per second:  {total_addresses/total_time:.1f}\")\n",
    "\n",
    "    summary = {\n",
    "        \"total_addresses\": total_addresses,\n",
    "        \"successful_parses\": successful_parses,\n",
    "        \"failed_parses\": total_addresses - successful_parses,\n",
    "        \"success_rate\": successful_parses / total_addresses if total_addresses > 0 else 0,\n",
    "        \"timing\": {\n",
    "            \"tokenization\": time_tokenization,\n",
    "            \"gpu_inference\": time_gpu_inference,\n",
    "            \"cpu_transfer_and_extraction\": time_cpu_transfer,\n",
    "            \"result_building\": time_result_building,\n",
    "            \"cleanup\": time_cleanup,\n",
    "            \"total\": total_time,\n",
    "            \"items_per_second\": total_addresses/total_time\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        \"summary\": summary,\n",
    "        \"results\": all_results if all_results is not None else f\"Streamed to {stream_results_path}\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 33348 addresses in batches of 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 261/261 [00:41<00:00,  6.32batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== STREAMING TIMING BREAKDOWN (CPU Transfer Optimized) ===\n",
      "Tokenization:      2.56s (6.3%)\n",
      "GPU Inference:     8.08s (19.7%)\n",
      "CPU Transfer+Entity Extraction: 29.57s (72.1%)\n",
      "Result Building:   0.06s (0.1%)\n",
      "Cleanup:           0.73s (1.8%)\n",
      "Total:             40.99s\n",
      "Average per batch: 0.157s\n",
      "Items per second:  813.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = parse_addresses_from_csv_streaming_with_cpu_transfer_optimization(\n",
    "    df=ground_truth_df,\n",
    "    model_path=str(model_path),\n",
    "    target_column=\"property_address\",\n",
    "    batch_size=128\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting parallel tokenization...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4cd52ac796749158eb8d5576fa657d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/33348 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallel tokenization failed: One of the subprocesses has abruptly died during map operation.To debug the error, disable multiprocessing.\n",
      "Falling back to single-process tokenization...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "809851c57e524a30a30f93efffb4979d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/33348 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single-process tokenization complete!\n",
      "Processing 33348 addresses in batches of 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 261/261 [00:45<00:00,  5.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DATALOADER PERFORMANCE ANALYSIS ===\n",
      "Data Loading:      0.04s (0.1%)\n",
      "GPU Inference:     8.14s (24.6%)\n",
      "CPU Transfer:      20.09s (60.8%)\n",
      "Entity Extraction: 4.77s (14.4%)\n",
      "Total Time:        33.04s\n",
      "Processing Speed:  1009.4 addresses/second\n",
      "Success Rate:      0.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.multiprocessing as mp\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, Optional\n",
    "import pandas as pd\n",
    "\n",
    "def parse_addresses_with_dataloader(\n",
    "    df: pd.DataFrame,\n",
    "    model_path: str,\n",
    "    target_column: str = \"address\", \n",
    "    index_column: Optional[str] = None,\n",
    "    batch_size: int = 512,\n",
    "    num_workers: int = 4\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Using HuggingFace Dataset + DataLoader approach with optimized processing\n",
    "    \"\"\"\n",
    "    # Set multiprocessing start method to avoid CUDA issues\n",
    "    try:\n",
    "        mp.set_start_method('spawn', force=True)\n",
    "    except RuntimeError:\n",
    "        pass  # Already set\n",
    "    \n",
    "    parser = AddressParserInference(model_path)\n",
    "    \n",
    "    # Setup index handling\n",
    "    if index_column is None and \"datapoint_id\" in df.columns:\n",
    "        index_column = \"datapoint_id\"\n",
    "    \n",
    "    # Create HuggingFace Dataset\n",
    "    dataset_dict = {\n",
    "        \"address\": df[target_column].fillna(\"\").astype(str).tolist(),\n",
    "        \"row_index\": df[index_column].tolist() if index_column else df.index.tolist()\n",
    "    }\n",
    "    \n",
    "    if \"datapoint_id\" in df.columns:\n",
    "        dataset_dict[\"datapoint_id\"] = df[\"datapoint_id\"].tolist()\n",
    "    \n",
    "    dataset = Dataset.from_dict(dataset_dict)\n",
    "    \n",
    "    # Tokenization function that preserves original data\n",
    "    def tokenize_function(examples):\n",
    "        # Create a CPU-only tokenizer to avoid CUDA issues in multiprocessing\n",
    "        from transformers import AutoTokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        \n",
    "        tokenized = tokenizer(\n",
    "            examples[\"address\"],\n",
    "            padding=False,  # DataLoader will handle padding\n",
    "            truncation=True,\n",
    "            max_length=128,\n",
    "            return_offsets_mapping=True\n",
    "        )\n",
    "        \n",
    "        # Keep the original address and metadata\n",
    "        tokenized[\"original_address\"] = examples[\"address\"]\n",
    "        tokenized[\"row_index\"] = examples[\"row_index\"]\n",
    "        \n",
    "        if \"datapoint_id\" in examples:\n",
    "            tokenized[\"datapoint_id\"] = examples[\"datapoint_id\"]\n",
    "        \n",
    "        return tokenized\n",
    "    \n",
    "    # Apply tokenization with fallback for multiprocessing issues\n",
    "    try:\n",
    "        print(\"Attempting parallel tokenization...\")\n",
    "        tokenized_dataset = dataset.map(\n",
    "            tokenize_function,\n",
    "            batched=True,\n",
    "            batch_size=1000,\n",
    "            num_proc=2,  # Reduced to avoid issues\n",
    "            remove_columns=[]  # Keep all columns\n",
    "        )\n",
    "        print(\"Parallel tokenization successful!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Parallel tokenization failed: {e}\")\n",
    "        print(\"Falling back to single-process tokenization...\")\n",
    "        tokenized_dataset = dataset.map(\n",
    "            tokenize_function,\n",
    "            batched=True,\n",
    "            batch_size=1000,\n",
    "            num_proc=None,  # Single process\n",
    "            remove_columns=[]  # Keep all columns\n",
    "        )\n",
    "        print(\"Single-process tokenization complete!\")\n",
    "    \n",
    "    # Custom collate function for efficient batching\n",
    "    def collate_fn(batch):\n",
    "        # Extract data from batch items\n",
    "        input_ids = [item[\"input_ids\"] for item in batch]\n",
    "        attention_mask = [item[\"attention_mask\"] for item in batch]\n",
    "        offset_mapping = [item[\"offset_mapping\"] for item in batch]\n",
    "        row_indices = [item[\"row_index\"] for item in batch]\n",
    "        addresses = [item[\"original_address\"] for item in batch]  # Direct access from batch\n",
    "        \n",
    "        # Efficient padding\n",
    "        max_len = max(len(ids) for ids in input_ids)\n",
    "        padded_input_ids = []\n",
    "        padded_attention_mask = []\n",
    "        \n",
    "        for ids, mask in zip(input_ids, attention_mask):\n",
    "            pad_len = max_len - len(ids)\n",
    "            padded_input_ids.append(ids + [parser.tokenizer.pad_token_id] * pad_len)\n",
    "            padded_attention_mask.append(mask + [0] * pad_len)\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(padded_input_ids),\n",
    "            \"attention_mask\": torch.tensor(padded_attention_mask),\n",
    "            \"offset_mapping\": offset_mapping,\n",
    "            \"addresses\": addresses,\n",
    "            \"row_indices\": row_indices,\n",
    "            \"datapoint_ids\": [item.get(\"datapoint_id\") for item in batch] if \"datapoint_id\" in batch[0] else None\n",
    "        }\n",
    "    \n",
    "    # Create DataLoader with conservative settings to avoid multiprocessing issues\n",
    "    dataloader = DataLoader(\n",
    "        tokenized_dataset,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=0,  # Disable multiprocessing for data loading\n",
    "        pin_memory=True,  # Faster GPU transfers\n",
    "        shuffle=False  # Maintain order\n",
    "    )\n",
    "    \n",
    "    print(f\"Processing {len(dataset)} addresses in batches of {batch_size}\")\n",
    "    \n",
    "    # Initialize results and timing\n",
    "    all_results = []\n",
    "    successful_parses = 0\n",
    "    \n",
    "    time_data_loading = 0\n",
    "    time_gpu_inference = 0\n",
    "    time_cpu_transfer = 0\n",
    "    time_entity_extraction = 0\n",
    "    \n",
    "    # Process batches\n",
    "    for batch in tqdm(dataloader, desc=\"Processing batches\"):\n",
    "        # Data loading timing\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Move tensors to GPU\n",
    "        input_ids = batch[\"input_ids\"].to(parser.device, non_blocking=True)\n",
    "        attention_mask = batch[\"attention_mask\"].to(parser.device, non_blocking=True)\n",
    "        \n",
    "        time_data_loading += time.time() - start_time\n",
    "        \n",
    "        # GPU Inference\n",
    "        start_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            outputs = parser.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "            predicted_token_class_ids = predictions.argmax(dim=-1)\n",
    "        time_gpu_inference += time.time() - start_time\n",
    "        \n",
    "        # CPU Transfer (Fixed: removed non_blocking argument from cpu())\n",
    "        start_time = time.time()\n",
    "        predicted_token_class_ids_cpu = predicted_token_class_ids.cpu()\n",
    "        input_ids_cpu = input_ids.cpu()\n",
    "        torch.cuda.synchronize()  # Ensure GPU operations are complete\n",
    "        time_cpu_transfer += time.time() - start_time\n",
    "        \n",
    "        # Entity Extraction\n",
    "        start_time = time.time()\n",
    "        for j in range(len(batch[\"addresses\"])):\n",
    "            address = batch[\"addresses\"][j]\n",
    "            tokens = parser.tokenizer.convert_ids_to_tokens(input_ids_cpu[j])\n",
    "            predicted_labels = [parser.id2label[pred.item()] for pred in predicted_token_class_ids_cpu[j]]\n",
    "            offset_mapping = batch[\"offset_mapping\"][j]\n",
    "            \n",
    "            try:\n",
    "                entities = parser._extract_entities(address, tokens, predicted_labels, offset_mapping)\n",
    "                if len(entities) > 0:\n",
    "                    successful_parses += 1\n",
    "            except Exception as e:\n",
    "                entities = []\n",
    "            \n",
    "            result = {\n",
    "                \"row_index\": batch[\"row_indices\"][j],\n",
    "                \"original_address\": address,\n",
    "                \"entities\": entities,\n",
    "                \"parsed_components\": parser._group_entities_by_type(entities)\n",
    "            }\n",
    "            \n",
    "            if batch[\"datapoint_ids\"] and batch[\"datapoint_ids\"][j]:\n",
    "                result[\"datapoint_id\"] = batch[\"datapoint_ids\"][j]\n",
    "            \n",
    "            all_results.append(result)\n",
    "        \n",
    "        time_entity_extraction += time.time() - start_time\n",
    "        \n",
    "        # Memory cleanup\n",
    "        del outputs, predictions, predicted_token_class_ids\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Print detailed timing information\n",
    "    total_time = time_data_loading + time_gpu_inference + time_cpu_transfer + time_entity_extraction\n",
    "    print(f\"\\n=== DATALOADER PERFORMANCE ANALYSIS ===\")\n",
    "    print(f\"Data Loading:      {time_data_loading:.2f}s ({time_data_loading/total_time*100:.1f}%)\")\n",
    "    print(f\"GPU Inference:     {time_gpu_inference:.2f}s ({time_gpu_inference/total_time*100:.1f}%)\")\n",
    "    print(f\"CPU Transfer:      {time_cpu_transfer:.2f}s ({time_cpu_transfer/total_time*100:.1f}%)\")\n",
    "    print(f\"Entity Extraction: {time_entity_extraction:.2f}s ({time_entity_extraction/total_time*100:.1f}%)\")\n",
    "    print(f\"Total Time:        {total_time:.2f}s\")\n",
    "    print(f\"Processing Speed:  {len(dataset)/total_time:.1f} addresses/second\")\n",
    "    print(f\"Success Rate:      {successful_parses/len(dataset)*100:.1f}%\")\n",
    "    \n",
    "    return {\n",
    "        \"summary\": {\n",
    "            \"total_addresses\": len(dataset),\n",
    "            \"successful_parses\": successful_parses,\n",
    "            \"failed_parses\": len(dataset) - successful_parses,\n",
    "            \"success_rate\": successful_parses / len(dataset) if len(dataset) > 0 else 0,\n",
    "            \"processing_time\": total_time,\n",
    "            \"addresses_per_second\": len(dataset) / total_time if total_time > 0 else 0\n",
    "        },\n",
    "        \"results\": all_results\n",
    "    }\n",
    "\n",
    "# Usage\n",
    "results = parse_addresses_with_dataloader(\n",
    "    df=ground_truth_df,\n",
    "    model_path=str(model_path),\n",
    "    target_column=\"property_address\",\n",
    "    batch_size=128\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying tokenization...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07e83c4921034ad6ba72c184f7966c34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/33348 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization complete!\n",
      "Processing 33348 addresses in batches of 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|          | 0/261 [00:00<?, ?it/s]Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_2027/3151257145.py\", line 160, in parse_addresses_with_dataloader_debug\n",
      "    entities = parser._extract_entities(address, tokens, predicted_labels, offset_mapping)\n",
      "  File \"/teamspace/studios/this_studio/enhance_ocod/src/enhance_ocod/inference_utils.py\", line 129, in _extract_entities\n",
      "    start_pos, end_pos = offset.tolist()\n",
      "AttributeError: 'list' object has no attribute 'tolist'\n",
      "Processing batches:   0%|          | 0/261 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DEBUGGING FIRST BATCH ===\n",
      "Address: 161, 163, 165, 167 and 169 uxbridge road, ealing\n",
      "Tokens: ['[CLS]', '161', ',', 'Ġ163', ',', 'Ġ165', ',', 'Ġ167', 'Ġand', 'Ġ169', 'Ġu', 'x', 'bridge', 'Ġroad', ',', 'Ġe', 'aling', '[SEP]', '[PAD]', '[PAD]']...\n",
      "Labels: ['O', 'B-street_number', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-street_name', 'I-street_name', 'I-street_name', 'O', 'O', 'O', 'O', 'O', 'O']...\n",
      "Unique labels in prediction: {'B-street_number', 'I-street_name', 'O'}\n",
      "Label distribution: {'B-street_number': 1, 'I-street_name': 3, 'O': 22}\n",
      "Offset mapping length: 18\n",
      "Model id2label keys: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]...\n",
      "Model id2label values: ['O', 'B-building_name', 'I-building_name', 'B-street_name', 'I-street_name', 'B-street_number', 'I-street_number', 'B-filter_type', 'I-filter_type', 'B-unit_id']...\n",
      "\n",
      "Attempting entity extraction...\n",
      "Entity extraction failed with error: 'list' object has no attribute 'tolist'\n",
      "=== END DEBUG ===\n",
      "\n",
      "Entity extraction error for item 0: 'list' object has no attribute 'tolist'\n",
      "Entity extraction error for item 1: 'list' object has no attribute 'tolist'\n",
      "Entity extraction error for item 2: 'list' object has no attribute 'tolist'\n",
      "Debug mode: Stopping after first batch. Found 0 successful parses out of 128 addresses.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.multiprocessing as mp\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, Optional\n",
    "import pandas as pd\n",
    "\n",
    "def parse_addresses_with_dataloader_debug(\n",
    "    df: pd.DataFrame,\n",
    "    model_path: str,\n",
    "    target_column: str = \"address\", \n",
    "    index_column: Optional[str] = None,\n",
    "    batch_size: int = 512,\n",
    "    num_workers: int = 4,\n",
    "    debug_first_batch: bool = True\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Debug version to identify entity extraction issues\n",
    "    \"\"\"\n",
    "    # Set multiprocessing start method to avoid CUDA issues\n",
    "    try:\n",
    "        mp.set_start_method('spawn', force=True)\n",
    "    except RuntimeError:\n",
    "        pass  # Already set\n",
    "    \n",
    "    parser = AddressParserInference(model_path)\n",
    "    \n",
    "    # Setup index handling\n",
    "    if index_column is None and \"datapoint_id\" in df.columns:\n",
    "        index_column = \"datapoint_id\"\n",
    "    \n",
    "    # Create HuggingFace Dataset\n",
    "    dataset_dict = {\n",
    "        \"address\": df[target_column].fillna(\"\").astype(str).tolist(),\n",
    "        \"row_index\": df[index_column].tolist() if index_column else df.index.tolist()\n",
    "    }\n",
    "    \n",
    "    if \"datapoint_id\" in df.columns:\n",
    "        dataset_dict[\"datapoint_id\"] = df[\"datapoint_id\"].tolist()\n",
    "    \n",
    "    dataset = Dataset.from_dict(dataset_dict)\n",
    "    \n",
    "    # Tokenization function that preserves original data\n",
    "    def tokenize_function(examples):\n",
    "        # IMPORTANT: Use the same tokenizer as the parser!\n",
    "        tokenized = parser.tokenizer(  # Changed from creating new tokenizer\n",
    "            examples[\"address\"],\n",
    "            padding=False,\n",
    "            truncation=True,\n",
    "            max_length=128,\n",
    "            return_offsets_mapping=True\n",
    "        )\n",
    "        \n",
    "        # Keep the original address and metadata\n",
    "        tokenized[\"original_address\"] = examples[\"address\"]\n",
    "        tokenized[\"row_index\"] = examples[\"row_index\"]\n",
    "        \n",
    "        if \"datapoint_id\" in examples:\n",
    "            tokenized[\"datapoint_id\"] = examples[\"datapoint_id\"]\n",
    "        \n",
    "        return tokenized\n",
    "    \n",
    "    # Apply tokenization (disable parallel for debugging)\n",
    "    print(\"Applying tokenization...\")\n",
    "    tokenized_dataset = dataset.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        batch_size=1000,\n",
    "        num_proc=None,  # Single process for debugging\n",
    "        remove_columns=[]\n",
    "    )\n",
    "    print(\"Tokenization complete!\")\n",
    "    \n",
    "    # Custom collate function\n",
    "    def collate_fn(batch):\n",
    "        input_ids = [item[\"input_ids\"] for item in batch]\n",
    "        attention_mask = [item[\"attention_mask\"] for item in batch]\n",
    "        offset_mapping = [item[\"offset_mapping\"] for item in batch]\n",
    "        row_indices = [item[\"row_index\"] for item in batch]\n",
    "        addresses = [item[\"original_address\"] for item in batch]\n",
    "        \n",
    "        # Efficient padding\n",
    "        max_len = max(len(ids) for ids in input_ids)\n",
    "        padded_input_ids = []\n",
    "        padded_attention_mask = []\n",
    "        \n",
    "        for ids, mask in zip(input_ids, attention_mask):\n",
    "            pad_len = max_len - len(ids)\n",
    "            padded_input_ids.append(ids + [parser.tokenizer.pad_token_id] * pad_len)\n",
    "            padded_attention_mask.append(mask + [0] * pad_len)\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(padded_input_ids),\n",
    "            \"attention_mask\": torch.tensor(padded_attention_mask),\n",
    "            \"offset_mapping\": offset_mapping,\n",
    "            \"addresses\": addresses,\n",
    "            \"row_indices\": row_indices,\n",
    "            \"datapoint_ids\": [item.get(\"datapoint_id\") for item in batch] if \"datapoint_id\" in batch[0] else None\n",
    "        }\n",
    "    \n",
    "    # Create DataLoader\n",
    "    dataloader = DataLoader(\n",
    "        tokenized_dataset,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    print(f\"Processing {len(dataset)} addresses in batches of {batch_size}\")\n",
    "    \n",
    "    all_results = []\n",
    "    successful_parses = 0\n",
    "    batch_count = 0\n",
    "    \n",
    "    for batch in tqdm(dataloader, desc=\"Processing batches\"):\n",
    "        batch_count += 1\n",
    "        \n",
    "        # Move tensors to GPU\n",
    "        input_ids = batch[\"input_ids\"].to(parser.device, non_blocking=True)\n",
    "        attention_mask = batch[\"attention_mask\"].to(parser.device, non_blocking=True)\n",
    "        \n",
    "        # GPU Inference\n",
    "        with torch.no_grad():\n",
    "            outputs = parser.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "            predicted_token_class_ids = predictions.argmax(dim=-1)\n",
    "        \n",
    "        # CPU Transfer\n",
    "        predicted_token_class_ids_cpu = predicted_token_class_ids.cpu()\n",
    "        input_ids_cpu = input_ids.cpu()\n",
    "        torch.cuda.synchronize()\n",
    "        \n",
    "        # Debug first batch in detail\n",
    "        if debug_first_batch and batch_count == 1:\n",
    "            print(\"\\n=== DEBUGGING FIRST BATCH ===\")\n",
    "            \n",
    "            # Check first address in detail\n",
    "            first_idx = 0\n",
    "            address = batch[\"addresses\"][first_idx]\n",
    "            tokens = parser.tokenizer.convert_ids_to_tokens(input_ids_cpu[first_idx])\n",
    "            predicted_labels = [parser.id2label[pred.item()] for pred in predicted_token_class_ids_cpu[first_idx]]\n",
    "            offset_mapping = batch[\"offset_mapping\"][first_idx]\n",
    "            \n",
    "            print(f\"Address: {address}\")\n",
    "            print(f\"Tokens: {tokens[:20]}...\")  # First 20 tokens\n",
    "            print(f\"Labels: {predicted_labels[:20]}...\")  # First 20 labels\n",
    "            print(f\"Unique labels in prediction: {set(predicted_labels)}\")\n",
    "            print(f\"Label distribution: {dict(zip(*np.unique(predicted_labels, return_counts=True)))}\")\n",
    "            print(f\"Offset mapping length: {len(offset_mapping)}\")\n",
    "            print(f\"Model id2label keys: {list(parser.id2label.keys())[:10]}...\")\n",
    "            print(f\"Model id2label values: {list(parser.id2label.values())[:10]}...\")\n",
    "            \n",
    "            # Try entity extraction with debug\n",
    "            try:\n",
    "                print(\"\\nAttempting entity extraction...\")\n",
    "                entities = parser._extract_entities(address, tokens, predicted_labels, offset_mapping)\n",
    "                print(f\"Extracted entities: {entities}\")\n",
    "                if len(entities) == 0:\n",
    "                    print(\"WARNING: No entities extracted!\")\n",
    "                    \n",
    "                    # Check if all predictions are \"O\" (outside)\n",
    "                    non_o_labels = [label for label in predicted_labels if label != \"O\"]\n",
    "                    print(f\"Non-O labels: {non_o_labels[:10]}...\")\n",
    "                    print(f\"Total non-O labels: {len(non_o_labels)}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Entity extraction failed with error: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "            \n",
    "            print(\"=== END DEBUG ===\\n\")\n",
    "        \n",
    "        # Entity Extraction for all items in batch\n",
    "        for j in range(len(batch[\"addresses\"])):\n",
    "            address = batch[\"addresses\"][j]\n",
    "            tokens = parser.tokenizer.convert_ids_to_tokens(input_ids_cpu[j])\n",
    "            predicted_labels = [parser.id2label[pred.item()] for pred in predicted_token_class_ids_cpu[j]]\n",
    "            offset_mapping = batch[\"offset_mapping\"][j]\n",
    "            \n",
    "            try:\n",
    "                entities = parser._extract_entities(address, tokens, predicted_labels, offset_mapping)\n",
    "                if len(entities) > 0:\n",
    "                    successful_parses += 1\n",
    "            except Exception as e:\n",
    "                entities = []\n",
    "                if batch_count == 1 and j < 3:  # Debug first few items\n",
    "                    print(f\"Entity extraction error for item {j}: {e}\")\n",
    "            \n",
    "            result = {\n",
    "                \"row_index\": batch[\"row_indices\"][j],\n",
    "                \"original_address\": address,\n",
    "                \"entities\": entities,\n",
    "                \"parsed_components\": parser._group_entities_by_type(entities)\n",
    "            }\n",
    "            \n",
    "            if batch[\"datapoint_ids\"] and batch[\"datapoint_ids\"][j]:\n",
    "                result[\"datapoint_id\"] = batch[\"datapoint_ids\"][j]\n",
    "            \n",
    "            all_results.append(result)\n",
    "        \n",
    "        # Memory cleanup\n",
    "        del outputs, predictions, predicted_token_class_ids\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Stop after first batch for debugging\n",
    "        if debug_first_batch and batch_count == 1:\n",
    "            print(f\"Debug mode: Stopping after first batch. Found {successful_parses} successful parses out of {len(batch['addresses'])} addresses.\")\n",
    "            break\n",
    "    \n",
    "    return {\n",
    "        \"summary\": {\n",
    "            \"total_addresses\": len(all_results),\n",
    "            \"successful_parses\": successful_parses,\n",
    "            \"failed_parses\": len(all_results) - successful_parses,\n",
    "            \"success_rate\": successful_parses / len(all_results) if len(all_results) > 0 else 0\n",
    "        },\n",
    "        \"results\": all_results\n",
    "    }\n",
    "\n",
    "# Run debug version\n",
    "debug_results = parse_addresses_with_dataloader_debug(\n",
    "    df=ground_truth_df,\n",
    "    model_path=str(model_path),\n",
    "    target_column=\"property_address\",\n",
    "    batch_size=128,\n",
    "    debug_first_batch=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting parallel tokenization...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09c5aa1d5b9f4262a7d19a90137861ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/33348 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-5:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/multiprocess/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/multiprocess/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/multiprocess/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/multiprocess/queues.py\", line 370, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/dill/_dill.py\", line 303, in loads\n",
      "    return load(file, ignore, **kwds)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/dill/_dill.py\", line 289, in load\n",
      "    return Unpickler(file, ignore=ignore, **kwds).load()\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/dill/_dill.py\", line 444, in load\n",
      "    obj = StockUnpickler.load(self)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/storage.py\", line 530, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b), weights_only=False)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/serialization.py\", line 1549, in load\n",
      "    return _legacy_load(\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/serialization.py\", line 1807, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/serialization.py\", line 1742, in persistent_load\n",
      "    obj = restore_location(obj, location)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/serialization.py\", line 698, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/serialization.py\", line 637, in _deserialize\n",
      "    return obj.to(device=device)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/storage.py\", line 287, in to\n",
      "    return _to(self, device, non_blocking)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_utils.py\", line 87, in _to\n",
      "    with device_module.device(device):\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/cuda/__init__.py\", line 495, in __enter__\n",
      "    self.prev_idx = torch.cuda._exchange_device(self.idx)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/cuda/__init__.py\", line 358, in _lazy_init\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallel tokenization failed: One of the subprocesses has abruptly died during map operation.To debug the error, disable multiprocessing.\n",
      "Falling back to single-process tokenization...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0e8c243b8af4d2ab7b4755e868af9a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/33348 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single-process tokenization complete!\n",
      "Processing 33348 addresses in batches of 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 261/261 [00:48<00:00,  5.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DATALOADER PERFORMANCE ANALYSIS ===\n",
      "Data Loading:      0.04s (0.1%)\n",
      "GPU Inference:     11.50s (31.5%)\n",
      "CPU Transfer:      19.77s (54.1%)\n",
      "Entity Extraction: 5.23s (14.3%)\n",
      "Total Time:        36.54s\n",
      "Processing Speed:  912.6 addresses/second\n",
      "Success Rate:      98.8%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def parse_addresses_with_dataloader_fixed(\n",
    "    df: pd.DataFrame,\n",
    "    model_path: str,\n",
    "    target_column: str = \"address\", \n",
    "    index_column: Optional[str] = None,\n",
    "    batch_size: int = 512,\n",
    "    num_workers: int = 4\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Fixed version with proper offset mapping handling\n",
    "    \"\"\"\n",
    "    # Set multiprocessing start method to avoid CUDA issues\n",
    "    try:\n",
    "        mp.set_start_method('spawn', force=True)\n",
    "    except RuntimeError:\n",
    "        pass  # Already set\n",
    "    \n",
    "    parser = AddressParserInference(model_path)\n",
    "    \n",
    "    # Setup index handling\n",
    "    if index_column is None and \"datapoint_id\" in df.columns:\n",
    "        index_column = \"datapoint_id\"\n",
    "    \n",
    "    # Create HuggingFace Dataset\n",
    "    dataset_dict = {\n",
    "        \"address\": df[target_column].fillna(\"\").astype(str).tolist(),\n",
    "        \"row_index\": df[index_column].tolist() if index_column else df.index.tolist()\n",
    "    }\n",
    "    \n",
    "    if \"datapoint_id\" in df.columns:\n",
    "        dataset_dict[\"datapoint_id\"] = df[\"datapoint_id\"].tolist()\n",
    "    \n",
    "    dataset = Dataset.from_dict(dataset_dict)\n",
    "    \n",
    "    # Tokenization function\n",
    "    def tokenize_function(examples):\n",
    "        tokenized = parser.tokenizer(\n",
    "            examples[\"address\"],\n",
    "            padding=False,\n",
    "            truncation=True,\n",
    "            max_length=128,\n",
    "            return_offsets_mapping=True\n",
    "        )\n",
    "        \n",
    "        # Keep the original address and metadata\n",
    "        tokenized[\"original_address\"] = examples[\"address\"]\n",
    "        tokenized[\"row_index\"] = examples[\"row_index\"]\n",
    "        \n",
    "        if \"datapoint_id\" in examples:\n",
    "            tokenized[\"datapoint_id\"] = examples[\"datapoint_id\"]\n",
    "        \n",
    "        return tokenized\n",
    "    \n",
    "    # Apply tokenization\n",
    "    try:\n",
    "        print(\"Attempting parallel tokenization...\")\n",
    "        tokenized_dataset = dataset.map(\n",
    "            tokenize_function,\n",
    "            batched=True,\n",
    "            batch_size=1000,\n",
    "            num_proc=2,\n",
    "            remove_columns=[]\n",
    "        )\n",
    "        print(\"Parallel tokenization successful!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Parallel tokenization failed: {e}\")\n",
    "        print(\"Falling back to single-process tokenization...\")\n",
    "        tokenized_dataset = dataset.map(\n",
    "            tokenize_function,\n",
    "            batched=True,\n",
    "            batch_size=1000,\n",
    "            num_proc=None,\n",
    "            remove_columns=[]\n",
    "        )\n",
    "        print(\"Single-process tokenization complete!\")\n",
    "    \n",
    "    # Fixed entity extraction function\n",
    "    def extract_entities_fixed(address, tokens, predicted_labels, offset_mapping):\n",
    "        \"\"\"\n",
    "        Fixed version that handles both tensor and list offset mappings\n",
    "        \"\"\"\n",
    "        entities = []\n",
    "        current_entity = None\n",
    "        \n",
    "        for i, (token, label, offset) in enumerate(zip(tokens, predicted_labels, offset_mapping)):\n",
    "            # Skip special tokens\n",
    "            if token in ['[CLS]', '[SEP]', '[PAD]']:\n",
    "                continue\n",
    "                \n",
    "            # Handle offset mapping (could be tensor or list)\n",
    "            if hasattr(offset, 'tolist'):\n",
    "                start_pos, end_pos = offset.tolist()  # It's a tensor\n",
    "            else:\n",
    "                start_pos, end_pos = offset  # It's already a list\n",
    "            \n",
    "            # Skip tokens with no mapping (like padding)\n",
    "            if start_pos == end_pos:\n",
    "                continue\n",
    "                \n",
    "            if label.startswith('B-'):\n",
    "                # Beginning of new entity\n",
    "                if current_entity:\n",
    "                    entities.append(current_entity)\n",
    "                \n",
    "                entity_type = label[2:]  # Remove 'B-' prefix\n",
    "                current_entity = {\n",
    "                    'type': entity_type,\n",
    "                    'start': start_pos,\n",
    "                    'end': end_pos,\n",
    "                    'text': address[start_pos:end_pos]\n",
    "                }\n",
    "                \n",
    "            elif label.startswith('I-') and current_entity:\n",
    "                # Inside entity (continuation)\n",
    "                entity_type = label[2:]  # Remove 'I-' prefix\n",
    "                if entity_type == current_entity['type']:\n",
    "                    # Extend current entity\n",
    "                    current_entity['end'] = end_pos\n",
    "                    current_entity['text'] = address[current_entity['start']:end_pos]\n",
    "                else:\n",
    "                    # Different entity type, start new one\n",
    "                    entities.append(current_entity)\n",
    "                    current_entity = {\n",
    "                        'type': entity_type,\n",
    "                        'start': start_pos,\n",
    "                        'end': end_pos,\n",
    "                        'text': address[start_pos:end_pos]\n",
    "                    }\n",
    "            else:\n",
    "                # 'O' label or mismatched I- label\n",
    "                if current_entity:\n",
    "                    entities.append(current_entity)\n",
    "                    current_entity = None\n",
    "        \n",
    "        # Don't forget the last entity\n",
    "        if current_entity:\n",
    "            entities.append(current_entity)\n",
    "            \n",
    "        return entities\n",
    "    \n",
    "    # Custom collate function\n",
    "    def collate_fn(batch):\n",
    "        input_ids = [item[\"input_ids\"] for item in batch]\n",
    "        attention_mask = [item[\"attention_mask\"] for item in batch]\n",
    "        offset_mapping = [item[\"offset_mapping\"] for item in batch]\n",
    "        row_indices = [item[\"row_index\"] for item in batch]\n",
    "        addresses = [item[\"original_address\"] for item in batch]\n",
    "        \n",
    "        # Efficient padding\n",
    "        max_len = max(len(ids) for ids in input_ids)\n",
    "        padded_input_ids = []\n",
    "        padded_attention_mask = []\n",
    "        \n",
    "        for ids, mask in zip(input_ids, attention_mask):\n",
    "            pad_len = max_len - len(ids)\n",
    "            padded_input_ids.append(ids + [parser.tokenizer.pad_token_id] * pad_len)\n",
    "            padded_attention_mask.append(mask + [0] * pad_len)\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(padded_input_ids),\n",
    "            \"attention_mask\": torch.tensor(padded_attention_mask),\n",
    "            \"offset_mapping\": offset_mapping,\n",
    "            \"addresses\": addresses,\n",
    "            \"row_indices\": row_indices,\n",
    "            \"datapoint_ids\": [item.get(\"datapoint_id\") for item in batch] if \"datapoint_id\" in batch[0] else None\n",
    "        }\n",
    "    \n",
    "    # Create DataLoader\n",
    "    dataloader = DataLoader(\n",
    "        tokenized_dataset,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    print(f\"Processing {len(dataset)} addresses in batches of {batch_size}\")\n",
    "    \n",
    "    # Initialize results and timing\n",
    "    all_results = []\n",
    "    successful_parses = 0\n",
    "    \n",
    "    time_data_loading = 0\n",
    "    time_gpu_inference = 0\n",
    "    time_cpu_transfer = 0\n",
    "    time_entity_extraction = 0\n",
    "    \n",
    "    # Process batches\n",
    "    for batch in tqdm(dataloader, desc=\"Processing batches\"):\n",
    "        # Data loading timing\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Move tensors to GPU\n",
    "        input_ids = batch[\"input_ids\"].to(parser.device, non_blocking=True)\n",
    "        attention_mask = batch[\"attention_mask\"].to(parser.device, non_blocking=True)\n",
    "        \n",
    "        time_data_loading += time.time() - start_time\n",
    "        \n",
    "        # GPU Inference\n",
    "        start_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            outputs = parser.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "            predicted_token_class_ids = predictions.argmax(dim=-1)\n",
    "        time_gpu_inference += time.time() - start_time\n",
    "        \n",
    "        # CPU Transfer\n",
    "        start_time = time.time()\n",
    "        predicted_token_class_ids_cpu = predicted_token_class_ids.cpu()\n",
    "        input_ids_cpu = input_ids.cpu()\n",
    "        torch.cuda.synchronize()\n",
    "        time_cpu_transfer += time.time() - start_time\n",
    "        \n",
    "        # Entity Extraction\n",
    "        start_time = time.time()\n",
    "        for j in range(len(batch[\"addresses\"])):\n",
    "            address = batch[\"addresses\"][j]\n",
    "            tokens = parser.tokenizer.convert_ids_to_tokens(input_ids_cpu[j])\n",
    "            predicted_labels = [parser.id2label[pred.item()] for pred in predicted_token_class_ids_cpu[j]]\n",
    "            offset_mapping = batch[\"offset_mapping\"][j]\n",
    "            \n",
    "            try:\n",
    "                # Use our fixed entity extraction\n",
    "                entities = extract_entities_fixed(address, tokens, predicted_labels, offset_mapping)\n",
    "                if len(entities) > 0:\n",
    "                    successful_parses += 1\n",
    "            except Exception as e:\n",
    "                entities = []\n",
    "                print(f\"Entity extraction error: {e}\")\n",
    "            \n",
    "            result = {\n",
    "                \"row_index\": batch[\"row_indices\"][j],\n",
    "                \"original_address\": address,\n",
    "                \"entities\": entities,\n",
    "                \"parsed_components\": parser._group_entities_by_type(entities) if hasattr(parser, '_group_entities_by_type') else {}\n",
    "            }\n",
    "            \n",
    "            if batch[\"datapoint_ids\"] and batch[\"datapoint_ids\"][j]:\n",
    "                result[\"datapoint_id\"] = batch[\"datapoint_ids\"][j]\n",
    "            \n",
    "            all_results.append(result)\n",
    "        \n",
    "        time_entity_extraction += time.time() - start_time\n",
    "        \n",
    "        # Memory cleanup\n",
    "        del outputs, predictions, predicted_token_class_ids\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Print detailed timing information\n",
    "    total_time = time_data_loading + time_gpu_inference + time_cpu_transfer + time_entity_extraction\n",
    "    print(f\"\\n=== DATALOADER PERFORMANCE ANALYSIS ===\")\n",
    "    print(f\"Data Loading:      {time_data_loading:.2f}s ({time_data_loading/total_time*100:.1f}%)\")\n",
    "    print(f\"GPU Inference:     {time_gpu_inference:.2f}s ({time_gpu_inference/total_time*100:.1f}%)\")\n",
    "    print(f\"CPU Transfer:      {time_cpu_transfer:.2f}s ({time_cpu_transfer/total_time*100:.1f}%)\")\n",
    "    print(f\"Entity Extraction: {time_entity_extraction:.2f}s ({time_entity_extraction/total_time*100:.1f}%)\")\n",
    "    print(f\"Total Time:        {total_time:.2f}s\")\n",
    "    print(f\"Processing Speed:  {len(dataset)/total_time:.1f} addresses/second\")\n",
    "    print(f\"Success Rate:      {successful_parses/len(dataset)*100:.1f}%\")\n",
    "    \n",
    "    return {\n",
    "        \"summary\": {\n",
    "            \"total_addresses\": len(dataset),\n",
    "            \"successful_parses\": successful_parses,\n",
    "            \"failed_parses\": len(dataset) - successful_parses,\n",
    "            \"success_rate\": successful_parses / len(dataset) if len(dataset) > 0 else 0,\n",
    "            \"processing_time\": total_time,\n",
    "            \"addresses_per_second\": len(dataset) / total_time if total_time > 0 else 0\n",
    "        },\n",
    "        \"results\": all_results\n",
    "    }\n",
    "\n",
    "# Test the fixed version\n",
    "results = parse_addresses_with_dataloader_fixed(\n",
    "    df=ground_truth_df,\n",
    "    model_path=str(model_path),\n",
    "    target_column=\"property_address\",\n",
    "    batch_size=128\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting parallel tokenization...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d77bad091c614d559fb71634b9a2bc63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing (num_proc=4):   0%|          | 0/33348 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallel tokenization successful!\n",
      "Processing 33348 addresses in batches of 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 261/261 [00:45<00:00,  5.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== FINAL PERFORMANCE ANALYSIS ===\n",
      "Data Loading:      0.04s (0.1%)\n",
      "GPU Inference:     8.14s (24.5%)\n",
      "CPU Transfer:      20.03s (60.2%)\n",
      "Entity Extraction: 5.08s (15.3%)\n",
      "Total Time:        33.29s\n",
      "Processing Speed:  1001.7 addresses/second\n",
      "Success Rate:      98.8%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def parse_addresses_with_dataloader_final(\n",
    "    df: pd.DataFrame,\n",
    "    model_path: str,\n",
    "    target_column: str = \"address\", \n",
    "    index_column: Optional[str] = None,\n",
    "    batch_size: int = 512,\n",
    "    num_workers: int = 4\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Final optimized version with working multiprocessing\n",
    "    \"\"\"\n",
    "    # Set multiprocessing start method to avoid CUDA issues\n",
    "    try:\n",
    "        mp.set_start_method('spawn', force=True)\n",
    "    except RuntimeError:\n",
    "        pass  # Already set\n",
    "    \n",
    "    parser = AddressParserInference(model_path)\n",
    "    \n",
    "    # Setup index handling\n",
    "    if index_column is None and \"datapoint_id\" in df.columns:\n",
    "        index_column = \"datapoint_id\"\n",
    "    \n",
    "    # Create HuggingFace Dataset\n",
    "    dataset_dict = {\n",
    "        \"address\": df[target_column].fillna(\"\").astype(str).tolist(),\n",
    "        \"row_index\": df[index_column].tolist() if index_column else df.index.tolist()\n",
    "    }\n",
    "    \n",
    "    if \"datapoint_id\" in df.columns:\n",
    "        dataset_dict[\"datapoint_id\"] = df[\"datapoint_id\"].tolist()\n",
    "    \n",
    "    dataset = Dataset.from_dict(dataset_dict)\n",
    "    \n",
    "    # Tokenization function that creates a fresh tokenizer (no CUDA dependencies)\n",
    "    def tokenize_function(examples):\n",
    "        # Import here to avoid issues with multiprocessing\n",
    "        from transformers import AutoTokenizer\n",
    "        \n",
    "        # Create a completely fresh tokenizer in each worker process\n",
    "        # This avoids CUDA state from the main process\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        \n",
    "        tokenized = tokenizer(\n",
    "            examples[\"address\"],\n",
    "            padding=False,\n",
    "            truncation=True,\n",
    "            max_length=128,\n",
    "            return_offsets_mapping=True\n",
    "        )\n",
    "        \n",
    "        # Keep the original address and metadata\n",
    "        tokenized[\"original_address\"] = examples[\"address\"]\n",
    "        tokenized[\"row_index\"] = examples[\"row_index\"]\n",
    "        \n",
    "        if \"datapoint_id\" in examples:\n",
    "            tokenized[\"datapoint_id\"] = examples[\"datapoint_id\"]\n",
    "        \n",
    "        return tokenized\n",
    "    \n",
    "    # Apply tokenization with better error handling\n",
    "    try:\n",
    "        print(\"Attempting parallel tokenization...\")\n",
    "        tokenized_dataset = dataset.map(\n",
    "            tokenize_function,\n",
    "            batched=True,\n",
    "            batch_size=1000,\n",
    "            num_proc=4,  # Try more workers since we fixed the CUDA issue\n",
    "            remove_columns=[],\n",
    "            desc=\"Tokenizing\"  # Progress bar for tokenization\n",
    "        )\n",
    "        print(\"Parallel tokenization successful!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Parallel tokenization failed: {e}\")\n",
    "        print(\"Falling back to single-process tokenization...\")\n",
    "        tokenized_dataset = dataset.map(\n",
    "            tokenize_function,\n",
    "            batched=True,\n",
    "            batch_size=1000,\n",
    "            num_proc=None,\n",
    "            remove_columns=[],\n",
    "            desc=\"Tokenizing (single-process)\"\n",
    "        )\n",
    "        print(\"Single-process tokenization complete!\")\n",
    "    \n",
    "    # Fixed entity extraction function\n",
    "    def extract_entities_fixed(address, tokens, predicted_labels, offset_mapping):\n",
    "        \"\"\"\n",
    "        Fixed version that handles both tensor and list offset mappings\n",
    "        \"\"\"\n",
    "        entities = []\n",
    "        current_entity = None\n",
    "        \n",
    "        for i, (token, label, offset) in enumerate(zip(tokens, predicted_labels, offset_mapping)):\n",
    "            # Skip special tokens\n",
    "            if token in ['[CLS]', '[SEP]', '[PAD]']:\n",
    "                continue\n",
    "                \n",
    "            # Handle offset mapping (could be tensor or list)\n",
    "            if hasattr(offset, 'tolist'):\n",
    "                start_pos, end_pos = offset.tolist()  # It's a tensor\n",
    "            else:\n",
    "                start_pos, end_pos = offset  # It's already a list\n",
    "            \n",
    "            # Skip tokens with no mapping (like padding)\n",
    "            if start_pos == end_pos:\n",
    "                continue\n",
    "                \n",
    "            if label.startswith('B-'):\n",
    "                # Beginning of new entity\n",
    "                if current_entity:\n",
    "                    entities.append(current_entity)\n",
    "                \n",
    "                entity_type = label[2:]  # Remove 'B-' prefix\n",
    "                current_entity = {\n",
    "                    'type': entity_type,\n",
    "                    'start': start_pos,\n",
    "                    'end': end_pos,\n",
    "                    'text': address[start_pos:end_pos]\n",
    "                }\n",
    "                \n",
    "            elif label.startswith('I-') and current_entity:\n",
    "                # Inside entity (continuation)\n",
    "                entity_type = label[2:]  # Remove 'I-' prefix\n",
    "                if entity_type == current_entity['type']:\n",
    "                    # Extend current entity\n",
    "                    current_entity['end'] = end_pos\n",
    "                    current_entity['text'] = address[current_entity['start']:end_pos]\n",
    "                else:\n",
    "                    # Different entity type, start new one\n",
    "                    entities.append(current_entity)\n",
    "                    current_entity = {\n",
    "                        'type': entity_type,\n",
    "                        'start': start_pos,\n",
    "                        'end': end_pos,\n",
    "                        'text': address[start_pos:end_pos]\n",
    "                    }\n",
    "            else:\n",
    "                # 'O' label or mismatched I- label\n",
    "                if current_entity:\n",
    "                    entities.append(current_entity)\n",
    "                    current_entity = None\n",
    "        \n",
    "        # Don't forget the last entity\n",
    "        if current_entity:\n",
    "            entities.append(current_entity)\n",
    "            \n",
    "        return entities\n",
    "    \n",
    "    # Custom collate function\n",
    "    def collate_fn(batch):\n",
    "        input_ids = [item[\"input_ids\"] for item in batch]\n",
    "        attention_mask = [item[\"attention_mask\"] for item in batch]\n",
    "        offset_mapping = [item[\"offset_mapping\"] for item in batch]\n",
    "        row_indices = [item[\"row_index\"] for item in batch]\n",
    "        addresses = [item[\"original_address\"] for item in batch]\n",
    "        \n",
    "        # Efficient padding\n",
    "        max_len = max(len(ids) for ids in input_ids)\n",
    "        padded_input_ids = []\n",
    "        padded_attention_mask = []\n",
    "        \n",
    "        for ids, mask in zip(input_ids, attention_mask):\n",
    "            pad_len = max_len - len(ids)\n",
    "            padded_input_ids.append(ids + [parser.tokenizer.pad_token_id] * pad_len)\n",
    "            padded_attention_mask.append(mask + [0] * pad_len)\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(padded_input_ids),\n",
    "            \"attention_mask\": torch.tensor(padded_attention_mask),\n",
    "            \"offset_mapping\": offset_mapping,\n",
    "            \"addresses\": addresses,\n",
    "            \"row_indices\": row_indices,\n",
    "            \"datapoint_ids\": [item.get(\"datapoint_id\") for item in batch] if \"datapoint_id\" in batch[0] else None\n",
    "        }\n",
    "    \n",
    "    # Create DataLoader\n",
    "    dataloader = DataLoader(\n",
    "        tokenized_dataset,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=0,  # Keep this at 0 since tokenization is the bottleneck\n",
    "        pin_memory=True,\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    print(f\"Processing {len(dataset)} addresses in batches of {batch_size}\")\n",
    "    \n",
    "    # Initialize results and timing\n",
    "    all_results = []\n",
    "    successful_parses = 0\n",
    "    \n",
    "    time_data_loading = 0\n",
    "    time_gpu_inference = 0\n",
    "    time_cpu_transfer = 0\n",
    "    time_entity_extraction = 0\n",
    "    \n",
    "    # Process batches\n",
    "    for batch in tqdm(dataloader, desc=\"Processing batches\"):\n",
    "        # Data loading timing\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Move tensors to GPU\n",
    "        input_ids = batch[\"input_ids\"].to(parser.device, non_blocking=True)\n",
    "        attention_mask = batch[\"attention_mask\"].to(parser.device, non_blocking=True)\n",
    "        \n",
    "        time_data_loading += time.time() - start_time\n",
    "        \n",
    "        # GPU Inference\n",
    "        start_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            outputs = parser.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "            predicted_token_class_ids = predictions.argmax(dim=-1)\n",
    "        time_gpu_inference += time.time() - start_time\n",
    "        \n",
    "        # CPU Transfer\n",
    "        start_time = time.time()\n",
    "        predicted_token_class_ids_cpu = predicted_token_class_ids.cpu()\n",
    "        input_ids_cpu = input_ids.cpu()\n",
    "        torch.cuda.synchronize()\n",
    "        time_cpu_transfer += time.time() - start_time\n",
    "        \n",
    "        # Entity Extraction\n",
    "        start_time = time.time()\n",
    "        for j in range(len(batch[\"addresses\"])):\n",
    "            address = batch[\"addresses\"][j]\n",
    "            tokens = parser.tokenizer.convert_ids_to_tokens(input_ids_cpu[j])\n",
    "            predicted_labels = [parser.id2label[pred.item()] for pred in predicted_token_class_ids_cpu[j]]\n",
    "            offset_mapping = batch[\"offset_mapping\"][j]\n",
    "            \n",
    "            try:\n",
    "                # Use our fixed entity extraction\n",
    "                entities = extract_entities_fixed(address, tokens, predicted_labels, offset_mapping)\n",
    "                if len(entities) > 0:\n",
    "                    successful_parses += 1\n",
    "            except Exception as e:\n",
    "                entities = []\n",
    "            \n",
    "            result = {\n",
    "                \"row_index\": batch[\"row_indices\"][j],\n",
    "                \"original_address\": address,\n",
    "                \"entities\": entities,\n",
    "                \"parsed_components\": parser._group_entities_by_type(entities) if hasattr(parser, '_group_entities_by_type') else {}\n",
    "            }\n",
    "            \n",
    "            if batch[\"datapoint_ids\"] and batch[\"datapoint_ids\"][j]:\n",
    "                result[\"datapoint_id\"] = batch[\"datapoint_ids\"][j]\n",
    "            \n",
    "            all_results.append(result)\n",
    "        \n",
    "        time_entity_extraction += time.time() - start_time\n",
    "        \n",
    "        # Memory cleanup\n",
    "        del outputs, predictions, predicted_token_class_ids\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Print detailed timing information\n",
    "    total_time = time_data_loading + time_gpu_inference + time_cpu_transfer + time_entity_extraction\n",
    "    print(f\"\\n=== FINAL PERFORMANCE ANALYSIS ===\")\n",
    "    print(f\"Data Loading:      {time_data_loading:.2f}s ({time_data_loading/total_time*100:.1f}%)\")\n",
    "    print(f\"GPU Inference:     {time_gpu_inference:.2f}s ({time_gpu_inference/total_time*100:.1f}%)\")\n",
    "    print(f\"CPU Transfer:      {time_cpu_transfer:.2f}s ({time_cpu_transfer/total_time*100:.1f}%)\")\n",
    "    print(f\"Entity Extraction: {time_entity_extraction:.2f}s ({time_entity_extraction/total_time*100:.1f}%)\")\n",
    "    print(f\"Total Time:        {total_time:.2f}s\")\n",
    "    print(f\"Processing Speed:  {len(dataset)/total_time:.1f} addresses/second\")\n",
    "    print(f\"Success Rate:      {successful_parses/len(dataset)*100:.1f}%\")\n",
    "    \n",
    "    return {\n",
    "        \"summary\": {\n",
    "            \"total_addresses\": len(dataset),\n",
    "            \"successful_parses\": successful_parses,\n",
    "            \"failed_parses\": len(dataset) - successful_parses,\n",
    "            \"success_rate\": successful_parses / len(dataset) if len(dataset) > 0 else 0,\n",
    "            \"processing_time\": total_time,\n",
    "            \"addresses_per_second\": len(dataset) / total_time if total_time > 0 else 0\n",
    "        },\n",
    "        \"results\": all_results\n",
    "    }\n",
    "\n",
    "# Test the final version\n",
    "results = parse_addresses_with_dataloader_final(\n",
    "    df=ground_truth_df,\n",
    "    model_path=str(model_path),\n",
    "    target_column=\"property_address\",\n",
    "    batch_size=128\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model with mixed precision: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 33348 addresses with optimized pipeline (internal batch_size=128)\n",
      "Let the pipeline handle all batching and optimization...\n",
      "Pipeline completed processing 33348 addresses in 74.53s\n",
      "\n",
      "=== OPTIMIZED PIPELINE PERFORMANCE ANALYSIS ===\n",
      "Pipeline Processing:   74.53s (99.7%)\n",
      "Post-processing:       0.25s (0.3%)\n",
      "Other overhead:        0.00s (0.0%)\n",
      "Total Time:            74.78s\n",
      "Processing Speed:      446.0 addresses/second\n",
      "Success Rate:          100.0%\n",
      "Mixed Precision:       Enabled (FP16)\n",
      "Average confidence:    0.925\n",
      "Pipeline batch size:   128\n"
     ]
    }
   ],
   "source": [
    "import torch.multiprocessing as mp\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, Optional\n",
    "import pandas as pd\n",
    "\n",
    "def parse_addresses_with_optimized_pipeline(\n",
    "    df: pd.DataFrame,\n",
    "    model_path: str,\n",
    "    target_column: str = \"address\", \n",
    "    index_column: Optional[str] = None,\n",
    "    batch_size: int = 256,\n",
    "    use_fp16: bool = True\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Optimized version using HuggingFace Pipeline + Mixed Precision\n",
    "    Let the pipeline handle all batching internally!\n",
    "    \"\"\"\n",
    "    \n",
    "    # Setup index handling\n",
    "    if index_column is None and \"datapoint_id\" in df.columns:\n",
    "        index_column = \"datapoint_id\"\n",
    "    \n",
    "    # Create optimized pipeline with mixed precision\n",
    "    print(f\"Loading model with mixed precision: {use_fp16}\")\n",
    "    \n",
    "    device = 0 if torch.cuda.is_available() else -1\n",
    "    torch_dtype = torch.float16 if use_fp16 and torch.cuda.is_available() else torch.float32\n",
    "    \n",
    "    # The key optimization: let the pipeline handle batching internally\n",
    "    classifier = pipeline(\n",
    "        \"token-classification\",\n",
    "        model=model_path,\n",
    "        tokenizer=model_path,\n",
    "        device=device,\n",
    "        torch_dtype=torch_dtype,\n",
    "        aggregation_strategy=\"simple\",\n",
    "        batch_size=batch_size  # This tells the pipeline its internal batch size\n",
    "    )\n",
    "    \n",
    "    # Prepare data\n",
    "    addresses = df[target_column].fillna(\"\").astype(str).tolist()\n",
    "    row_indices = df[index_column].tolist() if index_column else df.index.tolist()\n",
    "    datapoint_ids = df[\"datapoint_id\"].tolist() if \"datapoint_id\" in df.columns else None\n",
    "    \n",
    "    print(f\"Processing {len(addresses)} addresses with optimized pipeline (internal batch_size={batch_size})\")\n",
    "    print(\"Let the pipeline handle all batching and optimization...\")\n",
    "    \n",
    "    # Initialize timing\n",
    "    total_start = time.time()\n",
    "    pipeline_start = time.time()\n",
    "    \n",
    "    # The magic: let the pipeline process ALL addresses at once!\n",
    "    # It will handle batching, GPU optimization, everything internally\n",
    "    try:\n",
    "        all_predictions = classifier(addresses)\n",
    "        pipeline_time = time.time() - pipeline_start\n",
    "        print(f\"Pipeline completed processing {len(addresses)} addresses in {pipeline_time:.2f}s\")\n",
    "    except Exception as e:\n",
    "        print(f\"Full batch processing failed: {e}\")\n",
    "        print(\"This might be due to memory constraints, falling back to manual batching...\")\n",
    "        \n",
    "        # Fallback to manual batching if memory is an issue\n",
    "        all_predictions = []\n",
    "        pipeline_time = 0\n",
    "        \n",
    "        # Process in smaller chunks\n",
    "        chunk_size = batch_size\n",
    "        for i in tqdm(range(0, len(addresses), chunk_size), desc=\"Processing chunks\"):\n",
    "            chunk_start = time.time()\n",
    "            chunk = addresses[i:i + chunk_size]\n",
    "            chunk_predictions = classifier(chunk)\n",
    "            all_predictions.extend(chunk_predictions)\n",
    "            pipeline_time += time.time() - chunk_start\n",
    "    \n",
    "    # Post-process results\n",
    "    postprocess_start = time.time()\n",
    "    all_results = []\n",
    "    successful_parses = 0\n",
    "    \n",
    "    for i, (address, entities) in enumerate(zip(addresses, all_predictions)):\n",
    "        # Convert pipeline output to our format\n",
    "        converted_entities = []\n",
    "        \n",
    "        if entities and isinstance(entities, list):\n",
    "            for entity in entities:\n",
    "                if isinstance(entity, dict):\n",
    "                    converted_entities.append({\n",
    "                        'type': entity.get('entity_group', entity.get('entity', 'UNKNOWN')),\n",
    "                        'text': entity.get('word', ''),\n",
    "                        'start': entity.get('start', 0),\n",
    "                        'end': entity.get('end', 0),\n",
    "                        'confidence': entity.get('score', 0.0)\n",
    "                    })\n",
    "        \n",
    "        # Group entities by type\n",
    "        parsed_components = {}\n",
    "        for entity in converted_entities:\n",
    "            entity_type = entity['type']\n",
    "            if entity_type not in parsed_components:\n",
    "                parsed_components[entity_type] = []\n",
    "            parsed_components[entity_type].append(entity)\n",
    "        \n",
    "        if len(converted_entities) > 0:\n",
    "            successful_parses += 1\n",
    "        \n",
    "        result = {\n",
    "            \"row_index\": row_indices[i],\n",
    "            \"original_address\": address,\n",
    "            \"entities\": converted_entities,\n",
    "            \"parsed_components\": parsed_components\n",
    "        }\n",
    "        \n",
    "        if datapoint_ids and datapoint_ids[i]:\n",
    "            result[\"datapoint_id\"] = datapoint_ids[i]\n",
    "        \n",
    "        all_results.append(result)\n",
    "    \n",
    "    postprocess_time = time.time() - postprocess_start\n",
    "    total_time = time.time() - total_start\n",
    "    \n",
    "    # Calculate metrics\n",
    "    total_entities = sum(len(r['entities']) for r in all_results)\n",
    "    avg_confidence = (\n",
    "        sum(sum(e['confidence'] for e in r['entities']) for r in all_results if r['entities']) / max(total_entities, 1)\n",
    "        if total_entities > 0 else 0.0\n",
    "    )\n",
    "    \n",
    "    # Print detailed timing information\n",
    "    print(f\"\\n=== OPTIMIZED PIPELINE PERFORMANCE ANALYSIS ===\")\n",
    "    print(f\"Pipeline Processing:   {pipeline_time:.2f}s ({pipeline_time/total_time*100:.1f}%)\")\n",
    "    print(f\"Post-processing:       {postprocess_time:.2f}s ({postprocess_time/total_time*100:.1f}%)\")\n",
    "    print(f\"Other overhead:        {(total_time - pipeline_time - postprocess_time):.2f}s ({(total_time - pipeline_time - postprocess_time)/total_time*100:.1f}%)\")\n",
    "    print(f\"Total Time:            {total_time:.2f}s\")\n",
    "    print(f\"Processing Speed:      {len(addresses)/total_time:.1f} addresses/second\")\n",
    "    print(f\"Success Rate:          {successful_parses/len(addresses)*100:.1f}%\")\n",
    "    print(f\"Mixed Precision:       {'Enabled (FP16)' if use_fp16 else 'Disabled (FP32)'}\")\n",
    "    print(f\"Average confidence:    {avg_confidence:.3f}\")\n",
    "    print(f\"Pipeline batch size:   {batch_size}\")\n",
    "    \n",
    "    return {\n",
    "        \"summary\": {\n",
    "            \"total_addresses\": len(addresses),\n",
    "            \"successful_parses\": successful_parses,\n",
    "            \"failed_parses\": len(addresses) - successful_parses,\n",
    "            \"success_rate\": successful_parses / len(addresses) if len(addresses) > 0 else 0,\n",
    "            \"processing_time\": total_time,\n",
    "            \"addresses_per_second\": len(addresses) / total_time if total_time > 0 else 0,\n",
    "            \"pipeline_time\": pipeline_time,\n",
    "            \"postprocess_time\": postprocess_time,\n",
    "            \"mixed_precision_enabled\": use_fp16,\n",
    "            \"average_confidence\": avg_confidence\n",
    "        },\n",
    "        \"results\": all_results\n",
    "    }\n",
    "\n",
    "# Test with proper pipeline usage\n",
    "results_optimized = parse_addresses_with_optimized_pipeline(\n",
    "    df=ground_truth_df,\n",
    "    model_path=str(model_path),\n",
    "    target_column=\"property_address\",\n",
    "    batch_size=128,  # Can be much larger now since pipeline handles it efficiently\n",
    "    use_fp16=True\n",
    ")\n",
    "\n",
    "# You can also try different batch sizes to see what works best:\n",
    "# batch_size=256, 512, 1024, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mixed precision: True\n",
      "Attempting parallel tokenization...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0b549e4678b4c52acc69f523a28a301",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing (num_proc=4):   0%|          | 0/33348 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallel tokenization successful!\n",
      "Processing 33348 addresses in batches of 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 131/131 [00:27<00:00,  4.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== FINAL OPTIMIZED PERFORMANCE ===\n",
      "Data Loading:      0.02s (0.1%)\n",
      "GPU Inference:     4.78s (32.1%)\n",
      "CPU Transfer:      4.48s (30.2%)\n",
      "Entity Extraction: 5.58s (37.6%)\n",
      "Total Time:        14.86s\n",
      "Processing Speed:  2244.0 addresses/second\n",
      "Success Rate:      98.8%\n",
      "Mixed Precision:   Enabled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def parse_addresses_with_custom_optimizations_final(\n",
    "    df: pd.DataFrame,\n",
    "    model_path: str,\n",
    "    target_column: str = \"address\", \n",
    "    index_column: Optional[str] = None,\n",
    "    batch_size: int = 256,\n",
    "    use_fp16: bool = True\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Final optimized version with deprecation warnings fixed\n",
    "    \"\"\"\n",
    "    try:\n",
    "        mp.set_start_method('spawn', force=True)\n",
    "    except RuntimeError:\n",
    "        pass\n",
    "    \n",
    "    parser = AddressParserInference(model_path)\n",
    "    print(f\"Using mixed precision: {use_fp16}\")\n",
    "    \n",
    "    # Setup index handling\n",
    "    if index_column is None and \"datapoint_id\" in df.columns:\n",
    "        index_column = \"datapoint_id\"\n",
    "    \n",
    "    # Create HuggingFace Dataset\n",
    "    dataset_dict = {\n",
    "        \"address\": df[target_column].fillna(\"\").astype(str).tolist(),\n",
    "        \"row_index\": df[index_column].tolist() if index_column else df.index.tolist()\n",
    "    }\n",
    "    \n",
    "    if \"datapoint_id\" in df.columns:\n",
    "        dataset_dict[\"datapoint_id\"] = df[\"datapoint_id\"].tolist()\n",
    "    \n",
    "    dataset = Dataset.from_dict(dataset_dict)\n",
    "    \n",
    "    # Tokenization function\n",
    "    def tokenize_function(examples):\n",
    "        from transformers import AutoTokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        \n",
    "        tokenized = tokenizer(\n",
    "            examples[\"address\"],\n",
    "            padding=False,\n",
    "            truncation=True,\n",
    "            max_length=128,\n",
    "            return_offsets_mapping=True\n",
    "        )\n",
    "        \n",
    "        tokenized[\"original_address\"] = examples[\"address\"]\n",
    "        tokenized[\"row_index\"] = examples[\"row_index\"]\n",
    "        \n",
    "        if \"datapoint_id\" in examples:\n",
    "            tokenized[\"datapoint_id\"] = examples[\"datapoint_id\"]\n",
    "        \n",
    "        return tokenized\n",
    "    \n",
    "    # Tokenize with parallel processing\n",
    "    try:\n",
    "        print(\"Attempting parallel tokenization...\")\n",
    "        tokenized_dataset = dataset.map(\n",
    "            tokenize_function,\n",
    "            batched=True,\n",
    "            batch_size=1000,\n",
    "            num_proc=4,\n",
    "            remove_columns=[],\n",
    "            desc=\"Tokenizing\"\n",
    "        )\n",
    "        print(\"Parallel tokenization successful!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Parallel tokenization failed, using single process...\")\n",
    "        tokenized_dataset = dataset.map(\n",
    "            tokenize_function,\n",
    "            batched=True,\n",
    "            batch_size=1000,\n",
    "            remove_columns=[]\n",
    "        )\n",
    "    \n",
    "    # Entity extraction function\n",
    "    def extract_entities_optimized(address, tokens, predicted_labels, offset_mapping):\n",
    "        entities = []\n",
    "        current_entity = None\n",
    "        \n",
    "        for token, label, offset in zip(tokens, predicted_labels, offset_mapping):\n",
    "            if token in ['[CLS]', '[SEP]', '[PAD]']:\n",
    "                continue\n",
    "                \n",
    "            if hasattr(offset, 'tolist'):\n",
    "                start_pos, end_pos = offset.tolist()\n",
    "            else:\n",
    "                start_pos, end_pos = offset\n",
    "            \n",
    "            if start_pos == end_pos:\n",
    "                continue\n",
    "                \n",
    "            if label.startswith('B-'):\n",
    "                if current_entity:\n",
    "                    entities.append(current_entity)\n",
    "                \n",
    "                entity_type = label[2:]\n",
    "                current_entity = {\n",
    "                    'type': entity_type,\n",
    "                    'start': start_pos,\n",
    "                    'end': end_pos,\n",
    "                    'text': address[start_pos:end_pos]\n",
    "                }\n",
    "                \n",
    "            elif label.startswith('I-') and current_entity:\n",
    "                entity_type = label[2:]\n",
    "                if entity_type == current_entity['type']:\n",
    "                    current_entity['end'] = end_pos\n",
    "                    current_entity['text'] = address[current_entity['start']:end_pos]\n",
    "                else:\n",
    "                    entities.append(current_entity)\n",
    "                    current_entity = {\n",
    "                        'type': entity_type,\n",
    "                        'start': start_pos,\n",
    "                        'end': end_pos,\n",
    "                        'text': address[start_pos:end_pos]\n",
    "                    }\n",
    "            else:\n",
    "                if current_entity:\n",
    "                    entities.append(current_entity)\n",
    "                    current_entity = None\n",
    "        \n",
    "        if current_entity:\n",
    "            entities.append(current_entity)\n",
    "            \n",
    "        return entities\n",
    "    \n",
    "    # Collate function\n",
    "    def collate_fn(batch):\n",
    "        input_ids = [item[\"input_ids\"] for item in batch]\n",
    "        attention_mask = [item[\"attention_mask\"] for item in batch]\n",
    "        offset_mapping = [item[\"offset_mapping\"] for item in batch]\n",
    "        row_indices = [item[\"row_index\"] for item in batch]\n",
    "        addresses = [item[\"original_address\"] for item in batch]\n",
    "        \n",
    "        max_len = max(len(ids) for ids in input_ids)\n",
    "        padded_input_ids = []\n",
    "        padded_attention_mask = []\n",
    "        \n",
    "        for ids, mask in zip(input_ids, attention_mask):\n",
    "            pad_len = max_len - len(ids)\n",
    "            padded_input_ids.append(ids + [parser.tokenizer.pad_token_id] * pad_len)\n",
    "            padded_attention_mask.append(mask + [0] * pad_len)\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(padded_input_ids),\n",
    "            \"attention_mask\": torch.tensor(padded_attention_mask),\n",
    "            \"offset_mapping\": offset_mapping,\n",
    "            \"addresses\": addresses,\n",
    "            \"row_indices\": row_indices,\n",
    "            \"datapoint_ids\": [item.get(\"datapoint_id\") for item in batch] if \"datapoint_id\" in batch[0] else None\n",
    "        }\n",
    "    \n",
    "    # Create DataLoader\n",
    "    dataloader = DataLoader(\n",
    "        tokenized_dataset,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    print(f\"Processing {len(dataset)} addresses in batches of {batch_size}\")\n",
    "    \n",
    "    all_results = []\n",
    "    successful_parses = 0\n",
    "    \n",
    "    time_data_loading = 0\n",
    "    time_gpu_inference = 0\n",
    "    time_cpu_transfer = 0\n",
    "    time_entity_extraction = 0\n",
    "    \n",
    "    # Process batches with FIXED mixed precision (no deprecation warning)\n",
    "    for batch in tqdm(dataloader, desc=\"Processing batches\"):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        input_ids = batch[\"input_ids\"].to(parser.device, non_blocking=True)\n",
    "        attention_mask = batch[\"attention_mask\"].to(parser.device, non_blocking=True)\n",
    "        \n",
    "        time_data_loading += time.time() - start_time\n",
    "        \n",
    "        # GPU Inference with FIXED mixed precision syntax\n",
    "        start_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            if use_fp16:\n",
    "                # FIXED: Use the new autocast syntax\n",
    "                with torch.amp.autocast('cuda'):\n",
    "                    outputs = parser.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            else:\n",
    "                outputs = parser.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            \n",
    "            predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "            predicted_token_class_ids = predictions.argmax(dim=-1)\n",
    "        time_gpu_inference += time.time() - start_time\n",
    "        \n",
    "        # CPU Transfer\n",
    "        start_time = time.time()\n",
    "        predicted_token_class_ids_cpu = predicted_token_class_ids.cpu()\n",
    "        input_ids_cpu = input_ids.cpu()\n",
    "        torch.cuda.synchronize()\n",
    "        time_cpu_transfer += time.time() - start_time\n",
    "        \n",
    "        # Entity Extraction\n",
    "        start_time = time.time()\n",
    "        for j in range(len(batch[\"addresses\"])):\n",
    "            address = batch[\"addresses\"][j]\n",
    "            tokens = parser.tokenizer.convert_ids_to_tokens(input_ids_cpu[j])\n",
    "            predicted_labels = [parser.id2label[pred.item()] for pred in predicted_token_class_ids_cpu[j]]\n",
    "            offset_mapping = batch[\"offset_mapping\"][j]\n",
    "            \n",
    "            try:\n",
    "                entities = extract_entities_optimized(address, tokens, predicted_labels, offset_mapping)\n",
    "                if len(entities) > 0:\n",
    "                    successful_parses += 1\n",
    "            except Exception as e:\n",
    "                entities = []\n",
    "            \n",
    "            result = {\n",
    "                \"row_index\": batch[\"row_indices\"][j],\n",
    "                \"original_address\": address,\n",
    "                \"entities\": entities,\n",
    "                \"parsed_components\": parser._group_entities_by_type(entities) if hasattr(parser, '_group_entities_by_type') else {}\n",
    "            }\n",
    "            \n",
    "            if batch[\"datapoint_ids\"] and batch[\"datapoint_ids\"][j]:\n",
    "                result[\"datapoint_id\"] = batch[\"datapoint_ids\"][j]\n",
    "            \n",
    "            all_results.append(result)\n",
    "        \n",
    "        time_entity_extraction += time.time() - start_time\n",
    "        \n",
    "        del outputs, predictions, predicted_token_class_ids\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    total_time = time_data_loading + time_gpu_inference + time_cpu_transfer + time_entity_extraction\n",
    "    print(f\"\\n=== FINAL OPTIMIZED PERFORMANCE ===\")\n",
    "    print(f\"Data Loading:      {time_data_loading:.2f}s ({time_data_loading/total_time*100:.1f}%)\")\n",
    "    print(f\"GPU Inference:     {time_gpu_inference:.2f}s ({time_gpu_inference/total_time*100:.1f}%)\")\n",
    "    print(f\"CPU Transfer:      {time_cpu_transfer:.2f}s ({time_cpu_transfer/total_time*100:.1f}%)\")\n",
    "    print(f\"Entity Extraction: {time_entity_extraction:.2f}s ({time_entity_extraction/total_time*100:.1f}%)\")\n",
    "    print(f\"Total Time:        {total_time:.2f}s\")\n",
    "    print(f\"Processing Speed:  {len(dataset)/total_time:.1f} addresses/second\")\n",
    "    print(f\"Success Rate:      {successful_parses/len(dataset)*100:.1f}%\")\n",
    "    print(f\"Mixed Precision:   {'Enabled' if use_fp16 else 'Disabled'}\")\n",
    "    \n",
    "    return {\n",
    "        \"summary\": {\n",
    "            \"total_addresses\": len(dataset),\n",
    "            \"successful_parses\": successful_parses,\n",
    "            \"failed_parses\": len(dataset) - successful_parses,\n",
    "            \"success_rate\": successful_parses / len(dataset) if len(dataset) > 0 else 0,\n",
    "            \"processing_time\": total_time,\n",
    "            \"addresses_per_second\": len(dataset) / total_time if total_time > 0 else 0\n",
    "        },\n",
    "        \"results\": all_results\n",
    "    }\n",
    "\n",
    "# Test the final clean version\n",
    "results_final = parse_addresses_with_custom_optimizations_final(\n",
    "    df=ground_truth_df,\n",
    "    model_path=str(model_path),\n",
    "    target_column=\"property_address\",\n",
    "    batch_size=256,\n",
    "    use_fp16=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.multiprocessing as mp\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, Optional\n",
    "import pandas as pd\n",
    "\n",
    "def parse_addresses_batch(\n",
    "    df: pd.DataFrame,\n",
    "    model_path: str,\n",
    "    target_column: str = \"address\", \n",
    "    index_column: Optional[str] = None,\n",
    "    batch_size: int = 256,\n",
    "    use_fp16: bool = True,\n",
    "    show_progress: bool = True\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Production-ready batch address parsing with mixed precision optimization.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing addresses\n",
    "        model_path: Path to the trained model\n",
    "        target_column: Column name containing addresses\n",
    "        index_column: Column to use as index (optional)\n",
    "        batch_size: Batch size for processing\n",
    "        use_fp16: Enable mixed precision for speed\n",
    "        show_progress: Show progress bar\n",
    "    \n",
    "    Returns:\n",
    "        Dict with summary and results\n",
    "    \"\"\"\n",
    "    # Initialize multiprocessing\n",
    "    try:\n",
    "        mp.set_start_method('spawn', force=True)\n",
    "    except RuntimeError:\n",
    "        pass\n",
    "    \n",
    "    parser = AddressParserInference(model_path)\n",
    "    \n",
    "    # Setup indexing\n",
    "    if index_column is None and \"datapoint_id\" in df.columns:\n",
    "        index_column = \"datapoint_id\"\n",
    "    \n",
    "    # Prepare dataset\n",
    "    dataset_dict = {\n",
    "        \"address\": df[target_column].fillna(\"\").astype(str).tolist(),\n",
    "        \"row_index\": df[index_column].tolist() if index_column else df.index.tolist()\n",
    "    }\n",
    "    \n",
    "    if \"datapoint_id\" in df.columns:\n",
    "        dataset_dict[\"datapoint_id\"] = df[\"datapoint_id\"].tolist()\n",
    "    \n",
    "    dataset = Dataset.from_dict(dataset_dict)\n",
    "    \n",
    "    # Tokenization function\n",
    "    def tokenize_function(examples):\n",
    "        from transformers import AutoTokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        \n",
    "        tokenized = tokenizer(\n",
    "            examples[\"address\"],\n",
    "            padding=False,\n",
    "            truncation=True,\n",
    "            max_length=128,\n",
    "            return_offsets_mapping=True\n",
    "        )\n",
    "        \n",
    "        tokenized[\"original_address\"] = examples[\"address\"]\n",
    "        tokenized[\"row_index\"] = examples[\"row_index\"]\n",
    "        \n",
    "        if \"datapoint_id\" in examples:\n",
    "            tokenized[\"datapoint_id\"] = examples[\"datapoint_id\"]\n",
    "        \n",
    "        return tokenized\n",
    "    \n",
    "    # Apply tokenization\n",
    "    try:\n",
    "        tokenized_dataset = dataset.map(\n",
    "            tokenize_function,\n",
    "            batched=True,\n",
    "            batch_size=1000,\n",
    "            num_proc=4,\n",
    "            remove_columns=[]\n",
    "        )\n",
    "    except Exception:\n",
    "        tokenized_dataset = dataset.map(\n",
    "            tokenize_function,\n",
    "            batched=True,\n",
    "            batch_size=1000,\n",
    "            remove_columns=[]\n",
    "        )\n",
    "    \n",
    "    # Entity extraction\n",
    "    def extract_entities(address, tokens, predicted_labels, offset_mapping):\n",
    "        entities = []\n",
    "        current_entity = None\n",
    "        \n",
    "        for token, label, offset in zip(tokens, predicted_labels, offset_mapping):\n",
    "            if token in ['[CLS]', '[SEP]', '[PAD]']:\n",
    "                continue\n",
    "                \n",
    "            if hasattr(offset, 'tolist'):\n",
    "                start_pos, end_pos = offset.tolist()\n",
    "            else:\n",
    "                start_pos, end_pos = offset\n",
    "            \n",
    "            if start_pos == end_pos:\n",
    "                continue\n",
    "                \n",
    "            if label.startswith('B-'):\n",
    "                if current_entity:\n",
    "                    entities.append(current_entity)\n",
    "                \n",
    "                entity_type = label[2:]\n",
    "                current_entity = {\n",
    "                    'type': entity_type,\n",
    "                    'start': start_pos,\n",
    "                    'end': end_pos,\n",
    "                    'text': address[start_pos:end_pos]\n",
    "                }\n",
    "                \n",
    "            elif label.startswith('I-') and current_entity:\n",
    "                entity_type = label[2:]\n",
    "                if entity_type == current_entity['type']:\n",
    "                    current_entity['end'] = end_pos\n",
    "                    current_entity['text'] = address[current_entity['start']:end_pos]\n",
    "                else:\n",
    "                    entities.append(current_entity)\n",
    "                    current_entity = {\n",
    "                        'type': entity_type,\n",
    "                        'start': start_pos,\n",
    "                        'end': end_pos,\n",
    "                        'text': address[start_pos:end_pos]\n",
    "                    }\n",
    "            else:\n",
    "                if current_entity:\n",
    "                    entities.append(current_entity)\n",
    "                    current_entity = None\n",
    "        \n",
    "        if current_entity:\n",
    "            entities.append(current_entity)\n",
    "            \n",
    "        return entities\n",
    "    \n",
    "    # Collate function\n",
    "    def collate_fn(batch):\n",
    "        input_ids = [item[\"input_ids\"] for item in batch]\n",
    "        attention_mask = [item[\"attention_mask\"] for item in batch]\n",
    "        offset_mapping = [item[\"offset_mapping\"] for item in batch]\n",
    "        row_indices = [item[\"row_index\"] for item in batch]\n",
    "        addresses = [item[\"original_address\"] for item in batch]\n",
    "        \n",
    "        max_len = max(len(ids) for ids in input_ids)\n",
    "        padded_input_ids = []\n",
    "        padded_attention_mask = []\n",
    "        \n",
    "        for ids, mask in zip(input_ids, attention_mask):\n",
    "            pad_len = max_len - len(ids)\n",
    "            padded_input_ids.append(ids + [parser.tokenizer.pad_token_id] * pad_len)\n",
    "            padded_attention_mask.append(mask + [0] * pad_len)\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(padded_input_ids),\n",
    "            \"attention_mask\": torch.tensor(padded_attention_mask),\n",
    "            \"offset_mapping\": offset_mapping,\n",
    "            \"addresses\": addresses,\n",
    "            \"row_indices\": row_indices,\n",
    "            \"datapoint_ids\": [item.get(\"datapoint_id\") for item in batch] if \"datapoint_id\" in batch[0] else None\n",
    "        }\n",
    "    \n",
    "    # Create DataLoader\n",
    "    dataloader = DataLoader(\n",
    "        tokenized_dataset,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    # Process batches\n",
    "    all_results = []\n",
    "    successful_parses = 0\n",
    "    \n",
    "    iterator = tqdm(dataloader, desc=\"Processing addresses\") if show_progress else dataloader\n",
    "    \n",
    "    for batch in iterator:\n",
    "        # Move to GPU\n",
    "        input_ids = batch[\"input_ids\"].to(parser.device, non_blocking=True)\n",
    "        attention_mask = batch[\"attention_mask\"].to(parser.device, non_blocking=True)\n",
    "        \n",
    "        # Inference\n",
    "        with torch.no_grad():\n",
    "            if use_fp16:\n",
    "                with torch.amp.autocast('cuda'):\n",
    "                    outputs = parser.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            else:\n",
    "                outputs = parser.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            \n",
    "            predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "            predicted_token_class_ids = predictions.argmax(dim=-1)\n",
    "        \n",
    "        # Move to CPU\n",
    "        predicted_token_class_ids_cpu = predicted_token_class_ids.cpu()\n",
    "        input_ids_cpu = input_ids.cpu()\n",
    "        torch.cuda.synchronize()\n",
    "        \n",
    "        # Extract entities\n",
    "        for j in range(len(batch[\"addresses\"])):\n",
    "            address = batch[\"addresses\"][j]\n",
    "            tokens = parser.tokenizer.convert_ids_to_tokens(input_ids_cpu[j])\n",
    "            predicted_labels = [parser.id2label[pred.item()] for pred in predicted_token_class_ids_cpu[j]]\n",
    "            offset_mapping = batch[\"offset_mapping\"][j]\n",
    "            \n",
    "            try:\n",
    "                entities = extract_entities(address, tokens, predicted_labels, offset_mapping)\n",
    "                if len(entities) > 0:\n",
    "                    successful_parses += 1\n",
    "            except Exception:\n",
    "                entities = []\n",
    "            \n",
    "            result = {\n",
    "                \"row_index\": batch[\"row_indices\"][j],\n",
    "                \"original_address\": address,\n",
    "                \"entities\": entities,\n",
    "                \"parsed_components\": parser._group_entities_by_type(entities) if hasattr(parser, '_group_entities_by_type') else {}\n",
    "            }\n",
    "            \n",
    "            if batch[\"datapoint_ids\"] and batch[\"datapoint_ids\"][j]:\n",
    "                result[\"datapoint_id\"] = batch[\"datapoint_ids\"][j]\n",
    "            \n",
    "            all_results.append(result)\n",
    "        \n",
    "        # Cleanup\n",
    "        del outputs, predictions, predicted_token_class_ids\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return {\n",
    "        \"summary\": {\n",
    "            \"total_addresses\": len(dataset),\n",
    "            \"successful_parses\": successful_parses,\n",
    "            \"failed_parses\": len(dataset) - successful_parses,\n",
    "            \"success_rate\": successful_parses / len(dataset) if len(dataset) > 0 else 0\n",
    "        },\n",
    "        \"results\": all_results\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66e38b0a49d4467c991068fc827cebe4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/33348 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing addresses: 100%|██████████| 131/131 [00:28<00:00,  4.66it/s]\n"
     ]
    }
   ],
   "source": [
    "results_final = parse_addresses_batch(\n",
    "    df=ground_truth_df,\n",
    "    model_path=str(model_path),\n",
    "    target_column=\"property_address\",\n",
    "    batch_size=256,\n",
    "    use_fp16=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
