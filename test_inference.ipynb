{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enhance_ocod.inference_utils import parse_addresses_from_csv, convert_to_entity_dataframe\n",
    "from enhance_ocod.address_parsing_helper_functions import (load_and_prep_OCOD_data, parsing_and_expansion_process, post_process_expanded_data)\n",
    "from enhance_ocod.locate_and_classify_helper_functions import (load_postcode_district_lookup, preprocess_expandaded_ocod_data, \n",
    "                                                  add_missing_lads_ocod, load_voa_ratinglist, street_and_building_matching, substreet_matching,\n",
    "                                                  counts_of_businesses_per_oa_lsoa, voa_address_match_all_data, classification_type1, classification_type2,\n",
    "                                                  contract_ocod_after_classification)\n",
    "from enhance_ocod.price_paid_process import load_and_process_pricepaid_data\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "model_path = \"models/address_parser/checkpoint-750\"\n",
    "csv_path = \"data/ocod_history/OCOD_FULL_2015_10.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocod_data = load_and_prep_OCOD_data(\"data/ocod_history/OCOD_FULL_2022_07.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing and expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded existing test results from file\n",
      "Added missing columns: ['street_name', 'filter_type', 'city']\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists('data/test_results_df.parquet'):\n",
    "    # Load existing file\n",
    "    test = pd.read_parquet('data/test_results_df.parquet')\n",
    "    print(\"Loaded existing test results from file\")\n",
    "else:\n",
    "\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "\n",
    "    results = parse_addresses_from_csv(\n",
    "    df = ocod_data,\n",
    "    model_path=model_path,\n",
    "    target_column=\"property_address\",\n",
    "    batch_size=2048\n",
    "    )\n",
    "\n",
    "    end_time = time.time()\n",
    "    test = convert_to_entity_dataframe(results)\n",
    "    test.to_parquet('data/test_results_df.parquet')\n",
    "    print(\"Created new test results and saved to file\")\n",
    "\n",
    "test = parsing_and_expansion_process(all_entities = test)\n",
    "ocod_data = post_process_expanded_data(test, ocod_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geolocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/teamspace/studios/this_studio/enhance_ocod/src/enhance_ocod/locate_and_classify_helper_functions.py:108: DtypeWarning: Columns (18,31,39,44,51) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  postcode_district_lookup = pd.read_csv(f)[['pcds', 'oslaua', 'oa11', 'lsoa11', 'msoa11', 'ctry']]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load and pre-process the Land Registry price paid dataset\n",
      "Processed data found. Skipping preprocessing.\n",
      "Loaded 1067153 records for year 2017\n",
      "Loaded 1037132 records for year 2018\n",
      "Loaded 1011344 records for year 2019\n"
     ]
    }
   ],
   "source": [
    "ONSPD_path = \"data/ONSPD_FEB_2025.zip\"\n",
    "postcode_district_lookup = load_postcode_district_lookup(ONSPD_path)\n",
    "\n",
    "ocod_data = preprocess_expandaded_ocod_data(ocod_data, postcode_district_lookup)\n",
    "\n",
    "print(\"Load and pre-process the Land Registry price paid dataset\")\n",
    "price_paid_df = load_and_process_pricepaid_data(file_path ='data/price_paid_data/price_paid_complete_may_2025.csv', \n",
    "                                                postcode_district_lookup = postcode_district_lookup, years_needed = [2017, 2018, 2019])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data found. Skipping preprocessing.\n",
      "Loaded 1067153 records for year 2017\n",
      "Loaded 1037132 records for year 2018\n",
      "Loaded 1011344 records for year 2019\n",
      "Add in missing Local authority codes to the ocod dataset\n",
      "Load and pre-process the voa business ratings list dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/teamspace/studios/this_studio/enhance_ocod/src/enhance_ocod/locate_and_classify_helper_functions.py:481: DtypeWarning: Columns (2,13,22) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  voa_businesses = pd.read_csv(csv_file,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match street addresses and buildings\n",
      "replace the missing lsoa using street matching\n",
      "replace the missing lsoa using building matching\n",
      "insert newly ID'd LSOA and OA\n",
      "update missing LSOA and OA for nested properties where at least one nested property has an OA or LSOA\n",
      "Sub-street matching, this takes some time\n",
      "Add in businesses per oa and lsoa\n",
      "Identify businesses using address matching\n",
      "address matched  50 lads of 318\n",
      "address matched  100 lads of 318\n",
      "address matched  150 lads of 318\n",
      "address matched  200 lads of 318\n",
      "address matched  250 lads of 318\n",
      "address matched  300 lads of 318\n"
     ]
    }
   ],
   "source": [
    "price_paid_df = load_and_process_pricepaid_data(file_path ='data/price_paid_data/price_paid_complete_may_2025.csv', \n",
    "                                                postcode_district_lookup = postcode_district_lookup, years_needed = [2017, 2018, 2019])\n",
    "\n",
    "print(\"Add in missing Local authority codes to the ocod dataset\")\n",
    "ocod_data = add_missing_lads_ocod(ocod_data, price_paid_df)\n",
    "\n",
    "print(\"Load and pre-process the voa business ratings list dataset\")\n",
    "voa_businesses = load_voa_ratinglist(\"data/2023_non_domestic_rating_list_entries.zip\", postcode_district_lookup)\n",
    "del postcode_district_lookup  # memory management\n",
    "\n",
    "# Address matching\n",
    "print(\"Match street addresses and buildings\")\n",
    "ocod_data = street_and_building_matching(ocod_data, price_paid_df, voa_businesses)\n",
    "\n",
    "print('Sub-street matching, this takes some time')\n",
    "ocod_data = substreet_matching(ocod_data, price_paid_df, voa_businesses)\n",
    "del price_paid_df  # memory management\n",
    "\n",
    "# Business processing\n",
    "print('Add in businesses per oa and lsoa')\n",
    "ocod_data = counts_of_businesses_per_oa_lsoa(ocod_data, voa_businesses)\n",
    "\n",
    "print('Identify businesses using address matching')\n",
    "ocod_data = voa_address_match_all_data(ocod_data, voa_businesses)\n",
    "del voa_businesses  # memory management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MODEL LABEL CONFIGURATION ===\n",
      "Number of labels: 17\n",
      "id2label mapping: {0: 'O', 1: 'B-building_name', 2: 'I-building_name', 3: 'B-street_name', 4: 'I-street_name', 5: 'B-street_number', 6: 'I-street_number', 7: 'B-filter_type', 8: 'I-filter_type', 9: 'B-unit_id', 10: 'I-unit_id', 11: 'B-unit_type', 12: 'I-unit_type', 13: 'B-city', 14: 'I-city', 15: 'B-postcode', 16: 'I-postcode'}\n",
      "label2id mapping: {'B-building_name': 1, 'B-city': 13, 'B-filter_type': 7, 'B-postcode': 15, 'B-street_name': 3, 'B-street_number': 5, 'B-unit_id': 9, 'B-unit_type': 11, 'I-building_name': 2, 'I-city': 14, 'I-filter_type': 8, 'I-postcode': 16, 'I-street_name': 4, 'I-street_number': 6, 'I-unit_id': 10, 'I-unit_type': 12, 'O': 0}\n",
      "\n",
      "=== LABEL COMPARISON ===\n",
      "Expected labels: ['building_name', 'street_name', 'street_number', 'filter_type', 'unit_id', 'unit_type', 'city', 'postcode']\n",
      "Model entity types: ['building_name', 'city', 'filter_type', 'postcode', 'street_name', 'street_number', 'unit_id', 'unit_type']\n",
      "Missing from model: set()\n",
      "Extra in model: set()\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "def check_model_labels(model_path):\n",
    "    \"\"\"Check what labels your trained model actually contains\"\"\"\n",
    "    \n",
    "    # Load the model\n",
    "    model = AutoModelForTokenClassification.from_pretrained(model_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    \n",
    "    print(\"=== MODEL LABEL CONFIGURATION ===\")\n",
    "    print(f\"Number of labels: {model.config.num_labels}\")\n",
    "    print(f\"id2label mapping: {model.config.id2label}\")\n",
    "    print(f\"label2id mapping: {model.config.label2id}\")\n",
    "    \n",
    "    expected_labels = [\n",
    "        \"building_name\", \"street_name\", \"street_number\", \"filter_type\",\n",
    "        \"unit_id\", \"unit_type\", \"city\", \"postcode\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\n=== LABEL COMPARISON ===\")\n",
    "    model_labels = set(model.config.label2id.keys())\n",
    "    \n",
    "    # Remove BIO prefixes and 'O' for comparison\n",
    "    model_entity_types = set()\n",
    "    for label in model_labels:\n",
    "        if label.startswith('B-') or label.startswith('I-'):\n",
    "            model_entity_types.add(label[2:])  # Remove B- or I- prefix\n",
    "    \n",
    "    print(f\"Expected labels: {expected_labels}\")\n",
    "    print(f\"Model entity types: {sorted(model_entity_types)}\")\n",
    "    print(f\"Missing from model: {set(expected_labels) - model_entity_types}\")\n",
    "    print(f\"Extra in model: {model_entity_types - set(expected_labels)}\")\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "# Check your model\n",
    "model, tokenizer = check_model_labels(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TRAINING DATA ANALYSIS ===\n",
      "Total examples: 2000\n",
      "Total spans: 11176\n",
      "\n",
      "Label distribution:\n",
      "  building_name: 797 (7.1%)\n",
      "  city: 1937 (17.3%)\n",
      "  number_filter: 50 (0.4%)\n",
      "  postcode: 1618 (14.5%)\n",
      "  street_name: 2133 (19.1%)\n",
      "  street_number: 2939 (26.3%)\n",
      "  unit_id: 743 (6.6%)\n",
      "  unit_type: 959 (8.6%)\n",
      "\n",
      "✅ All expected labels found in training data\n"
     ]
    }
   ],
   "source": [
    "def analyze_training_data(json_path):\n",
    "    \"\"\"Analyze what labels are actually in your training data\"\"\"\n",
    "    \n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    print(\"=== TRAINING DATA ANALYSIS ===\")\n",
    "    print(f\"Total examples: {len(data)}\")\n",
    "    \n",
    "    # Count all labels\n",
    "    label_counts = Counter()\n",
    "    total_spans = 0\n",
    "    \n",
    "    for example in data:\n",
    "        for span in example.get('spans', []):\n",
    "            label_counts[span['label']] += 1\n",
    "            total_spans += 1\n",
    "    \n",
    "    print(f\"Total spans: {total_spans}\")\n",
    "    print(\"\\nLabel distribution:\")\n",
    "    for label, count in sorted(label_counts.items()):\n",
    "        percentage = (count / total_spans) * 100\n",
    "        print(f\"  {label}: {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Check for missing expected labels\n",
    "    expected_labels = [\n",
    "        \"building_name\", \"street_name\", \"street_number\", \"number_filter\",\n",
    "        \"unit_id\", \"unit_type\", \"city\", \"postcode\"\n",
    "    ]\n",
    "    \n",
    "    found_labels = set(label_counts.keys())\n",
    "    missing_labels = set(expected_labels) - found_labels\n",
    "    \n",
    "    if missing_labels:\n",
    "        print(f\"\\n⚠️  MISSING LABELS IN TRAINING DATA: {missing_labels}\")\n",
    "    else:\n",
    "        print(f\"\\n✅ All expected labels found in training data\")\n",
    "    \n",
    "    return label_counts\n",
    "\n",
    "# Analyze your training data\n",
    "label_counts = analyze_training_data(\"data/training_data/training_data_dev.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (973226428.py, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[17], line 11\u001b[0;36m\u001b[0m\n\u001b[0;31m    return_offsets_mapping=True,test\u001b[0m\n\u001b[0m                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "def debug_single_prediction(model, tokenizer, address):\n",
    "    \"\"\"Debug what the model predicts for a single address\"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(\n",
    "        address,\n",
    "        return_tensors=\"pt\",\n",
    "        return_offsets_mapping=True,test\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=128\n",
    "    )\n",
    "    \n",
    "    # Move to device (excluding offset_mapping)\n",
    "    model_inputs = {k: v.to(device) for k, v in inputs.items() if k != 'offset_mapping'}\n",
    "    \n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**model_inputs)\n",
    "        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "        predicted_ids = predictions.argmax(dim=-1)\n",
    "    \n",
    "    # Get tokens and labels\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "    predicted_labels = [model.config.id2label[pred.item()] for pred in predicted_ids[0]]\n",
    "    offset_mapping = inputs[\"offset_mapping\"][0]\n",
    "    \n",
    "    print(f\"=== DEBUGGING: {address} ===\")\n",
    "    print(\"Token -> Label -> Text_Span\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for i, (token, label, offset) in enumerate(zip(tokens, predicted_labels, offset_mapping)):\n",
    "        if token in ['[CLS]', '[SEP]', '[PAD]']:\n",
    "            continue\n",
    "        \n",
    "        start_pos, end_pos = offset.tolist()\n",
    "        if start_pos == 0 and end_pos == 0 and i > 0:\n",
    "            text_span = \"[SPECIAL]\"\n",
    "        else:\n",
    "            text_span = address[start_pos:end_pos]\n",
    "        \n",
    "        print(f\"{token:15} -> {label:15} -> '{text_span}'\")\n",
    "\n",
    "# Test with an address that should have street_name\n",
    "test_address = \"161, 163, 165, 167 and 169 uxbridge road, ealing\"\n",
    "debug_single_prediction(model, tokenizer, test_address)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
